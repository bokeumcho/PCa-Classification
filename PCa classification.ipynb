{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.interpolate as spi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NO</th>\n",
       "      <th>Age</th>\n",
       "      <th>event</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>size</th>\n",
       "      <th>GS</th>\n",
       "      <th>primary</th>\n",
       "      <th>secondary</th>\n",
       "      <th>P_day1</th>\n",
       "      <th>...</th>\n",
       "      <th>PSA14</th>\n",
       "      <th>PSA15</th>\n",
       "      <th>PSA16</th>\n",
       "      <th>PSA17</th>\n",
       "      <th>PSA18</th>\n",
       "      <th>PSA19</th>\n",
       "      <th>PSA20</th>\n",
       "      <th>PSA21</th>\n",
       "      <th>PSA22</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2002-07-27</td>\n",
       "      <td>...</td>\n",
       "      <td>21.09</td>\n",
       "      <td>23.62</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>71.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2005-10-10</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>63.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2008-06-20</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2002-02-26</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>83.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2000-03-08</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1838</td>\n",
       "      <td>126</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2008-10-24</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1839</td>\n",
       "      <td>127</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009-08-31</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>128</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-05-17</td>\n",
       "      <td>...</td>\n",
       "      <td>12.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1841</td>\n",
       "      <td>129</td>\n",
       "      <td>69.0</td>\n",
       "      <td>0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2008-04-16</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1842</td>\n",
       "      <td>130</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2008-12-02</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1843 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       NO   Age  event  height  weight size  GS   primary  secondary  \\\n",
       "0       1  74.0      1     NaN     NaN   60  8.0      4.0        4.0   \n",
       "1       2  71.0      1     NaN     NaN  NaN  7.0      3.0        4.0   \n",
       "2       3  63.0      1     NaN     NaN  NaN  8.0      4.0        4.0   \n",
       "3       4  80.0      1     NaN     NaN  NaN  7.0      4.0        3.0   \n",
       "4       5  83.0      1     NaN     NaN  NaN  9.0      5.0        4.0   \n",
       "...   ...   ...    ...     ...     ...  ...  ...      ...        ...   \n",
       "1838  126  70.0      0   168.0    82.0  NaN  NaN      NaN        NaN   \n",
       "1839  127  70.0      0   169.0    63.0  NaN  NaN      NaN        NaN   \n",
       "1840  128  76.0      0     NaN     NaN  NaN  NaN      NaN        NaN   \n",
       "1841  129  69.0      0   168.0    88.0  NaN  NaN      NaN        NaN   \n",
       "1842  130  82.0      0   173.0    72.0  NaN  NaN      NaN        NaN   \n",
       "\n",
       "          P_day1  ...  PSA14  PSA15 PSA16 PSA17 PSA18 PSA19 PSA20 PSA21 PSA22  \\\n",
       "0     2002-07-27  ...  21.09  23.62   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1     2005-10-10  ...    NaN    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "2     2008-06-20  ...    NaN    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "3     2002-02-26  ...    NaN    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "4     2000-03-08  ...    NaN    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "...          ...  ...    ...    ...   ...   ...   ...   ...   ...   ...   ...   \n",
       "1838  2008-10-24  ...    NaN    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1839  2009-08-31  ...    NaN    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1840  2011-05-17  ...  12.88    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1841  2008-04-16  ...    NaN    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1842  2008-12-02  ...    NaN    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "\n",
       "     index  \n",
       "0        0  \n",
       "1        1  \n",
       "2        2  \n",
       "3        3  \n",
       "4        4  \n",
       "...    ...  \n",
       "1838  1838  \n",
       "1839  1839  \n",
       "1840  1840  \n",
       "1841  1841  \n",
       "1842  1842  \n",
       "\n",
       "[1843 rows x 54 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('./PSA_dataset2.csv',encoding='utf-8')\n",
    "data.loc[data[\"event\"]=='Y',\"event\"]=1\n",
    "data.loc[data[\"event\"]!=1,\"event\"]=0\n",
    "data.loc[:,\"index\"]=data.index\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NO</th>\n",
       "      <th>Age</th>\n",
       "      <th>event</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>size</th>\n",
       "      <th>GS</th>\n",
       "      <th>primary</th>\n",
       "      <th>secondary</th>\n",
       "      <th>P_day1</th>\n",
       "      <th>...</th>\n",
       "      <th>PSA14</th>\n",
       "      <th>PSA15</th>\n",
       "      <th>PSA16</th>\n",
       "      <th>PSA17</th>\n",
       "      <th>PSA18</th>\n",
       "      <th>PSA19</th>\n",
       "      <th>PSA20</th>\n",
       "      <th>PSA21</th>\n",
       "      <th>PSA22</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2002-07-27</td>\n",
       "      <td>...</td>\n",
       "      <td>21.09</td>\n",
       "      <td>23.62</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>71.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2005-10-10</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>63.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2008-06-20</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2002-02-26</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>83.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2000-03-08</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1838</td>\n",
       "      <td>126</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2008-10-24</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1839</td>\n",
       "      <td>127</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009-08-31</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>128</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-05-17</td>\n",
       "      <td>...</td>\n",
       "      <td>12.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1841</td>\n",
       "      <td>129</td>\n",
       "      <td>69.0</td>\n",
       "      <td>0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2008-04-16</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1842</td>\n",
       "      <td>130</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2008-12-02</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1842 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       NO   Age  event  height  weight size  GS   primary  secondary  \\\n",
       "0       1  74.0      1     NaN     NaN   60  8.0      4.0        4.0   \n",
       "1       2  71.0      1     NaN     NaN  NaN  7.0      3.0        4.0   \n",
       "2       3  63.0      1     NaN     NaN  NaN  8.0      4.0        4.0   \n",
       "3       4  80.0      1     NaN     NaN  NaN  7.0      4.0        3.0   \n",
       "4       5  83.0      1     NaN     NaN  NaN  9.0      5.0        4.0   \n",
       "...   ...   ...    ...     ...     ...  ...  ...      ...        ...   \n",
       "1838  126  70.0      0   168.0    82.0  NaN  NaN      NaN        NaN   \n",
       "1839  127  70.0      0   169.0    63.0  NaN  NaN      NaN        NaN   \n",
       "1840  128  76.0      0     NaN     NaN  NaN  NaN      NaN        NaN   \n",
       "1841  129  69.0      0   168.0    88.0  NaN  NaN      NaN        NaN   \n",
       "1842  130  82.0      0   173.0    72.0  NaN  NaN      NaN        NaN   \n",
       "\n",
       "          P_day1  ...  PSA14  PSA15 PSA16 PSA17 PSA18 PSA19 PSA20 PSA21 PSA22  \\\n",
       "0     2002-07-27  ...  21.09  23.62   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1     2005-10-10  ...    NaN    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "2     2008-06-20  ...    NaN    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "3     2002-02-26  ...    NaN    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "4     2000-03-08  ...    NaN    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "...          ...  ...    ...    ...   ...   ...   ...   ...   ...   ...   ...   \n",
       "1838  2008-10-24  ...    NaN    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1839  2009-08-31  ...    NaN    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1840  2011-05-17  ...  12.88    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1841  2008-04-16  ...    NaN    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1842  2008-12-02  ...    NaN    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "\n",
       "     index  \n",
       "0        0  \n",
       "1        1  \n",
       "2        2  \n",
       "3        3  \n",
       "4        4  \n",
       "...    ...  \n",
       "1838  1838  \n",
       "1839  1839  \n",
       "1840  1840  \n",
       "1841  1841  \n",
       "1842  1842  \n",
       "\n",
       "[1842 rows x 54 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#날짜가 한바퀴 더 붙어있는 자료들\n",
    "data.loc[1723,\"P_day4\":\"P_day22\"]=np.nan\n",
    "data.loc[1723,\"PSA4\":\"PSA22\"]=np.nan\n",
    "data.loc[1746,\"P_day8\":\"P_day22\"]=np.nan\n",
    "data.loc[1746,\"PSA8\":\"PSA22\"]=np.nan\n",
    "data.loc[1773,\"P_day6\":\"P_day22\"]=np.nan\n",
    "data.loc[1773,\"PSA6\":\"PSA22\"]=np.nan\n",
    "data.loc[1776,\"P_day6\":\"P_day22\"]=np.nan\n",
    "data.loc[1776,\"PSA6\":\"PSA22\"]=np.nan\n",
    "data.loc[1778,\"P_day15\"]=np.nan\n",
    "data.loc[1778,\"PSA15\"]=np.nan\n",
    "data.loc[1793,\"P_day6\":\"P_day22\"]=np.nan\n",
    "data.loc[1793,\"PSA6\":\"PSA22\"]=np.nan\n",
    "data.loc[1808,\"P_day12\":\"P_day22\"]=np.nan\n",
    "data.loc[1808,\"PSA12\":\"PSA22\"]=np.nan\n",
    "data.loc[1827,\"P_day5\":\"P_day22\"]=np.nan\n",
    "data.loc[1827,\"PSA5\":\"PSA22\"]=np.nan\n",
    "data.loc[1831,\"P_day10\":\"P_day22\"]=np.nan\n",
    "data.loc[1831,\"PSA10\":\"PSA22\"]=np.nan\n",
    "data.loc[1834,\"P_day8\":\"P_day22\"]=np.nan\n",
    "data.loc[1834,\"PSA8\":\"PSA22\"]=np.nan\n",
    "data.loc[1501,\"PSA12\":\"PSA15\"]=np.nan\n",
    "data.loc[1501,\"P_day13\":\"P_day15\"]=np.nan\n",
    "\n",
    "data.loc[1733,\"Age\"]=69 #mean값\n",
    "data.drop(index=1607) #age=8인 오류값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interval 만들기\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "for i in range(0,1843):\n",
    "    n=22\n",
    "    while data.loc[i,\"P_day{0}\".format(n)]!=data.loc[i,\"P_day{0}\".format(n)]:\n",
    "        n-=1\n",
    "    data.loc[i,\"final_num\"]=n\n",
    "    data.loc[i,\"P_day_final\"]=data.loc[i,\"P_day{0}\".format(n)]\n",
    "    \n",
    "for i in range(1,23):\n",
    "    data[\"P_dt{0}\".format(i)]=pd.to_datetime(data[\"P_day{0}\".format(i)],format='%Y-%m-%d', errors='coerce')\n",
    "\n",
    "data[\"P_dt_final\"]=pd.to_datetime(data[\"P_day_final\"],format='%Y-%m-%d', errors='coerce')\n",
    "data[\"Bx_dt\"]=data[\"P_dt_final\"]\n",
    "\n",
    "for n in range(1,23):\n",
    "    data[\"interval{0}\".format(n)]=data[\"Bx_dt\"]-data[\"P_dt{0}\".format(n)]\n",
    "                           \n",
    "for i in range(0,1843):\n",
    "    for n in range(1,23):\n",
    "        data.loc[i,\"interval{0}\".format(n)]=data.loc[i,\"interval{0}\".format(n)].days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#날짜 순서 바뀌어 들어온 것들\n",
    "data.loc[1718,\"interval6\"]=849\n",
    "data.loc[1718,\"interval5\"]=866\n",
    "data.loc[1719,\"interval3\"]=159\n",
    "data.loc[1719,\"interval2\"]=802\n",
    "data.loc[1719,\"interval1\"]=935"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NO               0\n",
       "Age              0\n",
       "event            0\n",
       "height        1446\n",
       "weight        1446\n",
       "              ... \n",
       "interval18    1821\n",
       "interval19    1828\n",
       "interval20    1831\n",
       "interval21    1835\n",
       "interval22    1836\n",
       "Length: 102, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PSA1</th>\n",
       "      <th>PSA2</th>\n",
       "      <th>PSA3</th>\n",
       "      <th>PSA4</th>\n",
       "      <th>PSA5</th>\n",
       "      <th>PSA6</th>\n",
       "      <th>PSA7</th>\n",
       "      <th>PSA8</th>\n",
       "      <th>PSA9</th>\n",
       "      <th>PSA10</th>\n",
       "      <th>...</th>\n",
       "      <th>PSA13</th>\n",
       "      <th>PSA14</th>\n",
       "      <th>PSA15</th>\n",
       "      <th>PSA16</th>\n",
       "      <th>PSA17</th>\n",
       "      <th>PSA18</th>\n",
       "      <th>PSA19</th>\n",
       "      <th>PSA20</th>\n",
       "      <th>PSA21</th>\n",
       "      <th>PSA22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3</td>\n",
       "      <td>3.8</td>\n",
       "      <td>5.3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>6.19</td>\n",
       "      <td>5.81</td>\n",
       "      <td>5.73</td>\n",
       "      <td>6.62</td>\n",
       "      <td>6.23</td>\n",
       "      <td>...</td>\n",
       "      <td>6.07</td>\n",
       "      <td>21.09</td>\n",
       "      <td>23.62</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>7.66</td>\n",
       "      <td>9.84</td>\n",
       "      <td>0.81</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.15</td>\n",
       "      <td>4.13</td>\n",
       "      <td>5.11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.40</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.93</td>\n",
       "      <td>2.44</td>\n",
       "      <td>2.73</td>\n",
       "      <td>2.42</td>\n",
       "      <td>2.73</td>\n",
       "      <td>4.51</td>\n",
       "      <td>4.78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.17</td>\n",
       "      <td>1.78</td>\n",
       "      <td>16.72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1838</td>\n",
       "      <td>4.79</td>\n",
       "      <td>7.79</td>\n",
       "      <td>8.61</td>\n",
       "      <td>7.65</td>\n",
       "      <td>9.53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1839</td>\n",
       "      <td>6.23</td>\n",
       "      <td>14.01</td>\n",
       "      <td>8.72</td>\n",
       "      <td>12.72</td>\n",
       "      <td>4.89</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>8.97</td>\n",
       "      <td>8.76</td>\n",
       "      <td>9.15</td>\n",
       "      <td>10.29</td>\n",
       "      <td>12.73</td>\n",
       "      <td>10.4</td>\n",
       "      <td>11.73</td>\n",
       "      <td>11.95</td>\n",
       "      <td>11.10</td>\n",
       "      <td>9.26</td>\n",
       "      <td>...</td>\n",
       "      <td>11.31</td>\n",
       "      <td>12.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1841</td>\n",
       "      <td>1.79</td>\n",
       "      <td>1.82</td>\n",
       "      <td>9.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1842</td>\n",
       "      <td>13.69</td>\n",
       "      <td>14.68</td>\n",
       "      <td>9.43</td>\n",
       "      <td>10.58</td>\n",
       "      <td>30.91</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1843 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PSA1   PSA2   PSA3   PSA4   PSA5  PSA6   PSA7   PSA8   PSA9  PSA10  \\\n",
       "0      4.00      3    3.8    5.3    4.7  6.19   5.81   5.73   6.62   6.23   \n",
       "1      7.66   9.84   0.81    NaN    NaN   NaN    NaN    NaN    NaN    NaN   \n",
       "2      5.15   4.13   5.11    NaN    NaN   NaN    NaN    NaN    NaN    NaN   \n",
       "3      3.40    3.2   1.93   2.44   2.73  2.42   2.73   4.51   4.78    NaN   \n",
       "4      1.17   1.78  16.72    NaN    NaN   NaN    NaN    NaN    NaN    NaN   \n",
       "...     ...    ...    ...    ...    ...   ...    ...    ...    ...    ...   \n",
       "1838   4.79   7.79   8.61   7.65   9.53   NaN    NaN    NaN    NaN    NaN   \n",
       "1839   6.23  14.01   8.72  12.72   4.89   NaN    NaN    NaN    NaN    NaN   \n",
       "1840   8.97   8.76   9.15  10.29  12.73  10.4  11.73  11.95  11.10   9.26   \n",
       "1841   1.79   1.82    9.3    NaN    NaN   NaN    NaN    NaN    NaN    NaN   \n",
       "1842  13.69  14.68   9.43  10.58  30.91   NaN    NaN    NaN    NaN    NaN   \n",
       "\n",
       "      ...  PSA13  PSA14  PSA15  PSA16  PSA17  PSA18  PSA19  PSA20  PSA21  \\\n",
       "0     ...   6.07  21.09  23.62    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "1     ...    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2     ...    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "3     ...    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "4     ...    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "...   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "1838  ...    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "1839  ...    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "1840  ...  11.31  12.88    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "1841  ...    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "1842  ...    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "\n",
       "      PSA22  \n",
       "0       NaN  \n",
       "1       NaN  \n",
       "2       NaN  \n",
       "3       NaN  \n",
       "4       NaN  \n",
       "...     ...  \n",
       "1838    NaN  \n",
       "1839    NaN  \n",
       "1840    NaN  \n",
       "1841    NaN  \n",
       "1842    NaN  \n",
       "\n",
       "[1843 rows x 22 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_day=data.loc[:,\"interval1\":\"interval22\"]\n",
    "psa=data.loc[:,\"PSA1\":\"PSA22\"]\n",
    "\n",
    "p_day\n",
    "psa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#psa값 중 몇개가 >100, >5000 이런식으로 나와있다. 일단 >기호랑 ,기호 제거\n",
    "#TypeError: 'Series' objects are mutable, thus they cannot be hashed --> row값은 계속 바뀌어서(x hashable) key값으로 쓸 수 없다?\n",
    "for i in range(0,1843):\n",
    "    for n in range(0,22):\n",
    "        if type(psa.iloc[i,n])==str:\n",
    "            psa.iloc[i,n]=psa.iloc[i,n].replace('>','')\n",
    "            psa.iloc[i,n]=psa.iloc[i,n].replace('<','')\n",
    "            psa.iloc[i,n]=psa.iloc[i,n].replace('-','.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#숫자형으로 변환\n",
    "psa[[\"PSA1\",\"PSA2\",\"PSA3\",\"PSA4\",\"PSA5\",\"PSA6\",\"PSA7\",'PSA8']]=psa[[\"PSA1\",\"PSA2\",\"PSA3\",\"PSA4\",\"PSA5\",\"PSA6\",\"PSA7\",'PSA8']].apply(pd.to_numeric)\n",
    "\n",
    "data.loc[:,\"PSA1\":\"PSA22\"]=psa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PSA1</th>\n",
       "      <th>PSA2</th>\n",
       "      <th>PSA3</th>\n",
       "      <th>PSA4</th>\n",
       "      <th>PSA5</th>\n",
       "      <th>PSA6</th>\n",
       "      <th>PSA7</th>\n",
       "      <th>PSA8</th>\n",
       "      <th>PSA9</th>\n",
       "      <th>PSA10</th>\n",
       "      <th>...</th>\n",
       "      <th>PSA13</th>\n",
       "      <th>PSA14</th>\n",
       "      <th>PSA15</th>\n",
       "      <th>PSA16</th>\n",
       "      <th>PSA17</th>\n",
       "      <th>PSA18</th>\n",
       "      <th>PSA19</th>\n",
       "      <th>PSA20</th>\n",
       "      <th>PSA21</th>\n",
       "      <th>PSA22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>7.66</td>\n",
       "      <td>9.84</td>\n",
       "      <td>0.81</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.15</td>\n",
       "      <td>4.13</td>\n",
       "      <td>5.11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.40</td>\n",
       "      <td>3.20</td>\n",
       "      <td>1.93</td>\n",
       "      <td>2.44</td>\n",
       "      <td>2.73</td>\n",
       "      <td>2.42</td>\n",
       "      <td>2.73</td>\n",
       "      <td>4.51</td>\n",
       "      <td>4.78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.17</td>\n",
       "      <td>1.78</td>\n",
       "      <td>16.72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.13</td>\n",
       "      <td>1.59</td>\n",
       "      <td>1.22</td>\n",
       "      <td>4.28</td>\n",
       "      <td>3.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1721</td>\n",
       "      <td>14.55</td>\n",
       "      <td>11.37</td>\n",
       "      <td>8.31</td>\n",
       "      <td>5.53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1722</td>\n",
       "      <td>4.79</td>\n",
       "      <td>7.79</td>\n",
       "      <td>8.61</td>\n",
       "      <td>7.65</td>\n",
       "      <td>9.53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1723</td>\n",
       "      <td>6.23</td>\n",
       "      <td>14.01</td>\n",
       "      <td>8.72</td>\n",
       "      <td>12.72</td>\n",
       "      <td>4.89</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1724</td>\n",
       "      <td>8.97</td>\n",
       "      <td>8.76</td>\n",
       "      <td>9.15</td>\n",
       "      <td>10.29</td>\n",
       "      <td>12.73</td>\n",
       "      <td>10.40</td>\n",
       "      <td>11.73</td>\n",
       "      <td>11.95</td>\n",
       "      <td>11.10</td>\n",
       "      <td>9.26</td>\n",
       "      <td>...</td>\n",
       "      <td>11.31</td>\n",
       "      <td>12.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1725</td>\n",
       "      <td>1.79</td>\n",
       "      <td>1.82</td>\n",
       "      <td>9.30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1726 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PSA1   PSA2   PSA3   PSA4   PSA5   PSA6   PSA7   PSA8   PSA9  PSA10  \\\n",
       "0      7.66   9.84   0.81    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "1      5.15   4.13   5.11    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2      3.40   3.20   1.93   2.44   2.73   2.42   2.73   4.51   4.78    NaN   \n",
       "3      1.17   1.78  16.72    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "4      1.10   1.13   1.59   1.22   4.28   3.99    NaN    NaN    NaN    NaN   \n",
       "...     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "1721  14.55  11.37   8.31   5.53    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "1722   4.79   7.79   8.61   7.65   9.53    NaN    NaN    NaN    NaN    NaN   \n",
       "1723   6.23  14.01   8.72  12.72   4.89    NaN    NaN    NaN    NaN    NaN   \n",
       "1724   8.97   8.76   9.15  10.29  12.73  10.40  11.73  11.95  11.10   9.26   \n",
       "1725   1.79   1.82   9.30    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "\n",
       "      ...  PSA13  PSA14  PSA15  PSA16  PSA17  PSA18  PSA19  PSA20  PSA21  \\\n",
       "0     ...    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "1     ...    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2     ...    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "3     ...    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "4     ...    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "...   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "1721  ...    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "1722  ...    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "1723  ...    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "1724  ...  11.31  12.88    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "1725  ...    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "\n",
       "      PSA22  \n",
       "0       NaN  \n",
       "1       NaN  \n",
       "2       NaN  \n",
       "3       NaN  \n",
       "4       NaN  \n",
       "...     ...  \n",
       "1721    NaN  \n",
       "1722    NaN  \n",
       "1723    NaN  \n",
       "1724    NaN  \n",
       "1725    NaN  \n",
       "\n",
       "[1726 rows x 22 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_idx=[]\n",
    "for i in data.index:\n",
    "    idx=int(data.loc[i,\"final_num\"])-1\n",
    "    if psa.iloc[i,idx]>20:\n",
    "        drop_idx.append(i)\n",
    "\n",
    "data_drop=data.drop(index=drop_idx)\n",
    "data_drop.index=range(0,1726)\n",
    "data_drop.loc[:,\"PSA1\":\"PSA22\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PSA1</th>\n",
       "      <th>PSA2</th>\n",
       "      <th>PSA3</th>\n",
       "      <th>PSA4</th>\n",
       "      <th>PSA5</th>\n",
       "      <th>PSA6</th>\n",
       "      <th>PSA7</th>\n",
       "      <th>PSA8</th>\n",
       "      <th>PSA9</th>\n",
       "      <th>PSA10</th>\n",
       "      <th>...</th>\n",
       "      <th>PSA13</th>\n",
       "      <th>PSA14</th>\n",
       "      <th>PSA15</th>\n",
       "      <th>PSA16</th>\n",
       "      <th>PSA17</th>\n",
       "      <th>PSA18</th>\n",
       "      <th>PSA19</th>\n",
       "      <th>PSA20</th>\n",
       "      <th>PSA21</th>\n",
       "      <th>PSA22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>7.66</td>\n",
       "      <td>9.84</td>\n",
       "      <td>0.81</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.15</td>\n",
       "      <td>4.13</td>\n",
       "      <td>5.11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.40</td>\n",
       "      <td>3.20</td>\n",
       "      <td>1.93</td>\n",
       "      <td>2.44</td>\n",
       "      <td>2.73</td>\n",
       "      <td>2.42</td>\n",
       "      <td>2.73</td>\n",
       "      <td>4.51</td>\n",
       "      <td>4.78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.17</td>\n",
       "      <td>1.78</td>\n",
       "      <td>16.72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.13</td>\n",
       "      <td>1.59</td>\n",
       "      <td>1.22</td>\n",
       "      <td>4.28</td>\n",
       "      <td>3.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1721</td>\n",
       "      <td>14.55</td>\n",
       "      <td>11.37</td>\n",
       "      <td>8.31</td>\n",
       "      <td>5.53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1722</td>\n",
       "      <td>4.79</td>\n",
       "      <td>7.79</td>\n",
       "      <td>8.61</td>\n",
       "      <td>7.65</td>\n",
       "      <td>9.53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1723</td>\n",
       "      <td>6.23</td>\n",
       "      <td>14.01</td>\n",
       "      <td>8.72</td>\n",
       "      <td>12.72</td>\n",
       "      <td>4.89</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1724</td>\n",
       "      <td>8.97</td>\n",
       "      <td>8.76</td>\n",
       "      <td>9.15</td>\n",
       "      <td>10.29</td>\n",
       "      <td>12.73</td>\n",
       "      <td>10.40</td>\n",
       "      <td>11.73</td>\n",
       "      <td>11.95</td>\n",
       "      <td>11.10</td>\n",
       "      <td>9.26</td>\n",
       "      <td>...</td>\n",
       "      <td>11.31</td>\n",
       "      <td>12.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1725</td>\n",
       "      <td>1.79</td>\n",
       "      <td>1.82</td>\n",
       "      <td>9.30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1726 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PSA1   PSA2   PSA3   PSA4   PSA5   PSA6   PSA7   PSA8   PSA9  PSA10  \\\n",
       "0      7.66   9.84   0.81    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "1      5.15   4.13   5.11    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2      3.40   3.20   1.93   2.44   2.73   2.42   2.73   4.51   4.78    NaN   \n",
       "3      1.17   1.78  16.72    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "4      1.10   1.13   1.59   1.22   4.28   3.99    NaN    NaN    NaN    NaN   \n",
       "...     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "1721  14.55  11.37   8.31   5.53    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "1722   4.79   7.79   8.61   7.65   9.53    NaN    NaN    NaN    NaN    NaN   \n",
       "1723   6.23  14.01   8.72  12.72   4.89    NaN    NaN    NaN    NaN    NaN   \n",
       "1724   8.97   8.76   9.15  10.29  12.73  10.40  11.73  11.95  11.10   9.26   \n",
       "1725   1.79   1.82   9.30    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "\n",
       "      ...  PSA13  PSA14  PSA15  PSA16  PSA17  PSA18  PSA19  PSA20  PSA21  \\\n",
       "0     ...    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "1     ...    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2     ...    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "3     ...    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "4     ...    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "...   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "1721  ...    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "1722  ...    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "1723  ...    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "1724  ...  11.31  12.88    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "1725  ...    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "\n",
       "      PSA22  \n",
       "0       NaN  \n",
       "1       NaN  \n",
       "2       NaN  \n",
       "3       NaN  \n",
       "4       NaN  \n",
       "...     ...  \n",
       "1721    NaN  \n",
       "1722    NaN  \n",
       "1723    NaN  \n",
       "1724    NaN  \n",
       "1725    NaN  \n",
       "\n",
       "[1726 rows x 22 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_day=data_drop.loc[:,\"interval1\":\"interval22\"]\n",
    "psa=data_drop.loc[:,\"PSA1\":\"PSA22\"]\n",
    "\n",
    "p_day\n",
    "psa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#PSA_average 계산\n",
    "for i in data_drop.index:\n",
    "    data_drop.loc[i,\"PSA_average\"]=psa.iloc[i,:int(data_drop.loc[i,\"final_num\"])].mean()\n",
    "\n",
    "data_drop[\"PSA_average\"]=data_drop[\"PSA_average\"].apply(pd.to_numeric)\n",
    "    \n",
    "#같은 날짜 중복? 같은 날짜 여러번 검진? 일단 작은 수의 차이 주기\n",
    "for i in data_drop.index:\n",
    "    for n in range(1,22):\n",
    "        if p_day.loc[i,\"interval{0}\".format(n+1)]==p_day.loc[i,\"interval{0}\".format(n)]:\n",
    "            p_day.loc[i,\"interval{0}\".format(n)]=p_day.loc[i,\"interval{0}\".format(n+1)]+0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "선형 스플라인: 구간별 1차 함수를 구함 (https://m.blog.naver.com/PostView.nhn?blogId=mykepzzang&logNo=220577982182&proxyReferer=https%3A%2F%2Fwww.google.com%2F)\n",
    "\n",
    "SciPy 패키지의 interpolation 서브패키지는 1차원 스플라인 보간(spline interpolation)을 위한 다음과 같은 명령을 제공한다.\n",
    "\n",
    "splrep 명령: 스플라인 보간 모형(representation) 생성\n",
    "\n",
    "splev 명령: 만들어진 스플라인 보간 모형을 이용하여 새로운 x 값에 대해 y값 계산 (evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Age</th>\n",
       "      <th>event</th>\n",
       "      <th>PSA_average</th>\n",
       "      <th>PSA360</th>\n",
       "      <th>PSA270</th>\n",
       "      <th>PSA180</th>\n",
       "      <th>PSA90</th>\n",
       "      <th>PSA0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>71.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.103333</td>\n",
       "      <td>0</td>\n",
       "      <td>2.21</td>\n",
       "      <td>5.98308</td>\n",
       "      <td>9.75615</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>63.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.796667</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>14.4757</td>\n",
       "      <td>7.91857</td>\n",
       "      <td>5.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.126667</td>\n",
       "      <td>3.74997</td>\n",
       "      <td>4.00425</td>\n",
       "      <td>4.25854</td>\n",
       "      <td>4.51297</td>\n",
       "      <td>4.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>83.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.556667</td>\n",
       "      <td>15.7888</td>\n",
       "      <td>16.0216</td>\n",
       "      <td>16.2544</td>\n",
       "      <td>16.4872</td>\n",
       "      <td>16.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>76.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.218333</td>\n",
       "      <td>3.95337</td>\n",
       "      <td>4.04138</td>\n",
       "      <td>4.1294</td>\n",
       "      <td>4.21741</td>\n",
       "      <td>3.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1721</td>\n",
       "      <td>1837</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.940000</td>\n",
       "      <td>7.82016</td>\n",
       "      <td>7.24762</td>\n",
       "      <td>6.67508</td>\n",
       "      <td>6.10254</td>\n",
       "      <td>5.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1722</td>\n",
       "      <td>1838</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.674000</td>\n",
       "      <td>8.48755</td>\n",
       "      <td>8.04673</td>\n",
       "      <td>7.73952</td>\n",
       "      <td>8.63476</td>\n",
       "      <td>9.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1723</td>\n",
       "      <td>1839</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.314000</td>\n",
       "      <td>6.06352</td>\n",
       "      <td>5.77014</td>\n",
       "      <td>5.47676</td>\n",
       "      <td>5.18338</td>\n",
       "      <td>4.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1724</td>\n",
       "      <td>1840</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.893571</td>\n",
       "      <td>11.1232</td>\n",
       "      <td>10.7512</td>\n",
       "      <td>11.0322</td>\n",
       "      <td>11.3273</td>\n",
       "      <td>12.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1725</td>\n",
       "      <td>1841</td>\n",
       "      <td>69.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.303333</td>\n",
       "      <td>1.81693</td>\n",
       "      <td>2.7215</td>\n",
       "      <td>4.91433</td>\n",
       "      <td>7.10717</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1726 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index   Age  event  PSA_average   PSA360   PSA270   PSA180    PSA90  \\\n",
       "0         1  71.0      1     6.103333        0     2.21  5.98308  9.75615   \n",
       "1         2  63.0      1     4.796667       20       20  14.4757  7.91857   \n",
       "2         3  80.0      1     3.126667  3.74997  4.00425  4.25854  4.51297   \n",
       "3         4  83.0      1     6.556667  15.7888  16.0216  16.2544  16.4872   \n",
       "4         5  76.0      1     2.218333  3.95337  4.04138   4.1294  4.21741   \n",
       "...     ...   ...    ...          ...      ...      ...      ...      ...   \n",
       "1721   1837  63.0      0     9.940000  7.82016  7.24762  6.67508  6.10254   \n",
       "1722   1838  70.0      0     7.674000  8.48755  8.04673  7.73952  8.63476   \n",
       "1723   1839  70.0      0     9.314000  6.06352  5.77014  5.47676  5.18338   \n",
       "1724   1840  76.0      0    10.893571  11.1232  10.7512  11.0322  11.3273   \n",
       "1725   1841  69.0      0     4.303333  1.81693   2.7215  4.91433  7.10717   \n",
       "\n",
       "       PSA0  \n",
       "0      0.81  \n",
       "1      5.11  \n",
       "2      4.78  \n",
       "3     16.72  \n",
       "4      3.99  \n",
       "...     ...  \n",
       "1721   5.53  \n",
       "1722   9.53  \n",
       "1723   4.89  \n",
       "1724  12.88  \n",
       "1725    9.3  \n",
       "\n",
       "[1726 rows x 9 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#######3개월 단위\n",
    "x=np.array([360,270,180,90,0])\n",
    "PSA_uniform=pd.Series([1,1,1,1,1],index=[\"PSA360\",\"PSA270\",\"PSA180\",\"PSA90\",\"PSA0\"])\n",
    "\n",
    "for i in data_drop.index:\n",
    "    f=spi.interp1d(p_day.iloc[i,:int(data_drop.loc[i,\"final_num\"])].values,psa.iloc[i,:int(data_drop.loc[i,\"final_num\"])].values,bounds_error=False,fill_value=\"extrapolate\")\n",
    "    iy=f(x)\n",
    "    PSA_new=pd.Series(iy,index=[\"PSA360\",\"PSA270\",\"PSA180\",\"PSA90\",\"PSA0\"])\n",
    "    PSA_uniform=pd.concat([PSA_uniform,PSA_new],axis=1)\n",
    "\n",
    "PSA_uniform1=PSA_uniform.iloc[:,1:]\n",
    "PSA_uniform_tp=np.transpose(PSA_uniform1)\n",
    "PSA_uniform_tp.index=range(0,1726)\n",
    "\n",
    "PSA_uniform_tp[PSA_uniform_tp>20]=20\n",
    "PSA_uniform_tp[PSA_uniform_tp<0]=0\n",
    "\n",
    "data0=pd.concat([data_drop[[\"index\",\"Age\",\"event\",\"PSA_average\"]],PSA_uniform_tp],axis=1)\n",
    "data0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Age</th>\n",
       "      <th>event</th>\n",
       "      <th>PSA_average</th>\n",
       "      <th>PSA720</th>\n",
       "      <th>PSA630</th>\n",
       "      <th>PSA540</th>\n",
       "      <th>PSA450</th>\n",
       "      <th>PSA360</th>\n",
       "      <th>PSA270</th>\n",
       "      <th>PSA180</th>\n",
       "      <th>PSA90</th>\n",
       "      <th>PSA0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>71.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.103333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.21</td>\n",
       "      <td>5.98308</td>\n",
       "      <td>9.75615</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>63.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.796667</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>14.4757</td>\n",
       "      <td>7.91857</td>\n",
       "      <td>5.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.126667</td>\n",
       "      <td>2.73283</td>\n",
       "      <td>2.98711</td>\n",
       "      <td>3.2414</td>\n",
       "      <td>3.49568</td>\n",
       "      <td>3.74997</td>\n",
       "      <td>4.00425</td>\n",
       "      <td>4.25854</td>\n",
       "      <td>4.51297</td>\n",
       "      <td>4.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>83.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.556667</td>\n",
       "      <td>14.8577</td>\n",
       "      <td>15.0905</td>\n",
       "      <td>15.3233</td>\n",
       "      <td>15.556</td>\n",
       "      <td>15.7888</td>\n",
       "      <td>16.0216</td>\n",
       "      <td>16.2544</td>\n",
       "      <td>16.4872</td>\n",
       "      <td>16.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>76.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.218333</td>\n",
       "      <td>3.6013</td>\n",
       "      <td>3.68932</td>\n",
       "      <td>3.77733</td>\n",
       "      <td>3.86535</td>\n",
       "      <td>3.95337</td>\n",
       "      <td>4.04138</td>\n",
       "      <td>4.1294</td>\n",
       "      <td>4.21741</td>\n",
       "      <td>3.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1721</td>\n",
       "      <td>1837</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.940000</td>\n",
       "      <td>10.8347</td>\n",
       "      <td>10.0318</td>\n",
       "      <td>9.22889</td>\n",
       "      <td>8.42598</td>\n",
       "      <td>7.82016</td>\n",
       "      <td>7.24762</td>\n",
       "      <td>6.67508</td>\n",
       "      <td>6.10254</td>\n",
       "      <td>5.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1722</td>\n",
       "      <td>1838</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.674000</td>\n",
       "      <td>8.44422</td>\n",
       "      <td>8.48876</td>\n",
       "      <td>8.5333</td>\n",
       "      <td>8.57783</td>\n",
       "      <td>8.48755</td>\n",
       "      <td>8.04673</td>\n",
       "      <td>7.73952</td>\n",
       "      <td>8.63476</td>\n",
       "      <td>9.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1723</td>\n",
       "      <td>1839</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.314000</td>\n",
       "      <td>7.23704</td>\n",
       "      <td>6.94366</td>\n",
       "      <td>6.65028</td>\n",
       "      <td>6.3569</td>\n",
       "      <td>6.06352</td>\n",
       "      <td>5.77014</td>\n",
       "      <td>5.47676</td>\n",
       "      <td>5.18338</td>\n",
       "      <td>4.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1724</td>\n",
       "      <td>1840</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.893571</td>\n",
       "      <td>12.9375</td>\n",
       "      <td>12.4839</td>\n",
       "      <td>12.0303</td>\n",
       "      <td>11.5767</td>\n",
       "      <td>11.1232</td>\n",
       "      <td>10.7512</td>\n",
       "      <td>11.0322</td>\n",
       "      <td>11.3273</td>\n",
       "      <td>12.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1725</td>\n",
       "      <td>1841</td>\n",
       "      <td>69.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.303333</td>\n",
       "      <td>1.79608</td>\n",
       "      <td>1.80129</td>\n",
       "      <td>1.80651</td>\n",
       "      <td>1.81172</td>\n",
       "      <td>1.81693</td>\n",
       "      <td>2.7215</td>\n",
       "      <td>4.91433</td>\n",
       "      <td>7.10717</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1726 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index   Age  event  PSA_average   PSA720   PSA630   PSA540   PSA450  \\\n",
       "0         1  71.0      1     6.103333        0        0        0        0   \n",
       "1         2  63.0      1     4.796667       20       20       20       20   \n",
       "2         3  80.0      1     3.126667  2.73283  2.98711   3.2414  3.49568   \n",
       "3         4  83.0      1     6.556667  14.8577  15.0905  15.3233   15.556   \n",
       "4         5  76.0      1     2.218333   3.6013  3.68932  3.77733  3.86535   \n",
       "...     ...   ...    ...          ...      ...      ...      ...      ...   \n",
       "1721   1837  63.0      0     9.940000  10.8347  10.0318  9.22889  8.42598   \n",
       "1722   1838  70.0      0     7.674000  8.44422  8.48876   8.5333  8.57783   \n",
       "1723   1839  70.0      0     9.314000  7.23704  6.94366  6.65028   6.3569   \n",
       "1724   1840  76.0      0    10.893571  12.9375  12.4839  12.0303  11.5767   \n",
       "1725   1841  69.0      0     4.303333  1.79608  1.80129  1.80651  1.81172   \n",
       "\n",
       "       PSA360   PSA270   PSA180    PSA90   PSA0  \n",
       "0           0     2.21  5.98308  9.75615   0.81  \n",
       "1          20       20  14.4757  7.91857   5.11  \n",
       "2     3.74997  4.00425  4.25854  4.51297   4.78  \n",
       "3     15.7888  16.0216  16.2544  16.4872  16.72  \n",
       "4     3.95337  4.04138   4.1294  4.21741   3.99  \n",
       "...       ...      ...      ...      ...    ...  \n",
       "1721  7.82016  7.24762  6.67508  6.10254   5.53  \n",
       "1722  8.48755  8.04673  7.73952  8.63476   9.53  \n",
       "1723  6.06352  5.77014  5.47676  5.18338   4.89  \n",
       "1724  11.1232  10.7512  11.0322  11.3273  12.88  \n",
       "1725  1.81693   2.7215  4.91433  7.10717    9.3  \n",
       "\n",
       "[1726 rows x 13 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#######확장된 3개월 단위\n",
    "x=np.array([720,630,540,450,360,270,180,90,0])\n",
    "PSA_uniform=pd.Series([1,1,1,1,1,1,1,1,1],index=[\"PSA720\",\"PSA630\",\"PSA540\",\"PSA450\",\"PSA360\",\"PSA270\",\"PSA180\",\"PSA90\",\"PSA0\"])\n",
    "\n",
    "for i in data_drop.index:\n",
    "    f=spi.interp1d(p_day.iloc[i,:int(data_drop.loc[i,\"final_num\"])].values,psa.iloc[i,:int(data_drop.loc[i,\"final_num\"])].values,bounds_error=False,fill_value=\"extrapolate\")\n",
    "    iy=f(x)\n",
    "    PSA_new=pd.Series(iy,index=[\"PSA720\",\"PSA630\",\"PSA540\",\"PSA450\",\"PSA360\",\"PSA270\",\"PSA180\",\"PSA90\",\"PSA0\"])\n",
    "    PSA_uniform=pd.concat([PSA_uniform,PSA_new],axis=1)\n",
    "\n",
    "PSA_uniform1=PSA_uniform.iloc[:,1:]\n",
    "PSA_uniform_tp=np.transpose(PSA_uniform1)\n",
    "PSA_uniform_tp.index=range(0,1726)\n",
    "\n",
    "PSA_uniform_tp[PSA_uniform_tp>20]=20\n",
    "PSA_uniform_tp[PSA_uniform_tp<0]=0\n",
    "\n",
    "data1=pd.concat([data_drop[[\"index\",\"Age\",\"event\",\"PSA_average\"]],PSA_uniform_tp],axis=1)\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Age</th>\n",
       "      <th>event</th>\n",
       "      <th>PSA_average</th>\n",
       "      <th>PSA0m</th>\n",
       "      <th>PSA1m</th>\n",
       "      <th>PSA2m</th>\n",
       "      <th>PSA3m</th>\n",
       "      <th>PSA4m</th>\n",
       "      <th>PSA5m</th>\n",
       "      <th>PSA6m</th>\n",
       "      <th>PSA7m</th>\n",
       "      <th>PSA8m</th>\n",
       "      <th>PSA9m</th>\n",
       "      <th>PSA10m</th>\n",
       "      <th>PSA11m</th>\n",
       "      <th>PSA12m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>71.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.103333</td>\n",
       "      <td>0.81</td>\n",
       "      <td>3.88841</td>\n",
       "      <td>6.96682</td>\n",
       "      <td>9.75615</td>\n",
       "      <td>8.49846</td>\n",
       "      <td>7.24077</td>\n",
       "      <td>5.98308</td>\n",
       "      <td>4.72538</td>\n",
       "      <td>3.46769</td>\n",
       "      <td>2.21</td>\n",
       "      <td>0.952308</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>63.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.796667</td>\n",
       "      <td>5.11</td>\n",
       "      <td>4.33632</td>\n",
       "      <td>5.73286</td>\n",
       "      <td>7.91857</td>\n",
       "      <td>10.1043</td>\n",
       "      <td>12.29</td>\n",
       "      <td>14.4757</td>\n",
       "      <td>16.6614</td>\n",
       "      <td>18.8471</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.126667</td>\n",
       "      <td>4.78</td>\n",
       "      <td>4.69099</td>\n",
       "      <td>4.60198</td>\n",
       "      <td>4.51297</td>\n",
       "      <td>4.42806</td>\n",
       "      <td>4.3433</td>\n",
       "      <td>4.25854</td>\n",
       "      <td>4.17378</td>\n",
       "      <td>4.08902</td>\n",
       "      <td>4.00425</td>\n",
       "      <td>3.91949</td>\n",
       "      <td>3.83473</td>\n",
       "      <td>3.74997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>83.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.556667</td>\n",
       "      <td>16.72</td>\n",
       "      <td>16.6424</td>\n",
       "      <td>16.5648</td>\n",
       "      <td>16.4872</td>\n",
       "      <td>16.4096</td>\n",
       "      <td>16.332</td>\n",
       "      <td>16.2544</td>\n",
       "      <td>16.1768</td>\n",
       "      <td>16.0992</td>\n",
       "      <td>16.0216</td>\n",
       "      <td>15.944</td>\n",
       "      <td>15.8664</td>\n",
       "      <td>15.7888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>76.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.218333</td>\n",
       "      <td>3.99</td>\n",
       "      <td>4.27609</td>\n",
       "      <td>4.24675</td>\n",
       "      <td>4.21741</td>\n",
       "      <td>4.18807</td>\n",
       "      <td>4.15873</td>\n",
       "      <td>4.1294</td>\n",
       "      <td>4.10006</td>\n",
       "      <td>4.07072</td>\n",
       "      <td>4.04138</td>\n",
       "      <td>4.01204</td>\n",
       "      <td>3.9827</td>\n",
       "      <td>3.95337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1721</td>\n",
       "      <td>1837</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.940000</td>\n",
       "      <td>5.53</td>\n",
       "      <td>5.72085</td>\n",
       "      <td>5.91169</td>\n",
       "      <td>6.10254</td>\n",
       "      <td>6.29339</td>\n",
       "      <td>6.48423</td>\n",
       "      <td>6.67508</td>\n",
       "      <td>6.86593</td>\n",
       "      <td>7.05677</td>\n",
       "      <td>7.24762</td>\n",
       "      <td>7.43847</td>\n",
       "      <td>7.62931</td>\n",
       "      <td>7.82016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1722</td>\n",
       "      <td>1838</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.674000</td>\n",
       "      <td>9.53</td>\n",
       "      <td>9.23159</td>\n",
       "      <td>8.93317</td>\n",
       "      <td>8.63476</td>\n",
       "      <td>8.33635</td>\n",
       "      <td>8.03794</td>\n",
       "      <td>7.73952</td>\n",
       "      <td>7.75286</td>\n",
       "      <td>7.8998</td>\n",
       "      <td>8.04673</td>\n",
       "      <td>8.19367</td>\n",
       "      <td>8.34061</td>\n",
       "      <td>8.48755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1723</td>\n",
       "      <td>1839</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.314000</td>\n",
       "      <td>4.89</td>\n",
       "      <td>4.98779</td>\n",
       "      <td>5.08559</td>\n",
       "      <td>5.18338</td>\n",
       "      <td>5.28117</td>\n",
       "      <td>5.37897</td>\n",
       "      <td>5.47676</td>\n",
       "      <td>5.57455</td>\n",
       "      <td>5.67235</td>\n",
       "      <td>5.77014</td>\n",
       "      <td>5.86794</td>\n",
       "      <td>5.96573</td>\n",
       "      <td>6.06352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1724</td>\n",
       "      <td>1840</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.893571</td>\n",
       "      <td>12.88</td>\n",
       "      <td>12.3624</td>\n",
       "      <td>11.8448</td>\n",
       "      <td>11.3273</td>\n",
       "      <td>11.2195</td>\n",
       "      <td>11.1258</td>\n",
       "      <td>11.0322</td>\n",
       "      <td>10.9385</td>\n",
       "      <td>10.8449</td>\n",
       "      <td>10.7512</td>\n",
       "      <td>10.8208</td>\n",
       "      <td>10.972</td>\n",
       "      <td>11.1232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1725</td>\n",
       "      <td>1841</td>\n",
       "      <td>69.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.303333</td>\n",
       "      <td>9.3</td>\n",
       "      <td>8.56906</td>\n",
       "      <td>7.83811</td>\n",
       "      <td>7.10717</td>\n",
       "      <td>6.37622</td>\n",
       "      <td>5.64528</td>\n",
       "      <td>4.91433</td>\n",
       "      <td>4.18339</td>\n",
       "      <td>3.45244</td>\n",
       "      <td>2.7215</td>\n",
       "      <td>1.99055</td>\n",
       "      <td>1.81867</td>\n",
       "      <td>1.81693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1726 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index   Age  event  PSA_average  PSA0m    PSA1m    PSA2m    PSA3m  \\\n",
       "0         1  71.0      1     6.103333   0.81  3.88841  6.96682  9.75615   \n",
       "1         2  63.0      1     4.796667   5.11  4.33632  5.73286  7.91857   \n",
       "2         3  80.0      1     3.126667   4.78  4.69099  4.60198  4.51297   \n",
       "3         4  83.0      1     6.556667  16.72  16.6424  16.5648  16.4872   \n",
       "4         5  76.0      1     2.218333   3.99  4.27609  4.24675  4.21741   \n",
       "...     ...   ...    ...          ...    ...      ...      ...      ...   \n",
       "1721   1837  63.0      0     9.940000   5.53  5.72085  5.91169  6.10254   \n",
       "1722   1838  70.0      0     7.674000   9.53  9.23159  8.93317  8.63476   \n",
       "1723   1839  70.0      0     9.314000   4.89  4.98779  5.08559  5.18338   \n",
       "1724   1840  76.0      0    10.893571  12.88  12.3624  11.8448  11.3273   \n",
       "1725   1841  69.0      0     4.303333    9.3  8.56906  7.83811  7.10717   \n",
       "\n",
       "        PSA4m    PSA5m    PSA6m    PSA7m    PSA8m    PSA9m    PSA10m   PSA11m  \\\n",
       "0     8.49846  7.24077  5.98308  4.72538  3.46769     2.21  0.952308        0   \n",
       "1     10.1043    12.29  14.4757  16.6614  18.8471       20        20       20   \n",
       "2     4.42806   4.3433  4.25854  4.17378  4.08902  4.00425   3.91949  3.83473   \n",
       "3     16.4096   16.332  16.2544  16.1768  16.0992  16.0216    15.944  15.8664   \n",
       "4     4.18807  4.15873   4.1294  4.10006  4.07072  4.04138   4.01204   3.9827   \n",
       "...       ...      ...      ...      ...      ...      ...       ...      ...   \n",
       "1721  6.29339  6.48423  6.67508  6.86593  7.05677  7.24762   7.43847  7.62931   \n",
       "1722  8.33635  8.03794  7.73952  7.75286   7.8998  8.04673   8.19367  8.34061   \n",
       "1723  5.28117  5.37897  5.47676  5.57455  5.67235  5.77014   5.86794  5.96573   \n",
       "1724  11.2195  11.1258  11.0322  10.9385  10.8449  10.7512   10.8208   10.972   \n",
       "1725  6.37622  5.64528  4.91433  4.18339  3.45244   2.7215   1.99055  1.81867   \n",
       "\n",
       "       PSA12m  \n",
       "0           0  \n",
       "1          20  \n",
       "2     3.74997  \n",
       "3     15.7888  \n",
       "4     3.95337  \n",
       "...       ...  \n",
       "1721  7.82016  \n",
       "1722  8.48755  \n",
       "1723  6.06352  \n",
       "1724  11.1232  \n",
       "1725  1.81693  \n",
       "\n",
       "[1726 rows x 17 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####1개월 단위\n",
    "x=np.array(range(0,361,30))\n",
    "PSA_uniform=pd.Series(np.ones(13),index=[\"PSA0m\",\"PSA1m\",\"PSA2m\", \"PSA3m\",\"PSA4m\",\"PSA5m\",\"PSA6m\",\"PSA7m\",\"PSA8m\",\"PSA9m\",\"PSA10m\",\"PSA11m\",\"PSA12m\"])\n",
    "\n",
    "for i in data_drop.index:\n",
    "    f=spi.interp1d(p_day.iloc[i,:int(data_drop.loc[i,\"final_num\"])].values,psa.iloc[i,:int(data_drop.loc[i,\"final_num\"])].values,bounds_error=False,fill_value=\"extrapolate\")\n",
    "    iy=f(x)\n",
    "    PSA_new=pd.Series(iy,index=[\"PSA0m\",\"PSA1m\",\"PSA2m\", \"PSA3m\",\"PSA4m\",\"PSA5m\",\"PSA6m\",\"PSA7m\",\"PSA8m\",\"PSA9m\",\"PSA10m\",\"PSA11m\",\"PSA12m\"])\n",
    "    PSA_uniform=pd.concat([PSA_uniform,PSA_new],axis=1)\n",
    "\n",
    "PSA_uniform_tp=np.transpose(PSA_uniform)\n",
    "PSA_uniform_tp.index=range(0,1727)\n",
    "PSA_uniform_tp=PSA_uniform_tp.drop(0,0)\n",
    "PSA_uniform_tp.index=range(0,1726)\n",
    "\n",
    "PSA_uniform_tp[PSA_uniform_tp>20]=20\n",
    "PSA_uniform_tp[PSA_uniform_tp<0]=0\n",
    "\n",
    "data2=pd.concat([data_drop[[\"index\",\"Age\",\"event\",\"PSA_average\"]],PSA_uniform_tp],axis=1)\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>event</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1167.0</td>\n",
       "      <td>5.704704</td>\n",
       "      <td>4.140810</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>3.155556</td>\n",
       "      <td>4.682500</td>\n",
       "      <td>7.2225</td>\n",
       "      <td>42.557500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>559.0</td>\n",
       "      <td>5.909753</td>\n",
       "      <td>3.934747</td>\n",
       "      <td>0.294286</td>\n",
       "      <td>3.393000</td>\n",
       "      <td>4.796667</td>\n",
       "      <td>7.1700</td>\n",
       "      <td>32.089333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        count      mean       std       min       25%       50%     75%  \\\n",
       "event                                                                     \n",
       "0      1167.0  5.704704  4.140810  0.230000  3.155556  4.682500  7.2225   \n",
       "1       559.0  5.909753  3.934747  0.294286  3.393000  4.796667  7.1700   \n",
       "\n",
       "             max  \n",
       "event             \n",
       "0      42.557500  \n",
       "1      32.089333  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_drop.groupby('event').PSA_average.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of oversampled data is  1642\n",
      "Number of benign subjects in oversampled data 821\n",
      "Number of PCa patients 821\n",
      "Proportion of benign subjects in oversampled data is  0.5\n",
      "Proportion of PCa patients in oversampled data is  0.5\n",
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 6)]               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 15)                105       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 10)                160       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 276\n",
      "Trainable params: 276\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1313 samples, validate on 329 samples\n",
      "Epoch 1/150\n",
      "1313/1313 [==============================] - 1s 695us/sample - loss: 2.4268 - accuracy: 0.4090 - val_loss: 1.0651 - val_accuracy: 0.1125\n",
      "Epoch 2/150\n",
      "1313/1313 [==============================] - 0s 118us/sample - loss: 0.7236 - accuracy: 0.5666 - val_loss: 0.8694 - val_accuracy: 0.3252\n",
      "Epoch 3/150\n",
      "1313/1313 [==============================] - 0s 105us/sample - loss: 0.7047 - accuracy: 0.5781 - val_loss: 1.1539 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/150\n",
      "1313/1313 [==============================] - 0s 102us/sample - loss: 0.6916 - accuracy: 0.5872 - val_loss: 0.8604 - val_accuracy: 0.1337\n",
      "Epoch 5/150\n",
      "1313/1313 [==============================] - 0s 101us/sample - loss: 0.6852 - accuracy: 0.5971 - val_loss: 1.2134 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/150\n",
      "1313/1313 [==============================] - 0s 107us/sample - loss: 0.6777 - accuracy: 0.6078 - val_loss: 1.1721 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/150\n",
      "1313/1313 [==============================] - 0s 103us/sample - loss: 0.6727 - accuracy: 0.6146 - val_loss: 0.9131 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/150\n",
      "1313/1313 [==============================] - 0s 108us/sample - loss: 0.6707 - accuracy: 0.6192 - val_loss: 0.8573 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/150\n",
      "1313/1313 [==============================] - 0s 127us/sample - loss: 0.6671 - accuracy: 0.6222 - val_loss: 1.0878 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/150\n",
      "1313/1313 [==============================] - 0s 101us/sample - loss: 0.6639 - accuracy: 0.6169 - val_loss: 1.1648 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/150\n",
      "1313/1313 [==============================] - 0s 119us/sample - loss: 0.6655 - accuracy: 0.6146 - val_loss: 0.9874 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/150\n",
      "1313/1313 [==============================] - 0s 110us/sample - loss: 0.6623 - accuracy: 0.6192 - val_loss: 0.9313 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/150\n",
      "1313/1313 [==============================] - 0s 98us/sample - loss: 0.6606 - accuracy: 0.6169 - val_loss: 1.0263 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/150\n",
      "1313/1313 [==============================] - 0s 106us/sample - loss: 0.6606 - accuracy: 0.6184 - val_loss: 1.2502 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/150\n",
      "1313/1313 [==============================] - 0s 103us/sample - loss: 0.6604 - accuracy: 0.6276 - val_loss: 0.9360 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/150\n",
      "1313/1313 [==============================] - 0s 137us/sample - loss: 0.6584 - accuracy: 0.6207 - val_loss: 0.8531 - val_accuracy: 0.0091\n",
      "Epoch 17/150\n",
      "1313/1313 [==============================] - 0s 135us/sample - loss: 0.6551 - accuracy: 0.6116 - val_loss: 1.2195 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/150\n",
      "1313/1313 [==============================] - 0s 100us/sample - loss: 0.6560 - accuracy: 0.6207 - val_loss: 0.8334 - val_accuracy: 0.0213\n",
      "Epoch 19/150\n",
      "1313/1313 [==============================] - 0s 127us/sample - loss: 0.6556 - accuracy: 0.6215 - val_loss: 1.0165 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/150\n",
      "1313/1313 [==============================] - 0s 102us/sample - loss: 0.6544 - accuracy: 0.6200 - val_loss: 0.7669 - val_accuracy: 0.1763\n",
      "Epoch 21/150\n",
      "1313/1313 [==============================] - 0s 137us/sample - loss: 0.6549 - accuracy: 0.6245 - val_loss: 1.0135 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/150\n",
      "1313/1313 [==============================] - 0s 120us/sample - loss: 0.6532 - accuracy: 0.6238 - val_loss: 0.9340 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/150\n",
      "1313/1313 [==============================] - 0s 103us/sample - loss: 0.6546 - accuracy: 0.6222 - val_loss: 1.0277 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/150\n",
      "1313/1313 [==============================] - 0s 160us/sample - loss: 0.6522 - accuracy: 0.6268 - val_loss: 0.7750 - val_accuracy: 0.2523\n",
      "Epoch 25/150\n",
      "1313/1313 [==============================] - 0s 134us/sample - loss: 0.6525 - accuracy: 0.6139 - val_loss: 1.0217 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/150\n",
      "1313/1313 [==============================] - 0s 134us/sample - loss: 0.6475 - accuracy: 0.6253 - val_loss: 0.8216 - val_accuracy: 0.1064\n",
      "Epoch 27/150\n",
      "1313/1313 [==============================] - 0s 114us/sample - loss: 0.6492 - accuracy: 0.6238 - val_loss: 0.7024 - val_accuracy: 0.6960\n",
      "Epoch 28/150\n",
      "1313/1313 [==============================] - 0s 116us/sample - loss: 0.6526 - accuracy: 0.6161 - val_loss: 0.9142 - val_accuracy: 0.0091\n",
      "Epoch 29/150\n",
      "1313/1313 [==============================] - 0s 96us/sample - loss: 0.6506 - accuracy: 0.6184 - val_loss: 1.0166 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/150\n",
      "1313/1313 [==============================] - 0s 109us/sample - loss: 0.6499 - accuracy: 0.6291 - val_loss: 0.8883 - val_accuracy: 0.0152\n",
      "Epoch 31/150\n",
      "1313/1313 [==============================] - 0s 126us/sample - loss: 0.6486 - accuracy: 0.6200 - val_loss: 0.9495 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/150\n",
      "1313/1313 [==============================] - 0s 123us/sample - loss: 0.6466 - accuracy: 0.6207 - val_loss: 0.9991 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/150\n",
      "1313/1313 [==============================] - 0s 97us/sample - loss: 0.6471 - accuracy: 0.6276 - val_loss: 0.9238 - val_accuracy: 0.0061\n",
      "Epoch 34/150\n",
      "1313/1313 [==============================] - 0s 101us/sample - loss: 0.6465 - accuracy: 0.6260 - val_loss: 0.8278 - val_accuracy: 0.1246\n",
      "Epoch 35/150\n",
      "1313/1313 [==============================] - 0s 104us/sample - loss: 0.6476 - accuracy: 0.6161 - val_loss: 0.9860 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/150\n",
      "1313/1313 [==============================] - 0s 112us/sample - loss: 0.6459 - accuracy: 0.6230 - val_loss: 0.9134 - val_accuracy: 0.0061\n",
      "Epoch 37/150\n",
      "1313/1313 [==============================] - 0s 118us/sample - loss: 0.6449 - accuracy: 0.6230 - val_loss: 0.9674 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/150\n",
      "1313/1313 [==============================] - 0s 125us/sample - loss: 0.6460 - accuracy: 0.6177 - val_loss: 0.9872 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/150\n",
      "1313/1313 [==============================] - 0s 120us/sample - loss: 0.6459 - accuracy: 0.6215 - val_loss: 1.0073 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/150\n",
      "1313/1313 [==============================] - 0s 113us/sample - loss: 0.6454 - accuracy: 0.6268 - val_loss: 1.1084 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/150\n",
      "1313/1313 [==============================] - 0s 121us/sample - loss: 0.6449 - accuracy: 0.6260 - val_loss: 0.8599 - val_accuracy: 0.0334\n",
      "Epoch 42/150\n",
      "1313/1313 [==============================] - 0s 112us/sample - loss: 0.6452 - accuracy: 0.6238 - val_loss: 1.0373 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/150\n",
      "1313/1313 [==============================] - 0s 104us/sample - loss: 0.6447 - accuracy: 0.6222 - val_loss: 0.9535 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/150\n",
      "1313/1313 [==============================] - 0s 103us/sample - loss: 0.6421 - accuracy: 0.6260 - val_loss: 0.9129 - val_accuracy: 0.0486\n",
      "Epoch 45/150\n",
      "1313/1313 [==============================] - 0s 99us/sample - loss: 0.6440 - accuracy: 0.6215 - val_loss: 0.8713 - val_accuracy: 0.0973\n",
      "Epoch 46/150\n",
      "1313/1313 [==============================] - 0s 145us/sample - loss: 0.6449 - accuracy: 0.6123 - val_loss: 0.9584 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/150\n",
      "1313/1313 [==============================] - 0s 100us/sample - loss: 0.6418 - accuracy: 0.6238 - val_loss: 1.0733 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/150\n",
      "1313/1313 [==============================] - 0s 101us/sample - loss: 0.6424 - accuracy: 0.6321 - val_loss: 0.9775 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/150\n",
      "1313/1313 [==============================] - 0s 100us/sample - loss: 0.6433 - accuracy: 0.6177 - val_loss: 1.0689 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/150\n",
      "1313/1313 [==============================] - 0s 96us/sample - loss: 0.6439 - accuracy: 0.6222 - val_loss: 1.0182 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/150\n",
      "1313/1313 [==============================] - 0s 100us/sample - loss: 0.6419 - accuracy: 0.6146 - val_loss: 0.9453 - val_accuracy: 0.0061\n",
      "Epoch 52/150\n",
      "1313/1313 [==============================] - 0s 102us/sample - loss: 0.6410 - accuracy: 0.6177 - val_loss: 0.9015 - val_accuracy: 0.0699\n",
      "Epoch 53/150\n",
      "1313/1313 [==============================] - 0s 95us/sample - loss: 0.6406 - accuracy: 0.6436 - val_loss: 0.8072 - val_accuracy: 0.2796\n",
      "Epoch 54/150\n",
      "1313/1313 [==============================] - 0s 114us/sample - loss: 0.6400 - accuracy: 0.6344 - val_loss: 0.8932 - val_accuracy: 0.0365\n",
      "Epoch 55/150\n",
      "1313/1313 [==============================] - 0s 109us/sample - loss: 0.6424 - accuracy: 0.6283 - val_loss: 0.9747 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/150\n",
      "1313/1313 [==============================] - 0s 101us/sample - loss: 0.6405 - accuracy: 0.6276 - val_loss: 0.8917 - val_accuracy: 0.0243\n",
      "Epoch 57/150\n",
      "1313/1313 [==============================] - 0s 97us/sample - loss: 0.6406 - accuracy: 0.6283 - val_loss: 0.8010 - val_accuracy: 0.2462\n",
      "Epoch 58/150\n",
      "1313/1313 [==============================] - 0s 107us/sample - loss: 0.6408 - accuracy: 0.6200 - val_loss: 0.7817 - val_accuracy: 0.3404\n",
      "Epoch 59/150\n",
      "1313/1313 [==============================] - 0s 189us/sample - loss: 0.6407 - accuracy: 0.6306 - val_loss: 1.0019 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/150\n",
      "1313/1313 [==============================] - 0s 159us/sample - loss: 0.6395 - accuracy: 0.6276 - val_loss: 0.7899 - val_accuracy: 0.3161\n",
      "Epoch 61/150\n",
      "1313/1313 [==============================] - 0s 174us/sample - loss: 0.6407 - accuracy: 0.6222 - val_loss: 0.9115 - val_accuracy: 0.0182\n",
      "Epoch 62/150\n",
      "1313/1313 [==============================] - 0s 121us/sample - loss: 0.6400 - accuracy: 0.6291 - val_loss: 1.0382 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/150\n",
      "1313/1313 [==============================] - 0s 99us/sample - loss: 0.6380 - accuracy: 0.6306 - val_loss: 1.0423 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/150\n",
      "1313/1313 [==============================] - 0s 103us/sample - loss: 0.6393 - accuracy: 0.6314 - val_loss: 0.9616 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/150\n",
      "1313/1313 [==============================] - 0s 103us/sample - loss: 0.6387 - accuracy: 0.6337 - val_loss: 0.9878 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/150\n",
      "1313/1313 [==============================] - 0s 103us/sample - loss: 0.6370 - accuracy: 0.6329 - val_loss: 1.0824 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/150\n",
      "1313/1313 [==============================] - 0s 99us/sample - loss: 0.6385 - accuracy: 0.6283 - val_loss: 0.9183 - val_accuracy: 0.0456\n",
      "Epoch 68/150\n",
      "1313/1313 [==============================] - 0s 98us/sample - loss: 0.6376 - accuracy: 0.6306 - val_loss: 1.0096 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/150\n",
      "1313/1313 [==============================] - 0s 130us/sample - loss: 0.6389 - accuracy: 0.6291 - val_loss: 0.8838 - val_accuracy: 0.0699\n",
      "Epoch 70/150\n",
      "1313/1313 [==============================] - 0s 95us/sample - loss: 0.6349 - accuracy: 0.6306 - val_loss: 1.2157 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/150\n",
      "1313/1313 [==============================] - 0s 100us/sample - loss: 0.6377 - accuracy: 0.6276 - val_loss: 1.0219 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/150\n",
      "1313/1313 [==============================] - 0s 97us/sample - loss: 0.6386 - accuracy: 0.6299 - val_loss: 0.8825 - val_accuracy: 0.0760\n",
      "Epoch 73/150\n",
      "1313/1313 [==============================] - 0s 96us/sample - loss: 0.6362 - accuracy: 0.6230 - val_loss: 0.9075 - val_accuracy: 0.0365\n",
      "Epoch 74/150\n",
      "1313/1313 [==============================] - 0s 101us/sample - loss: 0.6350 - accuracy: 0.6344 - val_loss: 0.9825 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/150\n",
      "1313/1313 [==============================] - 0s 100us/sample - loss: 0.6360 - accuracy: 0.6299 - val_loss: 0.9443 - val_accuracy: 0.0152\n",
      "Epoch 76/150\n",
      "1313/1313 [==============================] - 0s 100us/sample - loss: 0.6349 - accuracy: 0.6291 - val_loss: 0.9809 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/150\n",
      "1313/1313 [==============================] - 0s 126us/sample - loss: 0.6341 - accuracy: 0.6329 - val_loss: 0.9805 - val_accuracy: 0.0030\n",
      "Epoch 78/150\n",
      "1313/1313 [==============================] - 0s 106us/sample - loss: 0.6337 - accuracy: 0.6253 - val_loss: 0.8092 - val_accuracy: 0.3161\n",
      "Epoch 79/150\n",
      "1313/1313 [==============================] - 0s 98us/sample - loss: 0.6340 - accuracy: 0.6245 - val_loss: 0.8032 - val_accuracy: 0.2948\n",
      "Epoch 80/150\n",
      "1313/1313 [==============================] - 0s 100us/sample - loss: 0.6354 - accuracy: 0.6321 - val_loss: 1.0937 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/150\n",
      "1313/1313 [==============================] - 0s 100us/sample - loss: 0.6334 - accuracy: 0.6405 - val_loss: 0.8042 - val_accuracy: 0.3070\n",
      "Epoch 82/150\n",
      "1313/1313 [==============================] - 0s 102us/sample - loss: 0.6339 - accuracy: 0.6344 - val_loss: 0.9793 - val_accuracy: 0.0152\n",
      "Epoch 83/150\n",
      "1313/1313 [==============================] - 0s 133us/sample - loss: 0.6344 - accuracy: 0.6283 - val_loss: 0.9938 - val_accuracy: 0.0061\n",
      "Epoch 84/150\n",
      "1313/1313 [==============================] - 0s 103us/sample - loss: 0.6344 - accuracy: 0.6329 - val_loss: 1.0466 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/150\n",
      "1313/1313 [==============================] - 0s 135us/sample - loss: 0.6362 - accuracy: 0.6359 - val_loss: 0.9704 - val_accuracy: 0.0122\n",
      "Epoch 86/150\n",
      "1313/1313 [==============================] - 0s 100us/sample - loss: 0.6326 - accuracy: 0.6382 - val_loss: 0.9415 - val_accuracy: 0.0456\n",
      "Epoch 87/150\n",
      "1313/1313 [==============================] - 0s 98us/sample - loss: 0.6337 - accuracy: 0.6253 - val_loss: 0.9983 - val_accuracy: 0.0243\n",
      "Epoch 88/150\n",
      "1313/1313 [==============================] - 0s 97us/sample - loss: 0.6317 - accuracy: 0.6382 - val_loss: 0.9958 - val_accuracy: 0.0304\n",
      "Epoch 89/150\n",
      "1313/1313 [==============================] - 0s 99us/sample - loss: 0.6331 - accuracy: 0.6375 - val_loss: 0.8286 - val_accuracy: 0.2462\n",
      "Epoch 90/150\n",
      "1313/1313 [==============================] - 0s 103us/sample - loss: 0.6329 - accuracy: 0.6291 - val_loss: 1.0838 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/150\n",
      "1313/1313 [==============================] - 0s 100us/sample - loss: 0.6329 - accuracy: 0.6337 - val_loss: 0.7953 - val_accuracy: 0.3374\n",
      "Epoch 92/150\n",
      "1313/1313 [==============================] - 0s 100us/sample - loss: 0.6306 - accuracy: 0.6390 - val_loss: 0.8985 - val_accuracy: 0.0760\n",
      "Epoch 93/150\n",
      "1313/1313 [==============================] - 0s 134us/sample - loss: 0.6311 - accuracy: 0.6352 - val_loss: 0.9838 - val_accuracy: 0.0091\n",
      "Epoch 94/150\n",
      "1313/1313 [==============================] - 0s 113us/sample - loss: 0.6331 - accuracy: 0.6222 - val_loss: 0.9488 - val_accuracy: 0.0243\n",
      "Epoch 95/150\n",
      "1313/1313 [==============================] - 0s 94us/sample - loss: 0.6312 - accuracy: 0.6238 - val_loss: 0.9347 - val_accuracy: 0.0304\n",
      "Epoch 96/150\n",
      "1313/1313 [==============================] - 0s 97us/sample - loss: 0.6328 - accuracy: 0.6276 - val_loss: 1.0124 - val_accuracy: 0.0061\n",
      "Epoch 97/150\n",
      "1313/1313 [==============================] - 0s 96us/sample - loss: 0.6308 - accuracy: 0.6253 - val_loss: 0.8458 - val_accuracy: 0.2188\n",
      "Epoch 98/150\n",
      "1313/1313 [==============================] - 0s 101us/sample - loss: 0.6311 - accuracy: 0.6329 - val_loss: 0.8728 - val_accuracy: 0.1672\n",
      "Epoch 99/150\n",
      "1313/1313 [==============================] - 0s 104us/sample - loss: 0.6312 - accuracy: 0.6337 - val_loss: 0.9053 - val_accuracy: 0.0790\n",
      "Epoch 100/150\n",
      "1313/1313 [==============================] - 0s 103us/sample - loss: 0.6319 - accuracy: 0.6359 - val_loss: 1.0236 - val_accuracy: 0.0061\n",
      "Epoch 101/150\n",
      "1313/1313 [==============================] - 0s 113us/sample - loss: 0.6292 - accuracy: 0.6352 - val_loss: 0.9137 - val_accuracy: 0.0699\n",
      "Epoch 102/150\n",
      "1313/1313 [==============================] - 0s 117us/sample - loss: 0.6308 - accuracy: 0.6291 - val_loss: 0.9165 - val_accuracy: 0.1003\n",
      "Epoch 103/150\n",
      "1313/1313 [==============================] - 0s 98us/sample - loss: 0.6289 - accuracy: 0.6276 - val_loss: 0.8476 - val_accuracy: 0.2310\n",
      "Epoch 104/150\n",
      "1313/1313 [==============================] - 0s 106us/sample - loss: 0.6312 - accuracy: 0.6299 - val_loss: 0.9606 - val_accuracy: 0.0304\n",
      "Epoch 105/150\n",
      "1313/1313 [==============================] - 0s 115us/sample - loss: 0.6297 - accuracy: 0.6413 - val_loss: 0.8861 - val_accuracy: 0.1307\n",
      "Epoch 106/150\n",
      "1313/1313 [==============================] - 0s 100us/sample - loss: 0.6282 - accuracy: 0.6398 - val_loss: 0.9481 - val_accuracy: 0.0395\n",
      "Epoch 107/150\n",
      "1313/1313 [==============================] - 0s 99us/sample - loss: 0.6294 - accuracy: 0.6451 - val_loss: 0.9473 - val_accuracy: 0.0669\n",
      "Epoch 108/150\n",
      "1313/1313 [==============================] - 0s 99us/sample - loss: 0.6300 - accuracy: 0.6321 - val_loss: 0.8918 - val_accuracy: 0.1489\n",
      "Epoch 109/150\n",
      "1313/1313 [==============================] - 0s 103us/sample - loss: 0.6272 - accuracy: 0.6299 - val_loss: 0.8742 - val_accuracy: 0.1672\n",
      "Epoch 110/150\n",
      "1313/1313 [==============================] - 0s 120us/sample - loss: 0.6299 - accuracy: 0.6314 - val_loss: 0.7960 - val_accuracy: 0.3921\n",
      "Epoch 111/150\n",
      "1313/1313 [==============================] - 0s 95us/sample - loss: 0.6284 - accuracy: 0.6413 - val_loss: 0.9450 - val_accuracy: 0.0851\n",
      "Epoch 112/150\n",
      "1313/1313 [==============================] - 0s 96us/sample - loss: 0.6275 - accuracy: 0.6375 - val_loss: 0.9255 - val_accuracy: 0.1277\n",
      "Epoch 113/150\n",
      "1313/1313 [==============================] - 0s 100us/sample - loss: 0.6302 - accuracy: 0.6291 - val_loss: 0.9024 - val_accuracy: 0.1277\n",
      "Epoch 114/150\n",
      "1313/1313 [==============================] - 0s 103us/sample - loss: 0.6279 - accuracy: 0.6405 - val_loss: 0.8755 - val_accuracy: 0.2036\n",
      "Epoch 115/150\n",
      "1313/1313 [==============================] - 0s 100us/sample - loss: 0.6273 - accuracy: 0.6382 - val_loss: 1.0971 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/150\n",
      "1313/1313 [==============================] - 0s 99us/sample - loss: 0.6278 - accuracy: 0.6344 - val_loss: 0.9542 - val_accuracy: 0.0790\n",
      "Epoch 117/150\n",
      "1313/1313 [==============================] - 0s 100us/sample - loss: 0.6270 - accuracy: 0.6420 - val_loss: 1.0101 - val_accuracy: 0.0091\n",
      "Epoch 118/150\n",
      "1313/1313 [==============================] - 0s 125us/sample - loss: 0.6276 - accuracy: 0.6436 - val_loss: 0.9322 - val_accuracy: 0.1064\n",
      "Epoch 119/150\n",
      "1313/1313 [==============================] - 0s 98us/sample - loss: 0.6286 - accuracy: 0.6329 - val_loss: 0.9673 - val_accuracy: 0.0365\n",
      "Epoch 120/150\n",
      "1313/1313 [==============================] - 0s 98us/sample - loss: 0.6263 - accuracy: 0.6466 - val_loss: 0.8808 - val_accuracy: 0.2036\n",
      "Epoch 121/150\n",
      "1313/1313 [==============================] - 0s 97us/sample - loss: 0.6262 - accuracy: 0.6321 - val_loss: 0.7698 - val_accuracy: 0.5015\n",
      "Epoch 122/150\n",
      "1313/1313 [==============================] - 0s 99us/sample - loss: 0.6266 - accuracy: 0.6413 - val_loss: 0.9407 - val_accuracy: 0.1246\n",
      "Epoch 123/150\n",
      "1313/1313 [==============================] - 0s 98us/sample - loss: 0.6282 - accuracy: 0.6375 - val_loss: 0.8990 - val_accuracy: 0.1854\n",
      "Epoch 124/150\n",
      "1313/1313 [==============================] - 0s 103us/sample - loss: 0.6279 - accuracy: 0.6253 - val_loss: 0.8812 - val_accuracy: 0.1976\n",
      "Epoch 125/150\n",
      "1313/1313 [==============================] - 0s 100us/sample - loss: 0.6264 - accuracy: 0.6451 - val_loss: 1.0001 - val_accuracy: 0.0486\n",
      "Epoch 126/150\n",
      "1313/1313 [==============================] - 0s 143us/sample - loss: 0.6264 - accuracy: 0.6283 - val_loss: 0.9481 - val_accuracy: 0.0669\n",
      "Epoch 127/150\n",
      "1313/1313 [==============================] - 0s 150us/sample - loss: 0.6249 - accuracy: 0.6306 - val_loss: 0.9353 - val_accuracy: 0.1033\n",
      "Epoch 128/150\n",
      "1313/1313 [==============================] - 0s 138us/sample - loss: 0.6249 - accuracy: 0.6443 - val_loss: 0.8092 - val_accuracy: 0.3921\n",
      "Epoch 129/150\n",
      "1313/1313 [==============================] - 0s 137us/sample - loss: 0.6246 - accuracy: 0.6344 - val_loss: 0.9064 - val_accuracy: 0.1398\n",
      "Epoch 130/150\n",
      "1313/1313 [==============================] - 0s 137us/sample - loss: 0.6255 - accuracy: 0.6428 - val_loss: 0.9589 - val_accuracy: 0.1125\n",
      "Epoch 131/150\n",
      "1313/1313 [==============================] - 0s 103us/sample - loss: 0.6246 - accuracy: 0.6352 - val_loss: 1.0361 - val_accuracy: 0.0182\n",
      "Epoch 132/150\n",
      "1313/1313 [==============================] - 0s 103us/sample - loss: 0.6255 - accuracy: 0.6314 - val_loss: 0.9693 - val_accuracy: 0.0638\n",
      "Epoch 133/150\n",
      "1313/1313 [==============================] - 0s 123us/sample - loss: 0.6244 - accuracy: 0.6398 - val_loss: 0.7517 - val_accuracy: 0.5289\n",
      "Epoch 134/150\n",
      "1313/1313 [==============================] - 0s 115us/sample - loss: 0.6254 - accuracy: 0.6291 - val_loss: 1.0157 - val_accuracy: 0.0152\n",
      "Epoch 135/150\n",
      "1313/1313 [==============================] - 0s 103us/sample - loss: 0.6247 - accuracy: 0.6390 - val_loss: 0.9297 - val_accuracy: 0.1277\n",
      "Epoch 136/150\n",
      "1313/1313 [==============================] - 0s 120us/sample - loss: 0.6249 - accuracy: 0.6329 - val_loss: 1.0246 - val_accuracy: 0.0091\n",
      "Epoch 137/150\n",
      "1313/1313 [==============================] - 0s 103us/sample - loss: 0.6245 - accuracy: 0.6451 - val_loss: 0.9577 - val_accuracy: 0.0608\n",
      "Epoch 138/150\n",
      "1313/1313 [==============================] - 0s 96us/sample - loss: 0.6247 - accuracy: 0.6375 - val_loss: 0.8500 - val_accuracy: 0.2796\n",
      "Epoch 139/150\n",
      "1313/1313 [==============================] - 0s 100us/sample - loss: 0.6246 - accuracy: 0.6359 - val_loss: 0.8614 - val_accuracy: 0.2523\n",
      "Epoch 140/150\n",
      "1313/1313 [==============================] - 0s 100us/sample - loss: 0.6234 - accuracy: 0.6451 - val_loss: 0.9665 - val_accuracy: 0.0669\n",
      "Epoch 141/150\n",
      "1313/1313 [==============================] - 0s 128us/sample - loss: 0.6226 - accuracy: 0.6359 - val_loss: 0.8668 - val_accuracy: 0.2401\n",
      "Epoch 142/150\n",
      "1313/1313 [==============================] - 0s 113us/sample - loss: 0.6229 - accuracy: 0.6443 - val_loss: 0.8912 - val_accuracy: 0.2036\n",
      "Epoch 143/150\n",
      "1313/1313 [==============================] - 0s 98us/sample - loss: 0.6236 - accuracy: 0.6291 - val_loss: 0.9481 - val_accuracy: 0.1307\n",
      "Epoch 144/150\n",
      "1313/1313 [==============================] - 0s 100us/sample - loss: 0.6234 - accuracy: 0.6398 - val_loss: 0.9868 - val_accuracy: 0.0578\n",
      "Epoch 145/150\n",
      "1313/1313 [==============================] - 0s 110us/sample - loss: 0.6231 - accuracy: 0.6283 - val_loss: 1.0179 - val_accuracy: 0.0517\n",
      "Epoch 146/150\n",
      "1313/1313 [==============================] - 0s 101us/sample - loss: 0.6228 - accuracy: 0.6337 - val_loss: 0.9928 - val_accuracy: 0.0517\n",
      "Epoch 147/150\n",
      "1313/1313 [==============================] - 0s 127us/sample - loss: 0.6220 - accuracy: 0.6436 - val_loss: 1.0208 - val_accuracy: 0.0486\n",
      "Epoch 148/150\n",
      "1313/1313 [==============================] - 0s 99us/sample - loss: 0.6213 - accuracy: 0.6382 - val_loss: 0.9916 - val_accuracy: 0.0395\n",
      "Epoch 149/150\n",
      "1313/1313 [==============================] - 0s 131us/sample - loss: 0.6234 - accuracy: 0.6314 - val_loss: 0.9816 - val_accuracy: 0.0669\n",
      "Epoch 150/150\n",
      "1313/1313 [==============================] - 0s 98us/sample - loss: 0.6209 - accuracy: 0.6527 - val_loss: 1.0392 - val_accuracy: 0.0122\n",
      "518/1 [====================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 25us/sample - loss: 0.5136 - accuracy: 0.6660\n",
      "Test Score: 0.6298345349240027\n",
      "Test Accuracy: 0.6660232\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign       0.67      0.98      0.80       346\n",
      "         PCa       0.46      0.03      0.06       172\n",
      "\n",
      "    accuracy                           0.67       518\n",
      "   macro avg       0.57      0.51      0.43       518\n",
      "weighted avg       0.60      0.67      0.55       518\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "for i in data0.index:\n",
    "    data0.loc[i,\"PSA_delta1\"]=data0.loc[i,\"PSA270\"]-data0.loc[i,\"PSA360\"]\n",
    "    data0.loc[i,\"PSA_delta2\"]=data0.loc[i,\"PSA180\"]-data0.loc[i,\"PSA270\"]\n",
    "    data0.loc[i,\"PSA_delta3\"]=data0.loc[i,\"PSA90\"]-data0.loc[i,\"PSA180\"]\n",
    "    data0.loc[i,\"PSA_delta4\"]=data0.loc[i,\"PSA0\"]-data0.loc[i,\"PSA90\"]\n",
    "\n",
    "data0.loc[:,\"age\"]=data0.loc[:,\"Age\"]\n",
    "data0.loc[:,\"PSA_Average\"]=data0.loc[:,\"PSA_average\"]\n",
    "#data0.loc[:,\"Index\"]=data0.loc[:,\"index\"]\n",
    "\n",
    "X=data0.loc[:,\"PSA360\":]\n",
    "X=X.astype(np.float32)\n",
    "y=data0.event\n",
    "\n",
    "#conda install -c conda-forge imbalanced-learn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "os = SMOTE(random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "X_train2=X_train.iloc[:,5:]\n",
    "X_test2=X_test.iloc[:,5:]\n",
    "\n",
    "columns = X_train2.columns\n",
    "os_data_X,os_data_y=os.fit_sample(X_train2, y_train)\n",
    "os_data_X = pd.DataFrame(data=os_data_X,columns=columns )\n",
    "os_data_y= pd.DataFrame(data=os_data_y,columns=['y'])\n",
    "# we can Check the numbers of our data\n",
    "print(\"length of oversampled data is \",len(os_data_X))\n",
    "print(\"Number of benign subjects in oversampled data\",len(os_data_y[os_data_y['y']==0]))\n",
    "print(\"Number of PCa patients\",len(os_data_y[os_data_y['y']==1]))\n",
    "print(\"Proportion of benign subjects in oversampled data is \",len(os_data_y[os_data_y['y']==0])/len(os_data_X))\n",
    "print(\"Proportion of PCa patients in oversampled data is \",len(os_data_y[os_data_y['y']==1])/len(os_data_X))\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Activation,Dropout,LSTM,Bidirectional\n",
    "from tensorflow.keras.models import Model,Sequential\n",
    "from tensorflow.keras.optimizers import Adadelta, SGD, RMSprop, Adam,Adamax\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "X_train2=os_data_X.values\n",
    "X_test2=X_test2.values\n",
    "y_train=os_data_y.values\n",
    "y_test=y_test.values\n",
    "\n",
    "input_layer = Input(shape=(X_train2.shape[1],))\n",
    "dense_layer_1 = Dense(15, activation='relu')(input_layer)\n",
    "dense_layer_2 = Dense(10, activation='relu')(dense_layer_1)\n",
    "output = Dense(1, activation='sigmoid')(dense_layer_2)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'RMSprop', metrics = ['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "history = model.fit(X_train2, y_train, batch_size=20, epochs=150, verbose=1, validation_split=0.2)\n",
    "score = model.evaluate(X_test2, y_test, verbose=1)\n",
    "\n",
    "print(\"Test Score:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "\n",
    "y_pred = model.predict(X_test2)\n",
    "y_pred=(y_pred>0.5)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred, target_names=['benign', 'PCa']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 38.15%\n",
      "Precision: 33.96%\n",
      "Recall: 96.43%\n",
      "accuracy: 34.10%\n",
      "Precision: 31.41%\n",
      "Recall: 87.50%\n",
      "accuracy: 38.73%\n",
      "Precision: 33.77%\n",
      "Recall: 92.86%\n",
      "accuracy: 56.07%\n",
      "Precision: 38.37%\n",
      "Recall: 58.93%\n",
      "accuracy: 37.57%\n",
      "Precision: 33.95%\n",
      "Recall: 98.21%\n",
      "accuracy: 39.88%\n",
      "Precision: 34.81%\n",
      "Recall: 98.21%\n",
      "accuracy: 64.74%\n",
      "Precision: 40.74%\n",
      "Recall: 19.64%\n",
      "accuracy: 38.95%\n",
      "Precision: 33.56%\n",
      "Recall: 89.29%\n",
      "accuracy: 37.79%\n",
      "Precision: 33.76%\n",
      "Recall: 94.64%\n",
      "accuracy: 40.35%\n",
      "Precision: 34.64%\n",
      "Recall: 96.36%\n",
      "accuracy:  42.63% (+/- 9.23%)\n",
      "precision:  34.90% (+/- 2.54%)\n",
      "recall:  83.21% (+/- 23.89%)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "for i in data0.index:\n",
    "    data0.loc[i,\"PSA_delta1\"]=data0.loc[i,\"PSA270\"]-data0.loc[i,\"PSA360\"]\n",
    "    data0.loc[i,\"PSA_delta2\"]=data0.loc[i,\"PSA180\"]-data0.loc[i,\"PSA270\"]\n",
    "    data0.loc[i,\"PSA_delta3\"]=data0.loc[i,\"PSA90\"]-data0.loc[i,\"PSA180\"]\n",
    "    data0.loc[i,\"PSA_delta4\"]=data0.loc[i,\"PSA0\"]-data0.loc[i,\"PSA90\"]\n",
    "\n",
    "\n",
    "data0.loc[:,\"age\"]=data0.loc[:,\"Age\"]\n",
    "data0.loc[:,\"PSA_Average\"]=data0.loc[:,\"PSA_average\"]\n",
    "\n",
    "y=data0.event.values\n",
    "\n",
    "X=data0.iloc[:,4:].values\n",
    "X=X.astype(np.float32)\n",
    "X2=X[:,5:]\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Activation,Dropout\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adadelta, SGD, RMSprop, Adam,Adamax\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold,cross_val_score\n",
    "#https://keras.io/optimizers/\n",
    "\n",
    "# define 10-fold cross validation test harness\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "acc = []\n",
    "prec=[]\n",
    "rec=[]\n",
    "for train, test in kfold.split(X2, y):\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    os = SMOTE(random_state=0)\n",
    "\n",
    "    X_train2=X2[train][:,5:]\n",
    "    X_test2=X2[test][:,5:]\n",
    "\n",
    "    os_data_X,os_data_y=os.fit_sample(X_train2, y[train])\n",
    "    \n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(15, input_dim=os_data_X.shape[1], activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='RMSprop', metrics=['accuracy','Precision','Recall'])\n",
    "    # Fit the model\n",
    "    model.fit(os_data_X, os_data_y, epochs=150, batch_size=10, verbose=0)\n",
    "    # evaluate the model\n",
    "    scores = model.evaluate(X_test2, y[test], verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[2], scores[2]*100))\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[3], scores[3]*100))\n",
    "    acc.append(scores[1] * 100)\n",
    "    prec.append(scores[2] * 100)\n",
    "    rec.append(scores[3]*100)\n",
    "print('accuracy: ',\"%.2f%% (+/- %.2f%%)\" % (np.mean(acc), np.std(acc)))\n",
    "print('precision: ',\"%.2f%% (+/- %.2f%%)\" % (np.mean(prec), np.std(prec)))\n",
    "print('recall: ',\"%.2f%% (+/- %.2f%%)\" % (np.mean(rec), np.std(rec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "datetime 형변환 등 다루는 법: https://devanix.tistory.com/306 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1. Logistic Regression - PSA_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 15)                120       \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 10)                160       \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 291\n",
      "Trainable params: 291\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1397 samples, validate on 156 samples\n",
      "Epoch 1/150\n",
      "1397/1397 [==============================] - 5s 4ms/sample - loss: 0.9784 - accuracy: 0.5719 - val_loss: 0.7032 - val_accuracy: 0.7115\n",
      "Epoch 2/150\n",
      "1397/1397 [==============================] - 0s 112us/sample - loss: 0.7115 - accuracy: 0.6500 - val_loss: 0.6495 - val_accuracy: 0.6667\n",
      "Epoch 3/150\n",
      "1397/1397 [==============================] - 0s 143us/sample - loss: 0.6932 - accuracy: 0.6485 - val_loss: 0.6425 - val_accuracy: 0.6859\n",
      "Epoch 4/150\n",
      "1397/1397 [==============================] - 0s 108us/sample - loss: 0.6856 - accuracy: 0.6578 - val_loss: 0.7263 - val_accuracy: 0.5833\n",
      "Epoch 5/150\n",
      "1397/1397 [==============================] - 0s 111us/sample - loss: 0.6756 - accuracy: 0.6557 - val_loss: 0.6411 - val_accuracy: 0.6603\n",
      "Epoch 6/150\n",
      "1397/1397 [==============================] - 0s 106us/sample - loss: 0.6646 - accuracy: 0.6528 - val_loss: 0.6370 - val_accuracy: 0.7051\n",
      "Epoch 7/150\n",
      "1397/1397 [==============================] - 0s 122us/sample - loss: 0.6639 - accuracy: 0.6671 - val_loss: 0.6653 - val_accuracy: 0.6154\n",
      "Epoch 8/150\n",
      "1397/1397 [==============================] - 0s 124us/sample - loss: 0.6630 - accuracy: 0.6614 - val_loss: 0.6817 - val_accuracy: 0.6026\n",
      "Epoch 9/150\n",
      "1397/1397 [==============================] - 0s 116us/sample - loss: 0.6598 - accuracy: 0.6500 - val_loss: 0.6568 - val_accuracy: 0.6987\n",
      "Epoch 10/150\n",
      "1397/1397 [==============================] - 0s 123us/sample - loss: 0.6436 - accuracy: 0.6686 - val_loss: 0.6088 - val_accuracy: 0.7051\n",
      "Epoch 11/150\n",
      "1397/1397 [==============================] - 0s 109us/sample - loss: 0.6442 - accuracy: 0.6679 - val_loss: 0.6144 - val_accuracy: 0.6731\n",
      "Epoch 12/150\n",
      "1397/1397 [==============================] - 0s 111us/sample - loss: 0.6417 - accuracy: 0.6586 - val_loss: 0.6421 - val_accuracy: 0.6026\n",
      "Epoch 13/150\n",
      "1397/1397 [==============================] - 0s 110us/sample - loss: 0.6469 - accuracy: 0.6657 - val_loss: 0.8263 - val_accuracy: 0.4103\n",
      "Epoch 14/150\n",
      "1397/1397 [==============================] - 0s 107us/sample - loss: 0.6383 - accuracy: 0.6643 - val_loss: 0.6083 - val_accuracy: 0.6859\n",
      "Epoch 15/150\n",
      "1397/1397 [==============================] - 0s 119us/sample - loss: 0.6373 - accuracy: 0.6607 - val_loss: 0.6241 - val_accuracy: 0.6346\n",
      "Epoch 16/150\n",
      "1397/1397 [==============================] - 0s 112us/sample - loss: 0.6410 - accuracy: 0.6707 - val_loss: 0.6043 - val_accuracy: 0.7115\n",
      "Epoch 17/150\n",
      "1397/1397 [==============================] - 0s 121us/sample - loss: 0.6349 - accuracy: 0.6750 - val_loss: 0.6794 - val_accuracy: 0.5577\n",
      "Epoch 18/150\n",
      "1397/1397 [==============================] - 0s 110us/sample - loss: 0.6399 - accuracy: 0.6628 - val_loss: 0.5968 - val_accuracy: 0.6987\n",
      "Epoch 19/150\n",
      "1397/1397 [==============================] - 0s 111us/sample - loss: 0.6336 - accuracy: 0.6478 - val_loss: 0.5967 - val_accuracy: 0.6987\n",
      "Epoch 20/150\n",
      "1397/1397 [==============================] - 0s 115us/sample - loss: 0.6351 - accuracy: 0.6686 - val_loss: 0.6145 - val_accuracy: 0.6603\n",
      "Epoch 21/150\n",
      "1397/1397 [==============================] - 0s 108us/sample - loss: 0.6360 - accuracy: 0.6736 - val_loss: 0.6091 - val_accuracy: 0.7051\n",
      "Epoch 22/150\n",
      "1397/1397 [==============================] - 0s 109us/sample - loss: 0.6262 - accuracy: 0.6643 - val_loss: 0.6181 - val_accuracy: 0.7051\n",
      "Epoch 23/150\n",
      "1397/1397 [==============================] - 0s 118us/sample - loss: 0.6336 - accuracy: 0.6707 - val_loss: 0.5904 - val_accuracy: 0.6923\n",
      "Epoch 24/150\n",
      "1397/1397 [==============================] - 0s 116us/sample - loss: 0.6300 - accuracy: 0.6621 - val_loss: 0.5925 - val_accuracy: 0.6987\n",
      "Epoch 25/150\n",
      "1397/1397 [==============================] - 0s 109us/sample - loss: 0.6312 - accuracy: 0.6628 - val_loss: 0.5981 - val_accuracy: 0.6987\n",
      "Epoch 26/150\n",
      "1397/1397 [==============================] - 0s 113us/sample - loss: 0.6273 - accuracy: 0.6614 - val_loss: 0.6040 - val_accuracy: 0.6987\n",
      "Epoch 27/150\n",
      "1397/1397 [==============================] - 0s 129us/sample - loss: 0.6250 - accuracy: 0.6736 - val_loss: 0.5923 - val_accuracy: 0.6987\n",
      "Epoch 28/150\n",
      "1397/1397 [==============================] - 0s 115us/sample - loss: 0.6308 - accuracy: 0.6586 - val_loss: 0.6201 - val_accuracy: 0.7115\n",
      "Epoch 29/150\n",
      "1397/1397 [==============================] - 0s 118us/sample - loss: 0.6316 - accuracy: 0.6628 - val_loss: 0.6116 - val_accuracy: 0.6923\n",
      "Epoch 30/150\n",
      "1397/1397 [==============================] - 0s 137us/sample - loss: 0.6258 - accuracy: 0.6700 - val_loss: 0.5909 - val_accuracy: 0.7051\n",
      "Epoch 31/150\n",
      "1397/1397 [==============================] - 0s 110us/sample - loss: 0.6238 - accuracy: 0.6643 - val_loss: 0.5953 - val_accuracy: 0.6987\n",
      "Epoch 32/150\n",
      "1397/1397 [==============================] - 0s 108us/sample - loss: 0.6288 - accuracy: 0.6700 - val_loss: 0.6158 - val_accuracy: 0.7115\n",
      "Epoch 33/150\n",
      "1397/1397 [==============================] - 0s 110us/sample - loss: 0.6272 - accuracy: 0.6614 - val_loss: 0.5981 - val_accuracy: 0.6987\n",
      "Epoch 34/150\n",
      "1397/1397 [==============================] - 0s 116us/sample - loss: 0.6247 - accuracy: 0.6593 - val_loss: 0.6382 - val_accuracy: 0.6026\n",
      "Epoch 35/150\n",
      "1397/1397 [==============================] - 0s 109us/sample - loss: 0.6163 - accuracy: 0.6693 - val_loss: 0.6029 - val_accuracy: 0.6859\n",
      "Epoch 36/150\n",
      "1397/1397 [==============================] - 0s 111us/sample - loss: 0.6254 - accuracy: 0.6621 - val_loss: 0.5898 - val_accuracy: 0.7115\n",
      "Epoch 37/150\n",
      "1397/1397 [==============================] - 0s 121us/sample - loss: 0.6289 - accuracy: 0.6657 - val_loss: 0.7210 - val_accuracy: 0.5128\n",
      "Epoch 38/150\n",
      "1397/1397 [==============================] - 0s 114us/sample - loss: 0.6166 - accuracy: 0.6686 - val_loss: 0.5907 - val_accuracy: 0.7115\n",
      "Epoch 39/150\n",
      "1397/1397 [==============================] - 0s 108us/sample - loss: 0.6224 - accuracy: 0.6636 - val_loss: 0.6041 - val_accuracy: 0.6923\n",
      "Epoch 40/150\n",
      "1397/1397 [==============================] - 0s 134us/sample - loss: 0.6196 - accuracy: 0.6636 - val_loss: 0.5983 - val_accuracy: 0.7115\n",
      "Epoch 41/150\n",
      "1397/1397 [==============================] - 0s 110us/sample - loss: 0.6214 - accuracy: 0.6643 - val_loss: 0.6044 - val_accuracy: 0.6923\n",
      "Epoch 42/150\n",
      "1397/1397 [==============================] - 0s 116us/sample - loss: 0.6208 - accuracy: 0.6607 - val_loss: 0.6192 - val_accuracy: 0.7115\n",
      "Epoch 43/150\n",
      "1397/1397 [==============================] - 0s 109us/sample - loss: 0.6228 - accuracy: 0.6671 - val_loss: 0.6138 - val_accuracy: 0.6731\n",
      "Epoch 44/150\n",
      "1397/1397 [==============================] - 0s 125us/sample - loss: 0.6221 - accuracy: 0.6729 - val_loss: 0.6079 - val_accuracy: 0.6987\n",
      "Epoch 45/150\n",
      "1397/1397 [==============================] - 0s 111us/sample - loss: 0.6187 - accuracy: 0.6636 - val_loss: 0.6374 - val_accuracy: 0.7115\n",
      "Epoch 46/150\n",
      "1397/1397 [==============================] - 0s 121us/sample - loss: 0.6221 - accuracy: 0.6700 - val_loss: 0.6004 - val_accuracy: 0.7115\n",
      "Epoch 47/150\n",
      "1397/1397 [==============================] - 0s 108us/sample - loss: 0.6237 - accuracy: 0.6793 - val_loss: 0.6176 - val_accuracy: 0.6667\n",
      "Epoch 48/150\n",
      "1397/1397 [==============================] - 0s 107us/sample - loss: 0.6243 - accuracy: 0.6578 - val_loss: 0.6219 - val_accuracy: 0.6538\n",
      "Epoch 49/150\n",
      "1397/1397 [==============================] - 0s 110us/sample - loss: 0.6202 - accuracy: 0.6786 - val_loss: 0.6619 - val_accuracy: 0.7115\n",
      "Epoch 50/150\n",
      "1397/1397 [==============================] - 0s 123us/sample - loss: 0.6170 - accuracy: 0.6729 - val_loss: 0.6294 - val_accuracy: 0.6410\n",
      "Epoch 51/150\n",
      "1397/1397 [==============================] - 0s 115us/sample - loss: 0.6210 - accuracy: 0.6643 - val_loss: 0.5972 - val_accuracy: 0.7179\n",
      "Epoch 52/150\n",
      "1397/1397 [==============================] - 0s 115us/sample - loss: 0.6181 - accuracy: 0.6671 - val_loss: 0.6142 - val_accuracy: 0.6987\n",
      "Epoch 53/150\n",
      "1397/1397 [==============================] - 0s 121us/sample - loss: 0.6188 - accuracy: 0.6628 - val_loss: 0.6851 - val_accuracy: 0.5256\n",
      "Epoch 54/150\n",
      "1397/1397 [==============================] - 0s 111us/sample - loss: 0.6147 - accuracy: 0.6671 - val_loss: 0.6018 - val_accuracy: 0.7051\n",
      "Epoch 55/150\n",
      "1397/1397 [==============================] - 0s 106us/sample - loss: 0.6144 - accuracy: 0.6722 - val_loss: 0.5883 - val_accuracy: 0.7115\n",
      "Epoch 56/150\n",
      "1397/1397 [==============================] - 0s 117us/sample - loss: 0.6212 - accuracy: 0.6657 - val_loss: 0.6045 - val_accuracy: 0.6731\n",
      "Epoch 57/150\n",
      "1397/1397 [==============================] - 0s 125us/sample - loss: 0.6134 - accuracy: 0.6679 - val_loss: 0.5939 - val_accuracy: 0.7179\n",
      "Epoch 58/150\n",
      "1397/1397 [==============================] - 0s 109us/sample - loss: 0.6132 - accuracy: 0.6729 - val_loss: 0.7210 - val_accuracy: 0.4936\n",
      "Epoch 59/150\n",
      "1397/1397 [==============================] - 0s 109us/sample - loss: 0.6171 - accuracy: 0.6700 - val_loss: 0.5834 - val_accuracy: 0.7115\n",
      "Epoch 60/150\n",
      "1397/1397 [==============================] - 0s 110us/sample - loss: 0.6171 - accuracy: 0.6707 - val_loss: 0.5857 - val_accuracy: 0.7115\n",
      "Epoch 61/150\n",
      "1397/1397 [==============================] - 0s 106us/sample - loss: 0.6192 - accuracy: 0.6686 - val_loss: 0.6485 - val_accuracy: 0.6218\n",
      "Epoch 62/150\n",
      "1397/1397 [==============================] - 0s 107us/sample - loss: 0.6118 - accuracy: 0.6743 - val_loss: 0.6087 - val_accuracy: 0.7051\n",
      "Epoch 63/150\n",
      "1397/1397 [==============================] - 0s 104us/sample - loss: 0.6133 - accuracy: 0.6722 - val_loss: 0.6291 - val_accuracy: 0.6346\n",
      "Epoch 64/150\n",
      "1397/1397 [==============================] - 0s 129us/sample - loss: 0.6160 - accuracy: 0.6772 - val_loss: 0.6483 - val_accuracy: 0.6218\n",
      "Epoch 65/150\n",
      "1397/1397 [==============================] - 0s 131us/sample - loss: 0.6170 - accuracy: 0.6664 - val_loss: 0.6031 - val_accuracy: 0.7051\n",
      "Epoch 66/150\n",
      "1397/1397 [==============================] - 0s 111us/sample - loss: 0.6117 - accuracy: 0.6693 - val_loss: 0.5927 - val_accuracy: 0.7051\n",
      "Epoch 67/150\n",
      "1397/1397 [==============================] - 0s 109us/sample - loss: 0.6162 - accuracy: 0.6586 - val_loss: 0.6084 - val_accuracy: 0.7051\n",
      "Epoch 68/150\n",
      "1397/1397 [==============================] - 0s 107us/sample - loss: 0.6119 - accuracy: 0.6807 - val_loss: 0.6081 - val_accuracy: 0.7051\n",
      "Epoch 69/150\n",
      "1397/1397 [==============================] - 0s 106us/sample - loss: 0.6181 - accuracy: 0.6628 - val_loss: 0.6080 - val_accuracy: 0.6987\n",
      "Epoch 70/150\n",
      "1397/1397 [==============================] - 0s 112us/sample - loss: 0.6114 - accuracy: 0.6650 - val_loss: 0.6117 - val_accuracy: 0.6923\n",
      "Epoch 71/150\n",
      "1397/1397 [==============================] - 0s 131us/sample - loss: 0.6161 - accuracy: 0.6700 - val_loss: 0.6701 - val_accuracy: 0.5705\n",
      "Epoch 72/150\n",
      "1397/1397 [==============================] - 0s 121us/sample - loss: 0.6118 - accuracy: 0.6664 - val_loss: 0.6132 - val_accuracy: 0.6923\n",
      "Epoch 73/150\n",
      "1397/1397 [==============================] - 0s 112us/sample - loss: 0.6086 - accuracy: 0.6915 - val_loss: 0.5963 - val_accuracy: 0.7051\n",
      "Epoch 74/150\n",
      "1397/1397 [==============================] - 0s 107us/sample - loss: 0.6130 - accuracy: 0.6693 - val_loss: 0.6160 - val_accuracy: 0.7051\n",
      "Epoch 75/150\n",
      "1397/1397 [==============================] - 0s 113us/sample - loss: 0.6128 - accuracy: 0.6743 - val_loss: 0.6023 - val_accuracy: 0.6923\n",
      "Epoch 76/150\n",
      "1397/1397 [==============================] - 0s 109us/sample - loss: 0.6150 - accuracy: 0.6714 - val_loss: 0.6092 - val_accuracy: 0.7051\n",
      "Epoch 77/150\n",
      "1397/1397 [==============================] - 0s 118us/sample - loss: 0.6150 - accuracy: 0.6679 - val_loss: 0.6471 - val_accuracy: 0.6154\n",
      "Epoch 78/150\n",
      "1397/1397 [==============================] - 0s 132us/sample - loss: 0.6123 - accuracy: 0.6757 - val_loss: 0.5984 - val_accuracy: 0.6987\n",
      "Epoch 79/150\n",
      "1397/1397 [==============================] - 0s 111us/sample - loss: 0.6120 - accuracy: 0.6807 - val_loss: 0.6089 - val_accuracy: 0.6923\n",
      "Epoch 80/150\n",
      "1397/1397 [==============================] - 0s 110us/sample - loss: 0.6138 - accuracy: 0.6800 - val_loss: 0.6045 - val_accuracy: 0.6923\n",
      "Epoch 81/150\n",
      "1397/1397 [==============================] - 0s 118us/sample - loss: 0.6140 - accuracy: 0.6829 - val_loss: 0.6395 - val_accuracy: 0.6282\n",
      "Epoch 82/150\n",
      "1397/1397 [==============================] - 0s 113us/sample - loss: 0.6127 - accuracy: 0.6714 - val_loss: 0.6121 - val_accuracy: 0.6987\n",
      "Epoch 83/150\n",
      "1397/1397 [==============================] - 0s 110us/sample - loss: 0.6152 - accuracy: 0.6707 - val_loss: 0.6043 - val_accuracy: 0.7115\n",
      "Epoch 84/150\n",
      "1397/1397 [==============================] - 0s 154us/sample - loss: 0.6097 - accuracy: 0.6664 - val_loss: 0.6071 - val_accuracy: 0.7051\n",
      "Epoch 85/150\n",
      "1397/1397 [==============================] - 0s 115us/sample - loss: 0.6143 - accuracy: 0.6693 - val_loss: 0.6108 - val_accuracy: 0.6859\n",
      "Epoch 86/150\n",
      "1397/1397 [==============================] - 0s 111us/sample - loss: 0.6060 - accuracy: 0.6829 - val_loss: 0.7607 - val_accuracy: 0.5064\n",
      "Epoch 87/150\n",
      "1397/1397 [==============================] - 0s 112us/sample - loss: 0.6145 - accuracy: 0.6593 - val_loss: 0.6274 - val_accuracy: 0.6731\n",
      "Epoch 88/150\n",
      "1397/1397 [==============================] - 0s 109us/sample - loss: 0.6156 - accuracy: 0.6786 - val_loss: 0.6103 - val_accuracy: 0.7051\n",
      "Epoch 89/150\n",
      "1397/1397 [==============================] - 0s 109us/sample - loss: 0.6077 - accuracy: 0.6750 - val_loss: 0.6040 - val_accuracy: 0.7051\n",
      "Epoch 90/150\n",
      "1397/1397 [==============================] - 0s 121us/sample - loss: 0.6170 - accuracy: 0.6693 - val_loss: 0.5978 - val_accuracy: 0.7051\n",
      "Epoch 91/150\n",
      "1397/1397 [==============================] - 0s 133us/sample - loss: 0.6083 - accuracy: 0.6850 - val_loss: 0.7681 - val_accuracy: 0.5064\n",
      "Epoch 92/150\n",
      "1397/1397 [==============================] - 0s 118us/sample - loss: 0.6076 - accuracy: 0.6707 - val_loss: 0.6164 - val_accuracy: 0.6923\n",
      "Epoch 93/150\n",
      "1397/1397 [==============================] - 0s 110us/sample - loss: 0.6095 - accuracy: 0.6786 - val_loss: 0.6115 - val_accuracy: 0.7051\n",
      "Epoch 94/150\n",
      "1397/1397 [==============================] - 0s 109us/sample - loss: 0.6061 - accuracy: 0.6879 - val_loss: 0.6034 - val_accuracy: 0.7051\n",
      "Epoch 95/150\n",
      "1397/1397 [==============================] - 0s 110us/sample - loss: 0.6024 - accuracy: 0.6829 - val_loss: 0.6310 - val_accuracy: 0.7051\n",
      "Epoch 96/150\n",
      "1397/1397 [==============================] - 0s 109us/sample - loss: 0.6069 - accuracy: 0.6722 - val_loss: 0.6052 - val_accuracy: 0.6923\n",
      "Epoch 97/150\n",
      "1397/1397 [==============================] - 0s 112us/sample - loss: 0.6117 - accuracy: 0.6650 - val_loss: 0.6021 - val_accuracy: 0.7051\n",
      "Epoch 98/150\n",
      "1397/1397 [==============================] - 0s 124us/sample - loss: 0.6044 - accuracy: 0.6793 - val_loss: 0.6215 - val_accuracy: 0.6923\n",
      "Epoch 99/150\n",
      "1397/1397 [==============================] - 0s 112us/sample - loss: 0.6064 - accuracy: 0.6793 - val_loss: 0.6117 - val_accuracy: 0.6538\n",
      "Epoch 100/150\n",
      "1397/1397 [==============================] - 0s 109us/sample - loss: 0.6137 - accuracy: 0.6822 - val_loss: 0.6086 - val_accuracy: 0.7051\n",
      "Epoch 101/150\n",
      "1397/1397 [==============================] - 0s 108us/sample - loss: 0.6120 - accuracy: 0.6858 - val_loss: 0.5954 - val_accuracy: 0.7051\n",
      "Epoch 102/150\n",
      "1397/1397 [==============================] - 0s 131us/sample - loss: 0.6077 - accuracy: 0.6822 - val_loss: 0.6414 - val_accuracy: 0.6282\n",
      "Epoch 103/150\n",
      "1397/1397 [==============================] - 0s 119us/sample - loss: 0.6124 - accuracy: 0.6707 - val_loss: 0.6076 - val_accuracy: 0.6987\n",
      "Epoch 104/150\n",
      "1397/1397 [==============================] - 0s 109us/sample - loss: 0.6049 - accuracy: 0.6729 - val_loss: 0.5984 - val_accuracy: 0.7051\n",
      "Epoch 105/150\n",
      "1397/1397 [==============================] - 0s 120us/sample - loss: 0.6067 - accuracy: 0.6879 - val_loss: 0.6189 - val_accuracy: 0.6731\n",
      "Epoch 106/150\n",
      "1397/1397 [==============================] - 0s 111us/sample - loss: 0.6110 - accuracy: 0.6793 - val_loss: 0.6124 - val_accuracy: 0.6859\n",
      "Epoch 107/150\n",
      "1397/1397 [==============================] - 0s 109us/sample - loss: 0.6121 - accuracy: 0.6736 - val_loss: 0.6124 - val_accuracy: 0.7115\n",
      "Epoch 108/150\n",
      "1397/1397 [==============================] - 0s 112us/sample - loss: 0.6077 - accuracy: 0.6858 - val_loss: 0.6395 - val_accuracy: 0.6859\n",
      "Epoch 109/150\n",
      "1397/1397 [==============================] - 0s 110us/sample - loss: 0.6058 - accuracy: 0.6850 - val_loss: 0.6086 - val_accuracy: 0.6859\n",
      "Epoch 110/150\n",
      "1397/1397 [==============================] - 0s 114us/sample - loss: 0.6069 - accuracy: 0.6750 - val_loss: 0.6084 - val_accuracy: 0.6859\n",
      "Epoch 111/150\n",
      "1397/1397 [==============================] - 0s 121us/sample - loss: 0.6051 - accuracy: 0.6843 - val_loss: 0.5940 - val_accuracy: 0.6987\n",
      "Epoch 112/150\n",
      "1397/1397 [==============================] - 0s 115us/sample - loss: 0.6080 - accuracy: 0.6822 - val_loss: 0.6276 - val_accuracy: 0.6987\n",
      "Epoch 113/150\n",
      "1397/1397 [==============================] - 0s 114us/sample - loss: 0.6085 - accuracy: 0.6800 - val_loss: 0.6326 - val_accuracy: 0.6859\n",
      "Epoch 114/150\n",
      "1397/1397 [==============================] - 0s 113us/sample - loss: 0.6018 - accuracy: 0.6858 - val_loss: 0.6233 - val_accuracy: 0.7051\n",
      "Epoch 115/150\n",
      "1397/1397 [==============================] - 0s 116us/sample - loss: 0.6059 - accuracy: 0.6807 - val_loss: 0.6119 - val_accuracy: 0.7051\n",
      "Epoch 116/150\n",
      "1397/1397 [==============================] - 0s 115us/sample - loss: 0.6065 - accuracy: 0.6722 - val_loss: 0.6053 - val_accuracy: 0.7051\n",
      "Epoch 117/150\n",
      "1397/1397 [==============================] - 0s 109us/sample - loss: 0.6060 - accuracy: 0.6807 - val_loss: 0.6091 - val_accuracy: 0.7051\n",
      "Epoch 118/150\n",
      "1397/1397 [==============================] - 0s 131us/sample - loss: 0.6015 - accuracy: 0.6807 - val_loss: 0.6488 - val_accuracy: 0.6282\n",
      "Epoch 119/150\n",
      "1397/1397 [==============================] - 0s 112us/sample - loss: 0.6110 - accuracy: 0.6807 - val_loss: 0.6160 - val_accuracy: 0.7051\n",
      "Epoch 120/150\n",
      "1397/1397 [==============================] - 0s 111us/sample - loss: 0.6073 - accuracy: 0.6714 - val_loss: 0.6488 - val_accuracy: 0.6410\n",
      "Epoch 121/150\n",
      "1397/1397 [==============================] - 0s 123us/sample - loss: 0.6073 - accuracy: 0.6736 - val_loss: 0.6428 - val_accuracy: 0.6218\n",
      "Epoch 122/150\n",
      "1397/1397 [==============================] - 0s 108us/sample - loss: 0.6051 - accuracy: 0.6886 - val_loss: 0.6145 - val_accuracy: 0.6923\n",
      "Epoch 123/150\n",
      "1397/1397 [==============================] - 0s 108us/sample - loss: 0.6029 - accuracy: 0.6764 - val_loss: 0.6157 - val_accuracy: 0.6859\n",
      "Epoch 124/150\n",
      "1397/1397 [==============================] - 0s 118us/sample - loss: 0.6077 - accuracy: 0.6836 - val_loss: 0.5999 - val_accuracy: 0.7051\n",
      "Epoch 125/150\n",
      "1397/1397 [==============================] - 0s 129us/sample - loss: 0.6145 - accuracy: 0.6807 - val_loss: 0.6156 - val_accuracy: 0.7051\n",
      "Epoch 126/150\n",
      "1397/1397 [==============================] - 0s 112us/sample - loss: 0.6093 - accuracy: 0.6815 - val_loss: 0.6132 - val_accuracy: 0.6987\n",
      "Epoch 127/150\n",
      "1397/1397 [==============================] - 0s 147us/sample - loss: 0.6012 - accuracy: 0.6786 - val_loss: 0.6237 - val_accuracy: 0.7051\n",
      "Epoch 128/150\n",
      "1397/1397 [==============================] - 0s 137us/sample - loss: 0.6063 - accuracy: 0.6829 - val_loss: 0.6368 - val_accuracy: 0.6346\n",
      "Epoch 129/150\n",
      "1397/1397 [==============================] - 0s 158us/sample - loss: 0.6104 - accuracy: 0.6836 - val_loss: 0.6100 - val_accuracy: 0.7051\n",
      "Epoch 130/150\n",
      "1397/1397 [==============================] - 0s 148us/sample - loss: 0.6045 - accuracy: 0.6779 - val_loss: 0.6364 - val_accuracy: 0.6923\n",
      "Epoch 131/150\n",
      "1397/1397 [==============================] - 0s 148us/sample - loss: 0.6130 - accuracy: 0.6800 - val_loss: 0.6015 - val_accuracy: 0.7051\n",
      "Epoch 132/150\n",
      "1397/1397 [==============================] - 0s 163us/sample - loss: 0.6070 - accuracy: 0.6764 - val_loss: 0.6122 - val_accuracy: 0.6731\n",
      "Epoch 133/150\n",
      "1397/1397 [==============================] - 0s 161us/sample - loss: 0.6041 - accuracy: 0.6829 - val_loss: 0.6583 - val_accuracy: 0.6282\n",
      "Epoch 134/150\n",
      "1397/1397 [==============================] - 0s 136us/sample - loss: 0.6031 - accuracy: 0.6929 - val_loss: 0.6100 - val_accuracy: 0.6987\n",
      "Epoch 135/150\n",
      "1397/1397 [==============================] - 0s 148us/sample - loss: 0.6027 - accuracy: 0.6908 - val_loss: 0.7228 - val_accuracy: 0.5256\n",
      "Epoch 136/150\n",
      "1397/1397 [==============================] - 0s 163us/sample - loss: 0.6036 - accuracy: 0.6908 - val_loss: 0.6717 - val_accuracy: 0.6026\n",
      "Epoch 137/150\n",
      "1397/1397 [==============================] - 0s 166us/sample - loss: 0.6083 - accuracy: 0.6664 - val_loss: 0.6128 - val_accuracy: 0.7051\n",
      "Epoch 138/150\n",
      "1397/1397 [==============================] - 0s 136us/sample - loss: 0.6099 - accuracy: 0.6722 - val_loss: 0.6465 - val_accuracy: 0.6538\n",
      "Epoch 139/150\n",
      "1397/1397 [==============================] - 0s 111us/sample - loss: 0.6076 - accuracy: 0.6815 - val_loss: 0.6202 - val_accuracy: 0.7051\n",
      "Epoch 140/150\n",
      "1397/1397 [==============================] - 0s 111us/sample - loss: 0.6075 - accuracy: 0.6686 - val_loss: 0.7162 - val_accuracy: 0.5321\n",
      "Epoch 141/150\n",
      "1397/1397 [==============================] - 0s 116us/sample - loss: 0.6007 - accuracy: 0.6886 - val_loss: 0.6268 - val_accuracy: 0.6667\n",
      "Epoch 142/150\n",
      "1397/1397 [==============================] - 0s 133us/sample - loss: 0.6084 - accuracy: 0.6757 - val_loss: 0.5963 - val_accuracy: 0.6987\n",
      "Epoch 143/150\n",
      "1397/1397 [==============================] - 0s 115us/sample - loss: 0.6065 - accuracy: 0.6858 - val_loss: 0.6171 - val_accuracy: 0.7051\n",
      "Epoch 144/150\n",
      "1397/1397 [==============================] - 0s 127us/sample - loss: 0.6054 - accuracy: 0.6800 - val_loss: 0.6038 - val_accuracy: 0.6859\n",
      "Epoch 145/150\n",
      "1397/1397 [==============================] - 0s 112us/sample - loss: 0.6027 - accuracy: 0.6772 - val_loss: 0.6100 - val_accuracy: 0.6859\n",
      "Epoch 146/150\n",
      "1397/1397 [==============================] - 0s 117us/sample - loss: 0.6092 - accuracy: 0.6786 - val_loss: 0.6189 - val_accuracy: 0.6859\n",
      "Epoch 147/150\n",
      "1397/1397 [==============================] - 0s 111us/sample - loss: 0.6080 - accuracy: 0.6736 - val_loss: 0.6403 - val_accuracy: 0.6795\n",
      "Epoch 148/150\n",
      "1397/1397 [==============================] - 0s 115us/sample - loss: 0.6050 - accuracy: 0.6779 - val_loss: 0.6090 - val_accuracy: 0.6859\n",
      "Epoch 149/150\n",
      "1397/1397 [==============================] - 0s 126us/sample - loss: 0.6090 - accuracy: 0.6750 - val_loss: 0.6066 - val_accuracy: 0.6987\n",
      "Epoch 150/150\n",
      "1397/1397 [==============================] - 0s 112us/sample - loss: 0.6066 - accuracy: 0.6836 - val_loss: 0.6034 - val_accuracy: 0.7051\n",
      "173/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 92us/sample - loss: 0.5613 - accuracy: 0.6879\n",
      "Test Score: 0.5831024167854661\n",
      "Test Accuracy: 0.68786126\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign       0.69      0.92      0.79       116\n",
      "         PCa       0.53      0.18      0.26        57\n",
      "\n",
      "    accuracy                           0.68       173\n",
      "   macro avg       0.61      0.55      0.53       173\n",
      "weighted avg       0.64      0.68      0.62       173\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "for i in data0.index:\n",
    "    data0.loc[i,\"PSA_delta1\"]=data0.loc[i,\"PSA270\"]-data0.loc[i,\"PSA360\"]\n",
    "    data0.loc[i,\"PSA_delta2\"]=data0.loc[i,\"PSA180\"]-data0.loc[i,\"PSA270\"]\n",
    "    data0.loc[i,\"PSA_delta3\"]=data0.loc[i,\"PSA90\"]-data0.loc[i,\"PSA180\"]\n",
    "    data0.loc[i,\"PSA_delta4\"]=data0.loc[i,\"PSA0\"]-data0.loc[i,\"PSA90\"]\n",
    "\n",
    "data0.loc[:,\"age\"]=data0.loc[:,\"Age\"]\n",
    "data0.loc[:,\"PSA_Average\"]=data0.loc[:,\"PSA_average\"]\n",
    "#data0.loc[:,\"Index\"]=data0.loc[:,\"index\"]\n",
    "\n",
    "y=data0.event.values\n",
    "data0___=data0.drop(columns=\"event\")\n",
    "X=data0___.loc[:,\"Age\":\"PSA0\"].values\n",
    "X=X.astype(np.float32)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold,cross_val_score\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=39)\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Activation,Dropout,LSTM,Bidirectional\n",
    "from tensorflow.keras.models import Model,Sequential\n",
    "from tensorflow.keras.optimizers import Adadelta, SGD, RMSprop, Adam,Adamax\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "input_layer = Input(shape=(X_train.shape[1],))\n",
    "dense_layer_1 = Dense(15, activation='relu')(input_layer)\n",
    "dense_layer_2 = Dense(10, activation='relu')(dense_layer_1)\n",
    "output = Dense(1, activation='sigmoid')(dense_layer_2)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'RMSprop', metrics = ['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=20, epochs=150, verbose=1, validation_split=0.1)\n",
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print(\"Test Score:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred=(y_pred>0.5)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred, target_names=['benign', 'PCa']))\n",
    "\n",
    "\n",
    "#출처: https://datascienceschool.net/view-notebook/731e0d2ef52c41c686ba53dcaf346f32/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 69.94%\n",
      "Precision: 54.35%\n",
      "Recall: 44.64%\n",
      "accuracy: 68.21%\n",
      "Precision: 55.56%\n",
      "Recall: 8.93%\n",
      "accuracy: 73.41%\n",
      "Precision: 77.78%\n",
      "Recall: 25.00%\n",
      "accuracy: 72.25%\n",
      "Precision: 61.76%\n",
      "Recall: 37.50%\n",
      "accuracy: 67.63%\n",
      "Precision: 50.00%\n",
      "Recall: 17.86%\n",
      "accuracy: 67.63%\n",
      "Precision: 50.00%\n",
      "Recall: 12.50%\n",
      "accuracy: 70.52%\n",
      "Precision: 55.32%\n",
      "Recall: 46.43%\n",
      "accuracy: 71.51%\n",
      "Precision: 58.97%\n",
      "Recall: 41.07%\n",
      "accuracy: 67.44%\n",
      "Precision: 50.00%\n",
      "Recall: 14.29%\n",
      "accuracy: 68.42%\n",
      "Precision: 66.67%\n",
      "Recall: 3.64%\n",
      "accuracy:  69.70% (+/- 2.05%)\n",
      "precision:  58.04% (+/- 8.37%)\n",
      "recall:  25.19% (+/- 15.15%)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of binary and continuous targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-a73d6cb0e8ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m     \"\"\"\n\u001b[1;32m--> 253\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"multiclass\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s is not supported\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[1;32m---> 81\u001b[1;33m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and continuous targets"
     ]
    }
   ],
   "source": [
    "# Manual K-fold validation\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "for i in data0.index:\n",
    "    data0.loc[i,\"PSA_delta1\"]=data0.loc[i,\"PSA270\"]-data0.loc[i,\"PSA360\"]\n",
    "    data0.loc[i,\"PSA_delta2\"]=data0.loc[i,\"PSA180\"]-data0.loc[i,\"PSA270\"]\n",
    "    data0.loc[i,\"PSA_delta3\"]=data0.loc[i,\"PSA90\"]-data0.loc[i,\"PSA180\"]\n",
    "    data0.loc[i,\"PSA_delta4\"]=data0.loc[i,\"PSA0\"]-data0.loc[i,\"PSA90\"]\n",
    "\n",
    "\n",
    "data0.loc[:,\"age\"]=data0.loc[:,\"Age\"]\n",
    "data0.loc[:,\"PSA_Average\"]=data0.loc[:,\"PSA_average\"]\n",
    "\n",
    "y=data0.event.values\n",
    "data0___=data0.drop(columns=\"event\")\n",
    "X=data0___.loc[:,\"Age\":\"PSA0\"].values\n",
    "X=X.astype(np.float32)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Activation,Dropout\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adadelta, SGD, RMSprop, Adam,Adamax\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold,cross_val_score\n",
    "#https://keras.io/optimizers/\n",
    "\n",
    "# define 10-fold cross validation test harness\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=np.random.seed(35))\n",
    "acc = []\n",
    "prec=[]\n",
    "rec=[]\n",
    "for train, test in kfold.split(X, y):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(15, input_dim=X.shape[1], activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='RMSprop', metrics=['accuracy','Precision','Recall'])\n",
    "    # Fit the model\n",
    "    model.fit(X[train], y[train], epochs=150, batch_size=10, verbose=0)\n",
    "    # evaluate the model\n",
    "    scores = model.evaluate(X[test], y[test], verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[2], scores[2]*100))\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[3], scores[3]*100))\n",
    "    acc.append(scores[1] * 100)\n",
    "    prec.append(scores[2] * 100)\n",
    "    rec.append(scores[3]*100)\n",
    "print('accuracy: ',\"%.2f%% (+/- %.2f%%)\" % (np.mean(acc), np.std(acc)))\n",
    "print('precision: ',\"%.2f%% (+/- %.2f%%)\" % (np.mean(prec), np.std(prec)))\n",
    "print('recall: ',\"%.2f%% (+/- %.2f%%)\" % (np.mean(rec), np.std(rec)))\n",
    "\n",
    "y_pred = model.predict(X[test])\n",
    "y_pred=y_pred(y_pred>0.3)\n",
    "y_pred=y_pred.reshape(171,1)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y[test], y_pred,labels=[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1. Logistic Regression - PSA_ewm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "for i in range(0,1843):\n",
    "    data0.loc[i,\"PSA_mean\"]=(data0.loc[i,\"PSA360\"]+data0.loc[i,\"PSA270\"]+data0.loc[i,\"PSA180\"]+data0.loc[i,\"PSA90\"]+data0.loc[i,\"PSA0\"])/5\n",
    "    data0.loc[i,\"PSA_delta1\"]=data0.loc[i,\"PSA270\"]-data0.loc[i,\"PSA360\"]\n",
    "    data0.loc[i,\"PSA_delta2\"]=data0.loc[i,\"PSA180\"]-data0.loc[i,\"PSA270\"]\n",
    "    data0.loc[i,\"PSA_delta3\"]=data0.loc[i,\"PSA90\"]-data0.loc[i,\"PSA180\"]\n",
    "    data0.loc[i,\"PSA_delta4\"]=data0.loc[i,\"PSA0\"]-data0.loc[i,\"PSA90\"]\n",
    "\n",
    "'''\n",
    "#exp.weighted.mean\n",
    "for i in data.index:\n",
    "    idx=int(p_day.loc[i,\"final_num\"])\n",
    "    data0.loc[i,\"PSA_ewm\"]=psa.iloc[i,:idx].transpose().ewm(span=idx,min_periods=idx).mean()[-1]\n",
    "\n",
    "data0.loc[1733,\"Age\"]=69 #mean값\n",
    "\n",
    "y=data0.event.values\n",
    "data0_=data0.drop(columns=[\"event\",\"Age\",\"Index\",\"PSA_average\",\"PSA_Average\"])\n",
    "\n",
    "X=data0_.values\n",
    "X=X.astype(np.float32)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold,cross_val_score\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=64)\n",
    "\n",
    "X_train2=X_train[:,7:]\n",
    "X_test2=X_test[:,7:]\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Activation,Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adadelta, SGD, RMSprop, Adam,Adamax\n",
    "#https://keras.io/optimizers/\n",
    "\n",
    "input_layer = Input(shape=(X_train2.shape[1],))\n",
    "dense_layer_1 = Dense(30, activation='relu')(input_layer)\n",
    "dense_layer_2 = Dense(20, activation='relu')(dense_layer_1)\n",
    "output = Dense(1, activation='sigmoid')(dense_layer_2)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output)\n",
    "model.compile(loss='binary_crossentropy', optimizer='RMSprop', metrics=['acc'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "history = model.fit(X_train2, y_train, batch_size=20, epochs=200, verbose=1, validation_split=0.1)\n",
    "\n",
    "score = model.evaluate(X_test2, y_test, verbose=1)\n",
    "\n",
    "print(\"Test Score:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1]) \n",
    "\n",
    "y_pred = model.predict(X_test2)\n",
    "y_pred=(y_pred>0.5)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred, target_names=['benign', 'PCa']))\n",
    "#출처: https://datascienceschool.net/view-notebook/731e0d2ef52c41c686ba53dcaf346f32/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 67.63%\n",
      "Precision: 0.00%\n",
      "Recall: 0.00%\n",
      "accuracy: 69.94%\n",
      "Precision: 83.33%\n",
      "Recall: 8.93%\n",
      "accuracy: 65.90%\n",
      "Precision: 44.83%\n",
      "Recall: 23.21%\n",
      "accuracy: 68.21%\n",
      "Precision: 54.55%\n",
      "Recall: 10.71%\n",
      "accuracy: 50.87%\n",
      "Precision: 38.02%\n",
      "Recall: 82.14%\n",
      "accuracy: 69.36%\n",
      "Precision: 80.00%\n",
      "Recall: 7.14%\n",
      "accuracy: 67.05%\n",
      "Precision: 0.00%\n",
      "Recall: 0.00%\n",
      "accuracy: 69.77%\n",
      "Precision: 56.67%\n",
      "Recall: 30.36%\n",
      "accuracy: 66.28%\n",
      "Precision: 33.33%\n",
      "Recall: 3.57%\n",
      "accuracy: 67.84%\n",
      "Precision: 50.00%\n",
      "Recall: 1.82%\n",
      "accuracy:  66.28% (+/- 5.30%)\n",
      "precision:  44.07% (+/- 26.78%)\n",
      "recall:  16.79% (+/- 23.77%)\n"
     ]
    }
   ],
   "source": [
    "# Manual K-fold validation\n",
    "\n",
    "'''import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "for i in range(0,1843):\n",
    "    data0.loc[i,\"PSA_mean\"]=(data0.loc[i,\"PSA360\"]+data0.loc[i,\"PSA270\"]+data0.loc[i,\"PSA180\"]+data0.loc[i,\"PSA90\"]+data0.loc[i,\"PSA0\"])/5\n",
    "    data0.loc[i,\"PSA_delta1\"]=data0.loc[i,\"PSA270\"]-data0.loc[i,\"PSA360\"]\n",
    "    data0.loc[i,\"PSA_delta2\"]=data0.loc[i,\"PSA180\"]-data0.loc[i,\"PSA270\"]\n",
    "    data0.loc[i,\"PSA_delta3\"]=data0.loc[i,\"PSA90\"]-data0.loc[i,\"PSA180\"]\n",
    "    data0.loc[i,\"PSA_delta4\"]=data0.loc[i,\"PSA0\"]-data0.loc[i,\"PSA90\"]\n",
    "\n",
    "'''\n",
    "#exp.weighted.mean\n",
    "for i in data_drop.index:\n",
    "    idx=int(data_drop.loc[i,\"final_num\"])\n",
    "    data0.loc[i,\"PSA_ewm\"]=psa.iloc[i,:idx].transpose().ewm(span=idx,min_periods=idx).mean()[-1]\n",
    "\n",
    "y=data0.event.values\n",
    "data0_=data0.drop(columns=[\"event\",\"Age\",\"index\",\"PSA_average\",\"PSA_Average\"])\n",
    "\n",
    "X=data0_.values\n",
    "X=X.astype(np.float32)\n",
    "X2=X[:,5:]\n",
    "'''\n",
    "from sklearn.metrics import confusion_matrix, precision_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold,cross_val_score\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=64)\n",
    "\n",
    "X_train2=X_train[:,7:]\n",
    "X_test2=X_test[:,7:]\n",
    "'''\n",
    "'''from sklearn.preprocessing import StandardScaler  # doctest: +SKIP\n",
    "scaler = StandardScaler()  # doctest: +SKIP\n",
    "scaler.fit(X_train)  # doctest: +SKIP\n",
    "X_train = scaler.transform(X_train)  # doctest: +SKIP\n",
    "X_test = scaler.transform(X_test)  # doctest: +SKIP'''\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Activation,Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adadelta, SGD, RMSprop, Adam,Adamax\n",
    "#https://keras.io/optimizers/\n",
    "\n",
    "# define 10-fold cross validation test harness\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=np.random.seed(35))\n",
    "acc = []\n",
    "prec=[]\n",
    "rec=[]\n",
    "for train, test in kfold.split(X2, y):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(15, input_dim=X2.shape[1], activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='RMSprop', metrics=['accuracy','Precision','Recall'])\n",
    "    # Fit the model\n",
    "    model.fit(X2[train], y[train], epochs=150, batch_size=10, verbose=0)\n",
    "    # evaluate the model\n",
    "    scores = model.evaluate(X2[test], y[test], verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[2], scores[2]*100))\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[3], scores[3]*100))\n",
    "    acc.append(scores[1] * 100)\n",
    "    prec.append(scores[2] * 100)\n",
    "    rec.append(scores[3]*100)\n",
    "print('accuracy: ',\"%.2f%% (+/- %.2f%%)\" % (np.mean(acc), np.std(acc)))\n",
    "print('precision: ',\"%.2f%% (+/- %.2f%%)\" % (np.mean(prec), np.std(prec)))\n",
    "print('recall: ',\"%.2f%% (+/- %.2f%%)\" % (np.mean(rec), np.std(rec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "binary classification에서 model.evaluate에서 accuracy=K.mean(K.equal(y_true, K.round(y_pred)))\n",
    "출처: https://datascience.stackexchange.com/questions/14415/how-does-keras-calculate-accuracy/14742#14742"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model...\n",
      "Compiling...\n",
      "Fitting model...\n",
      "Train on 1553 samples\n",
      "Epoch 1/100\n",
      "1553/1553 [==============================] - 5s 3ms/sample - loss: 0.6631 - accuracy: 0.6574\n",
      "Epoch 2/100\n",
      "1553/1553 [==============================] - 1s 833us/sample - loss: 0.6459 - accuracy: 0.6710\n",
      "Epoch 3/100\n",
      "1553/1553 [==============================] - 1s 848us/sample - loss: 0.6294 - accuracy: 0.6703\n",
      "Epoch 4/100\n",
      "1553/1553 [==============================] - 1s 816us/sample - loss: 0.6174 - accuracy: 0.6735\n",
      "Epoch 5/100\n",
      "1553/1553 [==============================] - 1s 859us/sample - loss: 0.6323 - accuracy: 0.6729\n",
      "Epoch 6/100\n",
      "1553/1553 [==============================] - 1s 824us/sample - loss: 0.6161 - accuracy: 0.6735\n",
      "Epoch 7/100\n",
      "1553/1553 [==============================] - 1s 861us/sample - loss: 0.6134 - accuracy: 0.6838\n",
      "Epoch 8/100\n",
      "1553/1553 [==============================] - 1s 847us/sample - loss: 0.6095 - accuracy: 0.6832\n",
      "Epoch 9/100\n",
      "1553/1553 [==============================] - 1s 816us/sample - loss: 0.6056 - accuracy: 0.6780\n",
      "Epoch 10/100\n",
      "1553/1553 [==============================] - 1s 854us/sample - loss: 0.6104 - accuracy: 0.6806\n",
      "Epoch 11/100\n",
      "1553/1553 [==============================] - 1s 809us/sample - loss: 0.6118 - accuracy: 0.6813\n",
      "Epoch 12/100\n",
      "1553/1553 [==============================] - 1s 846us/sample - loss: 0.6109 - accuracy: 0.6780\n",
      "Epoch 13/100\n",
      "1553/1553 [==============================] - 1s 830us/sample - loss: 0.6112 - accuracy: 0.6774\n",
      "Epoch 14/100\n",
      "1553/1553 [==============================] - 1s 816us/sample - loss: 0.6097 - accuracy: 0.6819\n",
      "Epoch 15/100\n",
      "1553/1553 [==============================] - 1s 861us/sample - loss: 0.6112 - accuracy: 0.6858\n",
      "Epoch 16/100\n",
      "1553/1553 [==============================] - 1s 827us/sample - loss: 0.6094 - accuracy: 0.6851\n",
      "Epoch 17/100\n",
      "1553/1553 [==============================] - 1s 848us/sample - loss: 0.6018 - accuracy: 0.6813\n",
      "Epoch 18/100\n",
      "1553/1553 [==============================] - 1s 823us/sample - loss: 0.6105 - accuracy: 0.6806\n",
      "Epoch 19/100\n",
      "1553/1553 [==============================] - 1s 814us/sample - loss: 0.6067 - accuracy: 0.6825\n",
      "Epoch 20/100\n",
      "1553/1553 [==============================] - 1s 832us/sample - loss: 0.6087 - accuracy: 0.6832\n",
      "Epoch 21/100\n",
      "1553/1553 [==============================] - 1s 862us/sample - loss: 0.6059 - accuracy: 0.6819\n",
      "Epoch 22/100\n",
      "1553/1553 [==============================] - 1s 928us/sample - loss: 0.6066 - accuracy: 0.6838\n",
      "Epoch 23/100\n",
      "1553/1553 [==============================] - 1s 886us/sample - loss: 0.6043 - accuracy: 0.6787 - loss: 0.6061 - accuracy\n",
      "Epoch 24/100\n",
      "1553/1553 [==============================] - 1s 901us/sample - loss: 0.6028 - accuracy: 0.6832\n",
      "Epoch 25/100\n",
      "1553/1553 [==============================] - 1s 877us/sample - loss: 0.6084 - accuracy: 0.6819\n",
      "Epoch 26/100\n",
      "1553/1553 [==============================] - 1s 936us/sample - loss: 0.6070 - accuracy: 0.6858\n",
      "Epoch 27/100\n",
      "1553/1553 [==============================] - 1s 890us/sample - loss: 0.6021 - accuracy: 0.6864\n",
      "Epoch 28/100\n",
      "1553/1553 [==============================] - 1s 865us/sample - loss: 0.6036 - accuracy: 0.6813\n",
      "Epoch 29/100\n",
      "1553/1553 [==============================] - 1s 910us/sample - loss: 0.6031 - accuracy: 0.6819\n",
      "Epoch 30/100\n",
      "1553/1553 [==============================] - 1s 884us/sample - loss: 0.6036 - accuracy: 0.6838\n",
      "Epoch 31/100\n",
      "1553/1553 [==============================] - 1s 911us/sample - loss: 0.5985 - accuracy: 0.6845\n",
      "Epoch 32/100\n",
      "1553/1553 [==============================] - 1s 898us/sample - loss: 0.6016 - accuracy: 0.6832\n",
      "Epoch 33/100\n",
      "1553/1553 [==============================] - 1s 916us/sample - loss: 0.6026 - accuracy: 0.6819\n",
      "Epoch 34/100\n",
      "1553/1553 [==============================] - 1s 897us/sample - loss: 0.6022 - accuracy: 0.6832\n",
      "Epoch 35/100\n",
      "1553/1553 [==============================] - 1s 903us/sample - loss: 0.5990 - accuracy: 0.6832\n",
      "Epoch 36/100\n",
      "1553/1553 [==============================] - 1s 882us/sample - loss: 0.5986 - accuracy: 0.6838\n",
      "Epoch 37/100\n",
      "1553/1553 [==============================] - 1s 936us/sample - loss: 0.5997 - accuracy: 0.6838\n",
      "Epoch 38/100\n",
      "1553/1553 [==============================] - 1s 896us/sample - loss: 0.5953 - accuracy: 0.6845\n",
      "Epoch 39/100\n",
      "1553/1553 [==============================] - 1s 899us/sample - loss: 0.5973 - accuracy: 0.6838\n",
      "Epoch 40/100\n",
      "1553/1553 [==============================] - 1s 904us/sample - loss: 0.5950 - accuracy: 0.6832\n",
      "Epoch 41/100\n",
      "1553/1553 [==============================] - 1s 879us/sample - loss: 0.5977 - accuracy: 0.6832\n",
      "Epoch 42/100\n",
      "1553/1553 [==============================] - 1s 872us/sample - loss: 0.5939 - accuracy: 0.6838\n",
      "Epoch 43/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5955 - accuracy: 0.6838\n",
      "Epoch 44/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6000 - accuracy: 0.6838\n",
      "Epoch 45/100\n",
      "1553/1553 [==============================] - 1s 952us/sample - loss: 0.5968 - accuracy: 0.6838\n",
      "Epoch 46/100\n",
      "1553/1553 [==============================] - 1s 906us/sample - loss: 0.5981 - accuracy: 0.6832\n",
      "Epoch 47/100\n",
      "1553/1553 [==============================] - 1s 891us/sample - loss: 0.5937 - accuracy: 0.6838\n",
      "Epoch 48/100\n",
      "1553/1553 [==============================] - 1s 877us/sample - loss: 0.5945 - accuracy: 0.6832\n",
      "Epoch 49/100\n",
      "1553/1553 [==============================] - 1s 877us/sample - loss: 0.5958 - accuracy: 0.6845\n",
      "Epoch 50/100\n",
      "1553/1553 [==============================] - 1s 882us/sample - loss: 0.5965 - accuracy: 0.6838\n",
      "Epoch 51/100\n",
      "1553/1553 [==============================] - 1s 901us/sample - loss: 0.5918 - accuracy: 0.6832\n",
      "Epoch 52/100\n",
      "1553/1553 [==============================] - 1s 872us/sample - loss: 0.5944 - accuracy: 0.6838\n",
      "Epoch 53/100\n",
      "1553/1553 [==============================] - 1s 872us/sample - loss: 0.5883 - accuracy: 0.6825\n",
      "Epoch 54/100\n",
      "1553/1553 [==============================] - 1s 877us/sample - loss: 0.5894 - accuracy: 0.6832\n",
      "Epoch 55/100\n",
      "1553/1553 [==============================] - 1s 902us/sample - loss: 0.5920 - accuracy: 0.6806\n",
      "Epoch 56/100\n",
      "1553/1553 [==============================] - 1s 955us/sample - loss: 0.5907 - accuracy: 0.6813\n",
      "Epoch 57/100\n",
      "1553/1553 [==============================] - 1s 934us/sample - loss: 0.5905 - accuracy: 0.6806\n",
      "Epoch 58/100\n",
      "1553/1553 [==============================] - 1s 893us/sample - loss: 0.5934 - accuracy: 0.6819\n",
      "Epoch 59/100\n",
      "1553/1553 [==============================] - 1s 857us/sample - loss: 0.5909 - accuracy: 0.6838\n",
      "Epoch 60/100\n",
      "1553/1553 [==============================] - 1s 895us/sample - loss: 0.5877 - accuracy: 0.6832\n",
      "Epoch 61/100\n",
      "1553/1553 [==============================] - 1s 898us/sample - loss: 0.5930 - accuracy: 0.6819\n",
      "Epoch 62/100\n",
      "1553/1553 [==============================] - 1s 898us/sample - loss: 0.5902 - accuracy: 0.6825\n",
      "Epoch 63/100\n",
      "1553/1553 [==============================] - 1s 943us/sample - loss: 0.5884 - accuracy: 0.6787\n",
      "Epoch 64/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5952 - accuracy: 0.6813\n",
      "Epoch 65/100\n",
      "1553/1553 [==============================] - 1s 929us/sample - loss: 0.5868 - accuracy: 0.6838\n",
      "Epoch 66/100\n",
      "1553/1553 [==============================] - 1s 955us/sample - loss: 0.5928 - accuracy: 0.6806\n",
      "Epoch 67/100\n",
      "1553/1553 [==============================] - 2s 986us/sample - loss: 0.5888 - accuracy: 0.6819\n",
      "Epoch 68/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5905 - accuracy: 0.6780\n",
      "Epoch 69/100\n",
      "1553/1553 [==============================] - 1s 901us/sample - loss: 0.5938 - accuracy: 0.6825\n",
      "Epoch 70/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5867 - accuracy: 0.6793\n",
      "Epoch 71/100\n",
      "1553/1553 [==============================] - 1s 941us/sample - loss: 0.5912 - accuracy: 0.6722\n",
      "Epoch 72/100\n",
      "1553/1553 [==============================] - 1s 935us/sample - loss: 0.5886 - accuracy: 0.6819\n",
      "Epoch 73/100\n",
      "1553/1553 [==============================] - 1s 946us/sample - loss: 0.5860 - accuracy: 0.6806\n",
      "Epoch 74/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5893 - accuracy: 0.6851\n",
      "Epoch 75/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5892 - accuracy: 0.6800\n",
      "Epoch 76/100\n",
      "1553/1553 [==============================] - 1s 916us/sample - loss: 0.5887 - accuracy: 0.6864\n",
      "Epoch 77/100\n",
      "1553/1553 [==============================] - 2s 969us/sample - loss: 0.5862 - accuracy: 0.6813\n",
      "Epoch 78/100\n",
      "1553/1553 [==============================] - 1s 953us/sample - loss: 0.5886 - accuracy: 0.6845\n",
      "Epoch 79/100\n",
      "1553/1553 [==============================] - 2s 972us/sample - loss: 0.5870 - accuracy: 0.6832\n",
      "Epoch 80/100\n",
      "1553/1553 [==============================] - 1s 964us/sample - loss: 0.5891 - accuracy: 0.6851\n",
      "Epoch 81/100\n",
      "1553/1553 [==============================] - 1s 936us/sample - loss: 0.5870 - accuracy: 0.6806\n",
      "Epoch 82/100\n",
      "1553/1553 [==============================] - 1s 881us/sample - loss: 0.5838 - accuracy: 0.6793\n",
      "Epoch 83/100\n",
      "1553/1553 [==============================] - 1s 958us/sample - loss: 0.5852 - accuracy: 0.6806\n",
      "Epoch 84/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5862 - accuracy: 0.6806\n",
      "Epoch 85/100\n",
      "1553/1553 [==============================] - 2s 977us/sample - loss: 0.5890 - accuracy: 0.6774\n",
      "Epoch 86/100\n",
      "1553/1553 [==============================] - 2s 982us/sample - loss: 0.5826 - accuracy: 0.6813\n",
      "Epoch 87/100\n",
      "1553/1553 [==============================] - 1s 956us/sample - loss: 0.5883 - accuracy: 0.6800\n",
      "Epoch 88/100\n",
      "1553/1553 [==============================] - 1s 870us/sample - loss: 0.5893 - accuracy: 0.6819\n",
      "Epoch 89/100\n",
      "1553/1553 [==============================] - 1s 911us/sample - loss: 0.5820 - accuracy: 0.6832\n",
      "Epoch 90/100\n",
      "1553/1553 [==============================] - 1s 925us/sample - loss: 0.5824 - accuracy: 0.6838\n",
      "Epoch 91/100\n",
      "1553/1553 [==============================] - 1s 932us/sample - loss: 0.5859 - accuracy: 0.6755\n",
      "Epoch 92/100\n",
      "1553/1553 [==============================] - 1s 871us/sample - loss: 0.5843 - accuracy: 0.6793 - loss: 0.5848 - accuracy: 0.67\n",
      "Epoch 93/100\n",
      "1553/1553 [==============================] - 1s 871us/sample - loss: 0.5863 - accuracy: 0.6851\n",
      "Epoch 94/100\n",
      "1553/1553 [==============================] - 1s 886us/sample - loss: 0.5867 - accuracy: 0.6806\n",
      "Epoch 95/100\n",
      "1553/1553 [==============================] - 1s 868us/sample - loss: 0.5880 - accuracy: 0.6825\n",
      "Epoch 96/100\n",
      "1553/1553 [==============================] - 1s 865us/sample - loss: 0.5839 - accuracy: 0.6768\n",
      "Epoch 97/100\n",
      "1553/1553 [==============================] - 1s 902us/sample - loss: 0.5843 - accuracy: 0.6825\n",
      "Epoch 98/100\n",
      "1553/1553 [==============================] - 1s 932us/sample - loss: 0.5853 - accuracy: 0.6845\n",
      "Epoch 99/100\n",
      "1553/1553 [==============================] - 1s 877us/sample - loss: 0.5845 - accuracy: 0.6813\n",
      "Epoch 100/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5824 - accuracy: 0.6832\n",
      "173/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 5ms/sample - loss: 0.7138 - accuracy: 0.6358\n",
      "Test score: 0.7130303989256048\n",
      "Test accuracy: 0.63583815\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign       0.62      1.00      0.77       105\n",
      "         PCa       1.00      0.07      0.14        68\n",
      "\n",
      "    accuracy                           0.64       173\n",
      "   macro avg       0.81      0.54      0.45       173\n",
      "weighted avg       0.77      0.64      0.52       173\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nhist = model.fit(X_train_t, y_train, batch_size=8, epochs=10, validation_split = 0.1, verbose = 1)\\n\\nscore, acc = model.evaluate(X_test_t, y_test, batch_size=1)\\nprint('Test score:', score)\\nprint('Test accuracy:', acc)\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "X=data0.loc[:,\"PSA360\":\"PSA0\"].values\n",
    "X=X.astype(np.float32)\n",
    "y=data0.event.values\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "\n",
    "#oversampling\n",
    "'''\n",
    "from imblearn.over_sampling import SMOTE\n",
    "os = SMOTE(random_state=0)\n",
    "\n",
    "os_data_X,os_data_y=os.fit_sample(X_train, y_train)\n",
    "    '''\n",
    "'''\n",
    "from sklearn.preprocessing import StandardScaler  # doctest: +SKIP\n",
    "scaler = StandardScaler()  # doctest: +SKIP\n",
    "scaler.fit(X_train)  # doctest: +SKIP\n",
    "X_train = scaler.transform(X_train)  # doctest: +SKIP\n",
    "X_test = scaler.transform(X_test)  # doctest: +SKIP'''\n",
    "\n",
    "X_train_t = X_train.reshape(X_train.shape[0], 5, 1) \n",
    "X_test_t = X_test.reshape(X_test.shape[0], 5, 1)\n",
    "\n",
    "from tensorflow.keras.layers import LSTM \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense \n",
    "import tensorflow.keras.backend as K \n",
    "from tensorflow.keras.callbacks import EarlyStopping \n",
    "\n",
    "'''K.clear_session() \n",
    "model = Sequential() # Sequeatial Model \n",
    "model.add(LSTM(20, input_shape=(9, 1),activation='sigmoid')) # (timestep, feature) \n",
    "model.add(Dense(1,activation='sigmoid')) # output = 1 \n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['acc']) \n",
    "model.summary()\n",
    "'''\n",
    "\n",
    "def create_model():\n",
    "    print ('Creating model...')\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50,input_shape=(5,1)))\n",
    "    model.add(LSTM(256, activation='sigmoid', return_sequences=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(256, activation='sigmoid'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    print ('Compiling...')\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "print ('Fitting model...')\n",
    "hist = model.fit(X_train_t, y_train, batch_size=50, epochs=100)\n",
    "\n",
    "score, acc = model.evaluate(X_test_t, y_test, batch_size=50)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "y_pred = model.predict(X_test_t)\n",
    "y_pred=(y_pred>0.5)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred, target_names=['benign', 'PCa']))\n",
    "'''\n",
    "hist = model.fit(X_train_t, y_train, batch_size=8, epochs=10, validation_split = 0.1, verbose = 1)\n",
    "\n",
    "score, acc = model.evaluate(X_test_t, y_test, batch_size=1)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model...\n",
      "Compiling...\n",
      "Fitting model...\n",
      "Train on 1553 samples\n",
      "Epoch 1/100\n",
      "1553/1553 [==============================] - 6s 4ms/sample - loss: 0.6661 - accuracy: 0.6465 - Precision: 0.3083 - Recall: 0.0736\n",
      "Epoch 2/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6427 - accuracy: 0.6574 - Precision: 0.3535 - Recall: 0.0696\n",
      "Epoch 3/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6338 - accuracy: 0.6587 - Precision: 0.3884 - Recall: 0.0934\n",
      "Epoch 4/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6245 - accuracy: 0.6574 - Precision: 0.3210 - Recall: 0.0517\n",
      "Epoch 5/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6282 - accuracy: 0.6710 - Precision: 0.4375 - Recall: 0.0557\n",
      "Epoch 6/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6248 - accuracy: 0.6658 - Precision: 0.3919 - Recall: 0.0577\n",
      "Epoch 7/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6247 - accuracy: 0.6658 - Precision: 0.3462 - Recall: 0.0358\n",
      "Epoch 8/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6266 - accuracy: 0.6671 - Precision: 0.3409 - Recall: 0.0298\n",
      "Epoch 9/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6195 - accuracy: 0.6729 - Precision: 0.4138 - Recall: 0.0239\n",
      "Epoch 10/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6182 - accuracy: 0.6697 - Precision: 0.2500 - Recall: 0.0099\n",
      "Epoch 11/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6234 - accuracy: 0.6729 - Precision: 0.3810 - Recall: 0.0159\n",
      "Epoch 12/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6147 - accuracy: 0.6742 - Precision: 0.4000 - Recall: 0.0119\n",
      "Epoch 13/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6196 - accuracy: 0.6768 - Precision: 0.6000 - Recall: 0.0060\n",
      "Epoch 14/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6195 - accuracy: 0.6774 - Precision: 0.6000 - Recall: 0.0119\n",
      "Epoch 15/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6230 - accuracy: 0.6729 - Precision: 0.3077 - Recall: 0.0080\n",
      "Epoch 16/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6199 - accuracy: 0.6774 - Precision: 0.7500 - Recall: 0.0060\n",
      "Epoch 17/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6179 - accuracy: 0.6755 - Precision: 0.0000e+00 - Recall: 0.0000e+00\n",
      "Epoch 18/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6176 - accuracy: 0.6768 - Precision: 1.0000 - Recall: 0.0020\n",
      "Epoch 19/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6172 - accuracy: 0.6761 - Precision: 0.0000e+00 - Recall: 0.0000e+00\n",
      "Epoch 20/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6181 - accuracy: 0.6761 - Precision: 0.5000 - Recall: 0.0020\n",
      "Epoch 21/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6159 - accuracy: 0.6755 - Precision: 0.0000e+00 - Recall: 0.0000e+00\n",
      "Epoch 22/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6177 - accuracy: 0.6755 - Precision: 0.4000 - Recall: 0.0040\n",
      "Epoch 23/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6150 - accuracy: 0.6780 - Precision: 0.7143 - Recall: 0.0099\n",
      "Epoch 24/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6166 - accuracy: 0.6742 - Precision: 0.2000 - Recall: 0.0020  \n",
      "Epoch 25/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6148 - accuracy: 0.6761 - Precision: 0.0000e+00 - Recall: 0.0000e+00\n",
      "Epoch 26/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6147 - accuracy: 0.6755 - Precision: 0.4286 - Recall: 0.0060\n",
      "Epoch 27/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6128 - accuracy: 0.6742 - Precision: 0.4483 - Recall: 0.0258\n",
      "Epoch 28/100\n",
      "1553/1553 [==============================] - 3s 2ms/sample - loss: 0.6109 - accuracy: 0.6768 - Precision: 0.6667 - Recall: 0.0040\n",
      "Epoch 29/100\n",
      "1553/1553 [==============================] - 3s 2ms/sample - loss: 0.6145 - accuracy: 0.6755 - Precision: 0.0000e+00 - Recall: 0.0000e+00 0s - loss: 0.6126 - accuracy: 0.6804 - Precision: 0.0000e+00 - Recal\n",
      "Epoch 30/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6143 - accuracy: 0.6684 - Precision: 0.1667 - Recall: 0.0060\n",
      "Epoch 31/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6083 - accuracy: 0.6748 - Precision: 0.3333 - Recall: 0.0040\n",
      "Epoch 32/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6097 - accuracy: 0.6742 - Precision: 0.2857 - Recall: 0.0040\n",
      "Epoch 33/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6131 - accuracy: 0.6774 - Precision: 1.0000 - Recall: 0.0040\n",
      "Epoch 34/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6130 - accuracy: 0.6748 - Precision: 0.0000e+00 - Recall: 0.0000e+00\n",
      "Epoch 35/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6090 - accuracy: 0.6742 - Precision: 0.2000 - Recall: 0.0020\n",
      "Epoch 36/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6072 - accuracy: 0.6735 - Precision: 0.1667 - Recall: 0.0020\n",
      "Epoch 37/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6067 - accuracy: 0.6729 - Precision: 0.2727 - Recall: 0.0060\n",
      "Epoch 38/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6076 - accuracy: 0.6755 - Precision: 0.0000e+00 - Recall: 0.0000e+00\n",
      "Epoch 39/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6050 - accuracy: 0.6755 - Precision: 0.4000 - Recall: 0.0040\n",
      "Epoch 40/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6017 - accuracy: 0.6722 - Precision: 0.2000 - Recall: 0.0040\n",
      "Epoch 41/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6041 - accuracy: 0.6774 - Precision: 0.5714 - Recall: 0.0159\n",
      "Epoch 42/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6027 - accuracy: 0.6742 - Precision: 0.4634 - Recall: 0.0378\n",
      "Epoch 43/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6062 - accuracy: 0.6748 - Precision: 0.4000 - Recall: 0.0080\n",
      "Epoch 44/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6052 - accuracy: 0.6703 - Precision: 0.2632 - Recall: 0.0099\n",
      "Epoch 45/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6029 - accuracy: 0.6755 - Precision: 0.4706 - Recall: 0.0159\n",
      "Epoch 46/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6047 - accuracy: 0.6722 - Precision: 0.3636 - Recall: 0.0159\n",
      "Epoch 47/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6014 - accuracy: 0.6716 - Precision: 0.3478 - Recall: 0.0159\n",
      "Epoch 48/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6024 - accuracy: 0.6729 - Precision: 0.4419 - Recall: 0.0378\n",
      "Epoch 49/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6021 - accuracy: 0.6729 - Precision: 0.4194 - Recall: 0.0258\n",
      "Epoch 50/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6038 - accuracy: 0.6735 - Precision: 0.4231 - Recall: 0.0219\n",
      "Epoch 51/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6026 - accuracy: 0.6645 - Precision: 0.3594 - Recall: 0.0457\n",
      "Epoch 52/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6050 - accuracy: 0.6677 - Precision: 0.3725 - Recall: 0.0378\n",
      "Epoch 53/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6005 - accuracy: 0.6665 - Precision: 0.3077 - Recall: 0.0239\n",
      "Epoch 54/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6027 - accuracy: 0.6697 - Precision: 0.3438 - Recall: 0.0219\n",
      "Epoch 55/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5995 - accuracy: 0.6768 - Precision: 0.5172 - Recall: 0.0298\n",
      "Epoch 56/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6024 - accuracy: 0.6729 - Precision: 0.4324 - Recall: 0.0318\n",
      "Epoch 57/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5968 - accuracy: 0.6787 - Precision: 0.5769 - Recall: 0.0298\n",
      "Epoch 58/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6026 - accuracy: 0.6671 - Precision: 0.3600 - Recall: 0.0358\n",
      "Epoch 59/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6016 - accuracy: 0.6787 - Precision: 0.5476 - Recall: 0.0457\n",
      "Epoch 60/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6014 - accuracy: 0.6729 - Precision: 0.3810 - Recall: 0.0159\n",
      "Epoch 61/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5960 - accuracy: 0.6742 - Precision: 0.4286 - Recall: 0.0179\n",
      "Epoch 62/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5996 - accuracy: 0.6787 - Precision: 0.5400 - Recall: 0.0537\n",
      "Epoch 63/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5967 - accuracy: 0.6671 - Precision: 0.3833 - Recall: 0.0457\n",
      "Epoch 64/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5992 - accuracy: 0.6710 - Precision: 0.4231 - Recall: 0.0437\n",
      "Epoch 65/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5916 - accuracy: 0.6780 - Precision: 0.5238 - Recall: 0.0656\n",
      "Epoch 66/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5965 - accuracy: 0.6761 - Precision: 0.5000 - Recall: 0.0934\n",
      "Epoch 67/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6002 - accuracy: 0.6768 - Precision: 0.5059 - Recall: 0.0855\n",
      "Epoch 68/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5975 - accuracy: 0.6787 - Precision: 0.5588 - Recall: 0.0378\n",
      "Epoch 69/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5984 - accuracy: 0.6716 - Precision: 0.4493 - Recall: 0.0616\n",
      "Epoch 70/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6007 - accuracy: 0.6690 - Precision: 0.4225 - Recall: 0.0596\n",
      "Epoch 71/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5998 - accuracy: 0.6774 - Precision: 0.5185 - Recall: 0.0557\n",
      "Epoch 72/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.6009 - accuracy: 0.6735 - Precision: 0.4600 - Recall: 0.0457\n",
      "Epoch 73/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5965 - accuracy: 0.6722 - Precision: 0.4559 - Recall: 0.0616\n",
      "Epoch 74/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5970 - accuracy: 0.6742 - Precision: 0.4706 - Recall: 0.0477\n",
      "Epoch 75/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5992 - accuracy: 0.6710 - Precision: 0.4412 - Recall: 0.0596\n",
      "Epoch 76/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5977 - accuracy: 0.6793 - Precision: 0.5581 - Recall: 0.0477\n",
      "Epoch 77/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5943 - accuracy: 0.6793 - Precision: 0.5472 - Recall: 0.0577\n",
      "Epoch 78/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5983 - accuracy: 0.6710 - Precision: 0.4500 - Recall: 0.0716\n",
      "Epoch 79/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5923 - accuracy: 0.6800 - Precision: 0.5750 - Recall: 0.0457\n",
      "Epoch 80/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5949 - accuracy: 0.6787 - Precision: 0.5244 - Recall: 0.0855\n",
      "Epoch 81/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5951 - accuracy: 0.6825 - Precision: 0.5833 - Recall: 0.0696\n",
      "Epoch 82/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5978 - accuracy: 0.6742 - Precision: 0.4795 - Recall: 0.0696\n",
      "Epoch 83/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5972 - accuracy: 0.6703 - Precision: 0.4262 - Recall: 0.0517\n",
      "Epoch 84/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5954 - accuracy: 0.6710 - Precision: 0.4394 - Recall: 0.0577\n",
      "Epoch 85/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5884 - accuracy: 0.6832 - Precision: 0.5965 - Recall: 0.0676\n",
      "Epoch 86/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5929 - accuracy: 0.6787 - Precision: 0.5294 - Recall: 0.0716\n",
      "Epoch 87/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5961 - accuracy: 0.6768 - Precision: 0.5135 - Recall: 0.0378\n",
      "Epoch 88/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5951 - accuracy: 0.6768 - Precision: 0.5082 - Recall: 0.0616\n",
      "Epoch 89/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5915 - accuracy: 0.6825 - Precision: 0.5962 - Recall: 0.0616\n",
      "Epoch 90/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5911 - accuracy: 0.6787 - Precision: 0.5323 - Recall: 0.0656\n",
      "Epoch 91/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5941 - accuracy: 0.6774 - Precision: 0.5294 - Recall: 0.0358\n",
      "Epoch 92/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5938 - accuracy: 0.6774 - Precision: 0.5185 - Recall: 0.0557\n",
      "Epoch 93/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5933 - accuracy: 0.6761 - Precision: 0.5000 - Recall: 0.0696\n",
      "Epoch 94/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5911 - accuracy: 0.6800 - Precision: 0.5577 - Recall: 0.0577\n",
      "Epoch 95/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5926 - accuracy: 0.6838 - Precision: 0.6250 - Recall: 0.0596\n",
      "Epoch 96/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5950 - accuracy: 0.6800 - Precision: 0.5455 - Recall: 0.0716\n",
      "Epoch 97/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5885 - accuracy: 0.6806 - Precision: 0.5660 - Recall: 0.0596\n",
      "Epoch 98/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5881 - accuracy: 0.6793 - Precision: 0.5397 - Recall: 0.0676\n",
      "Epoch 99/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5925 - accuracy: 0.6819 - Precision: 0.6000 - Recall: 0.0537\n",
      "Epoch 100/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5945 - accuracy: 0.6716 - Precision: 0.4462 - Recall: 0.0577\n",
      "accuracy: 66.47%\n",
      "Precision: 0.00%\n",
      "Recall: 0.00%\n",
      "Fitting model...\n",
      "Train on 1553 samples\n",
      "Epoch 1/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5938 - accuracy: 0.6819 - Precision: 0.6047 - Recall: 0.0517\n",
      "Epoch 2/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5935 - accuracy: 0.6768 - Precision: 0.5098 - Recall: 0.0517\n",
      "Epoch 3/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5939 - accuracy: 0.6722 - Precision: 0.4062 - Recall: 0.0258\n",
      "Epoch 4/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5918 - accuracy: 0.6825 - Precision: 0.5714 - Recall: 0.0795\n",
      "Epoch 5/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5964 - accuracy: 0.6748 - Precision: 0.4815 - Recall: 0.0517\n",
      "Epoch 6/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5952 - accuracy: 0.6800 - Precision: 0.5625 - Recall: 0.0537\n",
      "Epoch 7/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5938 - accuracy: 0.6722 - Precision: 0.4500 - Recall: 0.0537\n",
      "Epoch 8/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5892 - accuracy: 0.6780 - Precision: 0.5217 - Recall: 0.0716\n",
      "Epoch 9/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5944 - accuracy: 0.6710 - Precision: 0.4286 - Recall: 0.0477\n",
      "Epoch 10/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5927 - accuracy: 0.6806 - Precision: 0.6129 - Recall: 0.0378\n",
      "Epoch 11/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5897 - accuracy: 0.6825 - Precision: 0.6190 - Recall: 0.0517\n",
      "Epoch 12/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5929 - accuracy: 0.6806 - Precision: 0.5614 - Recall: 0.0636\n",
      "Epoch 13/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5940 - accuracy: 0.6774 - Precision: 0.5217 - Recall: 0.0477\n",
      "Epoch 14/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5906 - accuracy: 0.6768 - Precision: 0.5111 - Recall: 0.0457\n",
      "Epoch 15/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5900 - accuracy: 0.6729 - Precision: 0.4590 - Recall: 0.0557\n",
      "Epoch 16/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5915 - accuracy: 0.6742 - Precision: 0.4595 - Recall: 0.0338\n",
      "Epoch 17/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5875 - accuracy: 0.6806 - Precision: 0.5538 - Recall: 0.0716\n",
      "Epoch 18/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5887 - accuracy: 0.6787 - Precision: 0.5500 - Recall: 0.0437\n",
      "Epoch 19/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5847 - accuracy: 0.6780 - Precision: 0.5254 - Recall: 0.0616\n",
      "Epoch 20/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5936 - accuracy: 0.6780 - Precision: 0.5263 - Recall: 0.0596\n",
      "Epoch 21/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5910 - accuracy: 0.6716 - Precision: 0.4407 - Recall: 0.0517\n",
      "Epoch 22/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5906 - accuracy: 0.6832 - Precision: 0.6122 - Recall: 0.0596\n",
      "Epoch 23/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5897 - accuracy: 0.6780 - Precision: 0.5319 - Recall: 0.0497\n",
      "Epoch 24/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5886 - accuracy: 0.6755 - Precision: 0.4937 - Recall: 0.0775\n",
      "Epoch 25/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5936 - accuracy: 0.6800 - Precision: 0.5600 - Recall: 0.0557\n",
      "Epoch 26/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5912 - accuracy: 0.6780 - Precision: 0.5263 - Recall: 0.0596\n",
      "Epoch 27/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5875 - accuracy: 0.6864 - Precision: 0.6212 - Recall: 0.0815\n",
      "Epoch 28/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5877 - accuracy: 0.6819 - Precision: 0.5652 - Recall: 0.0775\n",
      "Epoch 29/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5919 - accuracy: 0.6761 - Precision: 0.5000 - Recall: 0.0696\n",
      "Epoch 30/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5900 - accuracy: 0.6774 - Precision: 0.5119 - Recall: 0.0855\n",
      "Epoch 31/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5837 - accuracy: 0.6755 - Precision: 0.4932 - Recall: 0.0716\n",
      "Epoch 32/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5899 - accuracy: 0.6806 - Precision: 0.5574 - Recall: 0.0676\n",
      "Epoch 33/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5815 - accuracy: 0.6883 - Precision: 0.6203 - Recall: 0.0974\n",
      "Epoch 34/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5911 - accuracy: 0.6793 - Precision: 0.5352 - Recall: 0.0755\n",
      "Epoch 35/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5933 - accuracy: 0.6748 - Precision: 0.4815 - Recall: 0.0517\n",
      "Epoch 36/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5875 - accuracy: 0.6761 - Precision: 0.5000 - Recall: 0.0596\n",
      "Epoch 37/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5933 - accuracy: 0.6780 - Precision: 0.5385 - Recall: 0.0417\n",
      "Epoch 38/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5875 - accuracy: 0.6780 - Precision: 0.5319 - Recall: 0.0497\n",
      "Epoch 39/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5839 - accuracy: 0.6800 - Precision: 0.5429 - Recall: 0.0755\n",
      "Epoch 40/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5926 - accuracy: 0.6722 - Precision: 0.4559 - Recall: 0.0616\n",
      "Epoch 41/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5860 - accuracy: 0.6819 - Precision: 0.5763 - Recall: 0.0676\n",
      "Epoch 42/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5899 - accuracy: 0.6768 - Precision: 0.5128 - Recall: 0.0398\n",
      "Epoch 43/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5907 - accuracy: 0.6819 - Precision: 0.5652 - Recall: 0.0775\n",
      "Epoch 44/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5858 - accuracy: 0.6825 - Precision: 0.5714 - Recall: 0.0795\n",
      "Epoch 45/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5855 - accuracy: 0.6800 - Precision: 0.5349 - Recall: 0.0915\n",
      "Epoch 46/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5889 - accuracy: 0.6774 - Precision: 0.5227 - Recall: 0.0457 0s - loss: 0.5824 - accuracy: 0.6775 - Precision: 0.4857 - Recall: 0\n",
      "Epoch 47/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5895 - accuracy: 0.6787 - Precision: 0.5323 - Recall: 0.0656\n",
      "Epoch 48/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5900 - accuracy: 0.6735 - Precision: 0.4655 - Recall: 0.0537\n",
      "Epoch 49/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5910 - accuracy: 0.6774 - Precision: 0.5152 - Recall: 0.0676\n",
      "Epoch 50/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5842 - accuracy: 0.6813 - Precision: 0.5714 - Recall: 0.0636\n",
      "Epoch 51/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5894 - accuracy: 0.6722 - Precision: 0.4559 - Recall: 0.0616\n",
      "Epoch 52/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5859 - accuracy: 0.6742 - Precision: 0.4815 - Recall: 0.0775\n",
      "Epoch 53/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5861 - accuracy: 0.6800 - Precision: 0.5750 - Recall: 0.0457\n",
      "Epoch 54/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5830 - accuracy: 0.6864 - Precision: 0.5930 - Recall: 0.1014\n",
      "Epoch 55/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5887 - accuracy: 0.6780 - Precision: 0.5385 - Recall: 0.0417\n",
      "Epoch 56/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5897 - accuracy: 0.6761 - Precision: 0.5000 - Recall: 0.0775\n",
      "Epoch 57/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5790 - accuracy: 0.6813 - Precision: 0.5465 - Recall: 0.0934\n",
      "Epoch 58/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5887 - accuracy: 0.6761 - Precision: 0.5000 - Recall: 0.0716\n",
      "Epoch 59/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5887 - accuracy: 0.6832 - Precision: 0.5663 - Recall: 0.0934\n",
      "Epoch 60/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5840 - accuracy: 0.6819 - Precision: 0.5652 - Recall: 0.0775\n",
      "Epoch 61/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5815 - accuracy: 0.6755 - Precision: 0.4925 - Recall: 0.0656\n",
      "Epoch 62/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5859 - accuracy: 0.6800 - Precision: 0.5469 - Recall: 0.0696\n",
      "Epoch 63/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5818 - accuracy: 0.6806 - Precision: 0.5636 - Recall: 0.0616\n",
      "Epoch 64/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5863 - accuracy: 0.6825 - Precision: 0.5806 - Recall: 0.0716\n",
      "Epoch 65/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5893 - accuracy: 0.6813 - Precision: 0.5588 - Recall: 0.0755\n",
      "Epoch 66/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5836 - accuracy: 0.6858 - Precision: 0.5688 - Recall: 0.1233\n",
      "Epoch 67/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5881 - accuracy: 0.6813 - Precision: 0.5667 - Recall: 0.0676\n",
      "Epoch 68/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5914 - accuracy: 0.6819 - Precision: 0.5849 - Recall: 0.0616\n",
      "Epoch 69/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5837 - accuracy: 0.6793 - Precision: 0.5362 - Recall: 0.0736\n",
      "Epoch 70/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5859 - accuracy: 0.6735 - Precision: 0.4767 - Recall: 0.0815\n",
      "Epoch 71/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5808 - accuracy: 0.6793 - Precision: 0.5197 - Recall: 0.1312\n",
      "Epoch 72/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5796 - accuracy: 0.6761 - Precision: 0.5000 - Recall: 0.0895\n",
      "Epoch 73/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5880 - accuracy: 0.6768 - Precision: 0.5088 - Recall: 0.0577\n",
      "Epoch 74/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5822 - accuracy: 0.6768 - Precision: 0.5094 - Recall: 0.0537\n",
      "Epoch 75/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5877 - accuracy: 0.6800 - Precision: 0.5517 - Recall: 0.0636\n",
      "Epoch 76/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5828 - accuracy: 0.6851 - Precision: 0.5833 - Recall: 0.0974\n",
      "Epoch 77/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5822 - accuracy: 0.6825 - Precision: 0.5641 - Recall: 0.0875\n",
      "Epoch 78/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5794 - accuracy: 0.6813 - Precision: 0.5465 - Recall: 0.0934\n",
      "Epoch 79/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5862 - accuracy: 0.6813 - Precision: 0.5444 - Recall: 0.0974\n",
      "Epoch 80/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5791 - accuracy: 0.6832 - Precision: 0.5567 - Recall: 0.1074\n",
      "Epoch 81/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5836 - accuracy: 0.6845 - Precision: 0.5670 - Recall: 0.1093\n",
      "Epoch 82/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5759 - accuracy: 0.6858 - Precision: 0.5758 - Recall: 0.1133\n",
      "Epoch 83/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5818 - accuracy: 0.6780 - Precision: 0.5122 - Recall: 0.1252\n",
      "Epoch 84/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5839 - accuracy: 0.6896 - Precision: 0.6129 - Recall: 0.1133\n",
      "Epoch 85/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5868 - accuracy: 0.6832 - Precision: 0.5821 - Recall: 0.0775\n",
      "Epoch 86/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5883 - accuracy: 0.6825 - Precision: 0.5581 - Recall: 0.0954\n",
      "Epoch 87/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5848 - accuracy: 0.6813 - Precision: 0.5435 - Recall: 0.0994\n",
      "Epoch 88/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5838 - accuracy: 0.6858 - Precision: 0.5743 - Recall: 0.1153\n",
      "Epoch 89/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5776 - accuracy: 0.6787 - Precision: 0.5172 - Recall: 0.1193\n",
      "Epoch 90/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5840 - accuracy: 0.6755 - Precision: 0.4954 - Recall: 0.1074\n",
      "Epoch 91/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5792 - accuracy: 0.6851 - Precision: 0.5814 - Recall: 0.0994\n",
      "Epoch 92/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5754 - accuracy: 0.6883 - Precision: 0.5785 - Recall: 0.1392\n",
      "Epoch 93/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5828 - accuracy: 0.6793 - Precision: 0.5301 - Recall: 0.0875\n",
      "Epoch 94/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5806 - accuracy: 0.6806 - Precision: 0.5376 - Recall: 0.0994\n",
      "Epoch 95/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5761 - accuracy: 0.6825 - Precision: 0.5472 - Recall: 0.1153\n",
      "Epoch 96/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5786 - accuracy: 0.6838 - Precision: 0.5476 - Recall: 0.1372\n",
      "Epoch 97/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5757 - accuracy: 0.6825 - Precision: 0.5439 - Recall: 0.1233\n",
      "Epoch 98/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5834 - accuracy: 0.6768 - Precision: 0.5050 - Recall: 0.10140 - ETA: 1s - loss: 0.5342 - accuracy: 0.7667 - Precision: 0.5000\n",
      "Epoch 99/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5826 - accuracy: 0.6787 - Precision: 0.5175 - Recall: 0.1173\n",
      "Epoch 100/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5787 - accuracy: 0.6832 - Precision: 0.5487 - Recall: 0.1233\n",
      "accuracy: 68.21%\n",
      "Precision: 55.56%\n",
      "Recall: 8.93%\n",
      "Fitting model...\n",
      "Train on 1553 samples\n",
      "Epoch 1/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5835 - accuracy: 0.6871 - Precision: 0.5780 - Recall: 0.1252\n",
      "Epoch 2/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5832 - accuracy: 0.6871 - Precision: 0.5726 - Recall: 0.1332\n",
      "Epoch 3/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5833 - accuracy: 0.6761 - Precision: 0.5000 - Recall: 0.0994\n",
      "Epoch 4/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5847 - accuracy: 0.6813 - Precision: 0.5364 - Recall: 0.1173\n",
      "Epoch 5/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5780 - accuracy: 0.6800 - Precision: 0.5234 - Recall: 0.1332\n",
      "Epoch 6/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5786 - accuracy: 0.6768 - Precision: 0.5047 - Recall: 0.1074\n",
      "Epoch 7/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5742 - accuracy: 0.6825 - Precision: 0.5455 - Recall: 0.1193\n",
      "Epoch 8/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5817 - accuracy: 0.6864 - Precision: 0.5769 - Recall: 0.1193\n",
      "Epoch 9/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5814 - accuracy: 0.6832 - Precision: 0.5414 - Recall: 0.1431\n",
      "Epoch 10/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5828 - accuracy: 0.6832 - Precision: 0.5514 - Recall: 0.1173\n",
      "Epoch 11/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5811 - accuracy: 0.6851 - Precision: 0.5603 - Recall: 0.1292\n",
      "Epoch 12/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5805 - accuracy: 0.6819 - Precision: 0.5429 - Recall: 0.1133\n",
      "Epoch 13/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5771 - accuracy: 0.6851 - Precision: 0.5814 - Recall: 0.0994\n",
      "Epoch 14/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5741 - accuracy: 0.6845 - Precision: 0.5512 - Recall: 0.1392\n",
      "Epoch 15/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5788 - accuracy: 0.6793 - Precision: 0.5168 - Recall: 0.1531\n",
      "Epoch 16/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5812 - accuracy: 0.6858 - Precision: 0.5789 - Recall: 0.1093\n",
      "Epoch 17/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5883 - accuracy: 0.6729 - Precision: 0.4742 - Recall: 0.0915\n",
      "Epoch 18/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5811 - accuracy: 0.6774 - Precision: 0.5094 - Recall: 0.1074\n",
      "Epoch 19/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5777 - accuracy: 0.6890 - Precision: 0.5893 - Recall: 0.1312\n",
      "Epoch 20/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5744 - accuracy: 0.6896 - Precision: 0.5778 - Recall: 0.1551\n",
      "Epoch 21/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5766 - accuracy: 0.6774 - Precision: 0.5079 - Recall: 0.1272\n",
      "Epoch 22/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5773 - accuracy: 0.6877 - Precision: 0.5495 - Recall: 0.1988\n",
      "Epoch 23/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5758 - accuracy: 0.6838 - Precision: 0.5508 - Recall: 0.1292\n",
      "Epoch 24/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5728 - accuracy: 0.6871 - Precision: 0.5794 - Recall: 0.1233\n",
      "Epoch 25/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5755 - accuracy: 0.6858 - Precision: 0.5517 - Recall: 0.1590\n",
      "Epoch 26/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5774 - accuracy: 0.6825 - Precision: 0.5357 - Recall: 0.1491\n",
      "Epoch 27/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5788 - accuracy: 0.6742 - Precision: 0.4901 - Recall: 0.1471\n",
      "Epoch 28/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5762 - accuracy: 0.6896 - Precision: 0.5686 - Recall: 0.1730\n",
      "Epoch 29/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5800 - accuracy: 0.6838 - Precision: 0.5400 - Recall: 0.1610\n",
      "Epoch 30/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5739 - accuracy: 0.6903 - Precision: 0.5797 - Recall: 0.1590\n",
      "Epoch 31/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5730 - accuracy: 0.6864 - Precision: 0.5580 - Recall: 0.1531\n",
      "Epoch 32/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5771 - accuracy: 0.6954 - Precision: 0.6042 - Recall: 0.1730\n",
      "Epoch 33/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5784 - accuracy: 0.6825 - Precision: 0.5373 - Recall: 0.1431\n",
      "Epoch 34/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5818 - accuracy: 0.6819 - Precision: 0.5372 - Recall: 0.1292\n",
      "Epoch 35/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5736 - accuracy: 0.6883 - Precision: 0.5785 - Recall: 0.1392\n",
      "Epoch 36/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5816 - accuracy: 0.6877 - Precision: 0.5714 - Recall: 0.1431\n",
      "Epoch 37/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5650 - accuracy: 0.7019 - Precision: 0.6493 - Recall: 0.1730\n",
      "Epoch 38/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5776 - accuracy: 0.6948 - Precision: 0.6408 - Recall: 0.1312\n",
      "Epoch 39/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5723 - accuracy: 0.6768 - Precision: 0.5034 - Recall: 0.1471\n",
      "Epoch 40/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5785 - accuracy: 0.6813 - Precision: 0.5303 - Recall: 0.1392\n",
      "Epoch 41/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5729 - accuracy: 0.6890 - Precision: 0.5820 - Recall: 0.1412\n",
      "Epoch 42/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5721 - accuracy: 0.6961 - Precision: 0.6026 - Recall: 0.1809\n",
      "Epoch 43/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5781 - accuracy: 0.6941 - Precision: 0.6094 - Recall: 0.1551\n",
      "Epoch 44/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5811 - accuracy: 0.6883 - Precision: 0.5798 - Recall: 0.1372\n",
      "Epoch 45/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5695 - accuracy: 0.6929 - Precision: 0.6048 - Recall: 0.1491\n",
      "Epoch 46/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5766 - accuracy: 0.6916 - Precision: 0.5732 - Recall: 0.1869\n",
      "Epoch 47/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5735 - accuracy: 0.6825 - Precision: 0.5357 - Recall: 0.1491\n",
      "Epoch 48/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5765 - accuracy: 0.6890 - Precision: 0.5980 - Recall: 0.1213\n",
      "Epoch 49/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5735 - accuracy: 0.6864 - Precision: 0.5702 - Recall: 0.1292  - ETA: 2s - loss: 0.5475 - accuracy: 0.7444 - Precision: 0.000\n",
      "Epoch 50/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5741 - accuracy: 0.6832 - Precision: 0.5396 - Recall: 0.1491\n",
      "Epoch 51/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5713 - accuracy: 0.6813 - Precision: 0.5303 - Recall: 0.1392\n",
      "Epoch 52/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5770 - accuracy: 0.6883 - Precision: 0.5772 - Recall: 0.1412\n",
      "Epoch 53/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5739 - accuracy: 0.6883 - Precision: 0.5736 - Recall: 0.1471\n",
      "Epoch 54/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5721 - accuracy: 0.6922 - Precision: 0.5984 - Recall: 0.1511\n",
      "Epoch 55/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5720 - accuracy: 0.6864 - Precision: 0.5563 - Recall: 0.1571\n",
      "Epoch 56/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5732 - accuracy: 0.6845 - Precision: 0.5565 - Recall: 0.1272\n",
      "Epoch 57/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5646 - accuracy: 0.6954 - Precision: 0.5974 - Recall: 0.1829\n",
      "Epoch 58/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5714 - accuracy: 0.6877 - Precision: 0.5479 - Recall: 0.2048\n",
      "Epoch 59/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5784 - accuracy: 0.6787 - Precision: 0.5143 - Recall: 0.1431\n",
      "Epoch 60/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5696 - accuracy: 0.6948 - Precision: 0.6261 - Recall: 0.1431\n",
      "Epoch 61/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5745 - accuracy: 0.6825 - Precision: 0.5333 - Recall: 0.1590\n",
      "Epoch 62/100\n",
      "1553/1553 [==============================] - 3s 2ms/sample - loss: 0.5732 - accuracy: 0.6883 - Precision: 0.5693 - Recall: 0.1551\n",
      "Epoch 63/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5768 - accuracy: 0.6742 - Precision: 0.4885 - Recall: 0.1272\n",
      "Epoch 64/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5808 - accuracy: 0.6864 - Precision: 0.5606 - Recall: 0.1471\n",
      "Epoch 65/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5665 - accuracy: 0.6903 - Precision: 0.5948 - Recall: 0.1372\n",
      "Epoch 66/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5647 - accuracy: 0.6806 - Precision: 0.5271 - Recall: 0.1352\n",
      "Epoch 67/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5762 - accuracy: 0.6916 - Precision: 0.5952 - Recall: 0.1491\n",
      "Epoch 68/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5748 - accuracy: 0.6735 - Precision: 0.4863 - Recall: 0.1412\n",
      "Epoch 69/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5785 - accuracy: 0.6793 - Precision: 0.5172 - Recall: 0.1491\n",
      "Epoch 70/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5729 - accuracy: 0.6941 - Precision: 0.6000 - Recall: 0.1670\n",
      "Epoch 71/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5748 - accuracy: 0.6819 - Precision: 0.5385 - Recall: 0.1252\n",
      "Epoch 72/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5626 - accuracy: 0.6916 - Precision: 0.5938 - Recall: 0.1511\n",
      "Epoch 73/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5707 - accuracy: 0.6742 - Precision: 0.4898 - Recall: 0.1431\n",
      "Epoch 74/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5718 - accuracy: 0.6929 - Precision: 0.5878 - Recall: 0.1730\n",
      "Epoch 75/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5731 - accuracy: 0.6858 - Precision: 0.5701 - Recall: 0.1213\n",
      "Epoch 76/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5727 - accuracy: 0.6851 - Precision: 0.5530 - Recall: 0.1451\n",
      "Epoch 77/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5706 - accuracy: 0.6890 - Precision: 0.5833 - Recall: 0.1392\n",
      "Epoch 78/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5720 - accuracy: 0.6864 - Precision: 0.5678 - Recall: 0.1332\n",
      "Epoch 79/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5732 - accuracy: 0.6954 - Precision: 0.6190 - Recall: 0.1551\n",
      "Epoch 80/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5689 - accuracy: 0.6877 - Precision: 0.5549 - Recall: 0.1809\n",
      "Epoch 81/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5673 - accuracy: 0.6864 - Precision: 0.5606 - Recall: 0.1471\n",
      "Epoch 82/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5730 - accuracy: 0.6800 - Precision: 0.5254 - Recall: 0.1233\n",
      "Epoch 83/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5676 - accuracy: 0.6961 - Precision: 0.6372 - Recall: 0.1431\n",
      "Epoch 84/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5696 - accuracy: 0.6825 - Precision: 0.5275 - Recall: 0.1909\n",
      "Epoch 85/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5692 - accuracy: 0.6858 - Precision: 0.5490 - Recall: 0.1670\n",
      "Epoch 86/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5703 - accuracy: 0.6832 - Precision: 0.5470 - Recall: 0.1272\n",
      "Epoch 87/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5729 - accuracy: 0.6896 - Precision: 0.5734 - Recall: 0.1630\n",
      "Epoch 88/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5718 - accuracy: 0.6710 - Precision: 0.4726 - Recall: 0.1372\n",
      "Epoch 89/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5668 - accuracy: 0.6832 - Precision: 0.5487 - Recall: 0.1233\n",
      "Epoch 90/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5690 - accuracy: 0.6877 - Precision: 0.5682 - Recall: 0.1491\n",
      "Epoch 91/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5711 - accuracy: 0.6877 - Precision: 0.5738 - Recall: 0.1392\n",
      "Epoch 92/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5717 - accuracy: 0.6806 - Precision: 0.5267 - Recall: 0.1372\n",
      "Epoch 93/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5735 - accuracy: 0.6877 - Precision: 0.5592 - Recall: 0.1690\n",
      "Epoch 94/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5749 - accuracy: 0.6941 - Precision: 0.5933 - Recall: 0.1769\n",
      "Epoch 95/100\n",
      "1553/1553 [==============================] - ETA: 0s - loss: 0.5767 - accuracy: 0.6954 - Precision: 0.6107 - Recall: 0.161 - 2s 1ms/sample - loss: 0.5757 - accuracy: 0.6961 - Precision: 0.6165 - Recall: 0.1630\n",
      "Epoch 96/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5623 - accuracy: 0.6954 - Precision: 0.6210 - Recall: 0.1531\n",
      "Epoch 97/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5625 - accuracy: 0.6941 - Precision: 0.6207 - Recall: 0.1431 0s - loss: 0.5641 - accuracy: 0.6957 - Precision: 0.6082 - Recall: 0.1\n",
      "Epoch 98/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5677 - accuracy: 0.6864 - Precision: 0.5421 - Recall: 0.2048\n",
      "Epoch 99/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5738 - accuracy: 0.6916 - Precision: 0.6000 - Recall: 0.1431\n",
      "Epoch 100/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5655 - accuracy: 0.6896 - Precision: 0.5854 - Recall: 0.1431\n",
      "accuracy: 67.05%\n",
      "Precision: 45.45%\n",
      "Recall: 8.93%\n",
      "Fitting model...\n",
      "Train on 1553 samples\n",
      "Epoch 1/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5790 - accuracy: 0.6954 - Precision: 0.6103 - Recall: 0.1650\n",
      "Epoch 2/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5787 - accuracy: 0.6896 - Precision: 0.5929 - Recall: 0.1332\n",
      "Epoch 3/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5685 - accuracy: 0.6877 - Precision: 0.6000 - Recall: 0.1074\n",
      "Epoch 4/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5739 - accuracy: 0.6825 - Precision: 0.5568 - Recall: 0.0974\n",
      "Epoch 5/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5708 - accuracy: 0.6851 - Precision: 0.5745 - Recall: 0.1074\n",
      "Epoch 6/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5759 - accuracy: 0.6858 - Precision: 0.5641 - Recall: 0.1312\n",
      "Epoch 7/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5724 - accuracy: 0.6941 - Precision: 0.5946 - Recall: 0.1750\n",
      "Epoch 8/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5734 - accuracy: 0.6806 - Precision: 0.5285 - Recall: 0.1292\n",
      "Epoch 9/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5806 - accuracy: 0.6858 - Precision: 0.5641 - Recall: 0.1312\n",
      "Epoch 10/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5696 - accuracy: 0.6858 - Precision: 0.5620 - Recall: 0.1352\n",
      "Epoch 11/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5722 - accuracy: 0.6999 - Precision: 0.6609 - Recall: 0.1511\n",
      "Epoch 12/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5763 - accuracy: 0.6787 - Precision: 0.5169 - Recall: 0.1213\n",
      "Epoch 13/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5783 - accuracy: 0.6909 - Precision: 0.5782 - Recall: 0.1690\n",
      "Epoch 14/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5726 - accuracy: 0.6813 - Precision: 0.5357 - Recall: 0.1193\n",
      "Epoch 15/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5729 - accuracy: 0.6864 - Precision: 0.5625 - Recall: 0.1431\n",
      "Epoch 16/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5748 - accuracy: 0.6787 - Precision: 0.5149 - Recall: 0.1372\n",
      "Epoch 17/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5708 - accuracy: 0.6896 - Precision: 0.5766 - Recall: 0.1571\n",
      "Epoch 18/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5734 - accuracy: 0.6871 - Precision: 0.5612 - Recall: 0.1551\n",
      "Epoch 19/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5738 - accuracy: 0.6845 - Precision: 0.5504 - Recall: 0.1412\n",
      "Epoch 20/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5709 - accuracy: 0.6871 - Precision: 0.5548 - Recall: 0.1710\n",
      "Epoch 21/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5688 - accuracy: 0.6845 - Precision: 0.5596 - Recall: 0.1213\n",
      "Epoch 22/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5655 - accuracy: 0.6858 - Precision: 0.5503 - Recall: 0.1630\n",
      "Epoch 23/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5698 - accuracy: 0.6845 - Precision: 0.5489 - Recall: 0.1451\n",
      "Epoch 24/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5621 - accuracy: 0.6909 - Precision: 0.5865 - Recall: 0.1551 0s - loss: 0.5635 - accuracy: 0.6878 - Precision: 0.5842 - Recall: \n",
      "Epoch 25/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5672 - accuracy: 0.6871 - Precision: 0.5521 - Recall: 0.1789\n",
      "Epoch 26/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5681 - accuracy: 0.6922 - Precision: 0.5850 - Recall: 0.1710\n",
      "Epoch 27/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5744 - accuracy: 0.6819 - Precision: 0.5385 - Recall: 0.1252\n",
      "Epoch 28/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5673 - accuracy: 0.6838 - Precision: 0.5411 - Recall: 0.1571\n",
      "Epoch 29/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5702 - accuracy: 0.6832 - Precision: 0.5470 - Recall: 0.1272\n",
      "Epoch 30/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5661 - accuracy: 0.6935 - Precision: 0.5746 - Recall: 0.2068\n",
      "Epoch 31/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5762 - accuracy: 0.6929 - Precision: 0.5915 - Recall: 0.1670\n",
      "Epoch 32/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5712 - accuracy: 0.6877 - Precision: 0.5804 - Recall: 0.1292\n",
      "Epoch 33/100\n",
      "1553/1553 [==============================] - ETA: 0s - loss: 0.5642 - accuracy: 0.6980 - Precision: 0.5946 - Recall: 0.221 - 2s 1ms/sample - loss: 0.5637 - accuracy: 0.6980 - Precision: 0.5895 - Recall: 0.2227\n",
      "Epoch 34/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5698 - accuracy: 0.6825 - Precision: 0.5338 - Recall: 0.1571\n",
      "Epoch 35/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5706 - accuracy: 0.6864 - Precision: 0.5500 - Recall: 0.1750\n",
      "Epoch 36/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5644 - accuracy: 0.6980 - Precision: 0.6024 - Recall: 0.1988\n",
      "Epoch 37/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5686 - accuracy: 0.6948 - Precision: 0.5810 - Recall: 0.2068\n",
      "Epoch 38/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5672 - accuracy: 0.6961 - Precision: 0.5756 - Recall: 0.2346\n",
      "Epoch 39/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5704 - accuracy: 0.6922 - Precision: 0.5698 - Recall: 0.2028A: 1s - loss: 0.5506 - accuracy: 0.7244 - Precision: 0.5\n",
      "Epoch 40/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5642 - accuracy: 0.6858 - Precision: 0.5349 - Recall: 0.2286\n",
      "Epoch 41/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5719 - accuracy: 0.6871 - Precision: 0.5556 - Recall: 0.1690\n",
      "Epoch 42/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5712 - accuracy: 0.6980 - Precision: 0.5934 - Recall: 0.2147\n",
      "Epoch 43/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5726 - accuracy: 0.6922 - Precision: 0.5668 - Recall: 0.2107\n",
      "Epoch 44/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5702 - accuracy: 0.6941 - Precision: 0.5986 - Recall: 0.1690\n",
      "Epoch 45/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5603 - accuracy: 0.6903 - Precision: 0.5679 - Recall: 0.1829\n",
      "Epoch 46/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5679 - accuracy: 0.6825 - Precision: 0.5329 - Recall: 0.1610\n",
      "Epoch 47/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5612 - accuracy: 0.6922 - Precision: 0.5796 - Recall: 0.1809\n",
      "Epoch 48/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5639 - accuracy: 0.6941 - Precision: 0.5805 - Recall: 0.2008\n",
      "Epoch 49/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5753 - accuracy: 0.6916 - Precision: 0.5667 - Recall: 0.2028\n",
      "Epoch 50/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5653 - accuracy: 0.6806 - Precision: 0.5238 - Recall: 0.1531\n",
      "Epoch 51/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5637 - accuracy: 0.6838 - Precision: 0.5370 - Recall: 0.1730\n",
      "Epoch 52/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5681 - accuracy: 0.6941 - Precision: 0.5787 - Recall: 0.2048\n",
      "Epoch 53/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5541 - accuracy: 0.6954 - Precision: 0.5806 - Recall: 0.2147\n",
      "Epoch 54/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5668 - accuracy: 0.6890 - Precision: 0.5495 - Recall: 0.2207\n",
      "Epoch 55/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5679 - accuracy: 0.6916 - Precision: 0.5800 - Recall: 0.1730\n",
      "Epoch 56/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5624 - accuracy: 0.7006 - Precision: 0.6056 - Recall: 0.2167\n",
      "Epoch 57/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5697 - accuracy: 0.6935 - Precision: 0.5754 - Recall: 0.2048\n",
      "Epoch 58/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5615 - accuracy: 0.6851 - Precision: 0.5437 - Recall: 0.1730\n",
      "Epoch 59/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5600 - accuracy: 0.7032 - Precision: 0.6082 - Recall: 0.2346\n",
      "Epoch 60/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5659 - accuracy: 0.6858 - Precision: 0.5355 - Recall: 0.2247\n",
      "Epoch 61/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5672 - accuracy: 0.6929 - Precision: 0.5699 - Recall: 0.2107\n",
      "Epoch 62/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5670 - accuracy: 0.6929 - Precision: 0.5707 - Recall: 0.2087\n",
      "Epoch 63/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5760 - accuracy: 0.6838 - Precision: 0.5484 - Recall: 0.1352\n",
      "Epoch 64/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5653 - accuracy: 0.7006 - Precision: 0.6044 - Recall: 0.2187\n",
      "Epoch 65/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5576 - accuracy: 0.6941 - Precision: 0.5686 - Recall: 0.2306\n",
      "Epoch 66/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5660 - accuracy: 0.6896 - Precision: 0.5533 - Recall: 0.2167\n",
      "Epoch 67/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5572 - accuracy: 0.6890 - Precision: 0.5543 - Recall: 0.2028\n",
      "Epoch 68/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5543 - accuracy: 0.6941 - Precision: 0.5729 - Recall: 0.2187\n",
      "Epoch 69/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5651 - accuracy: 0.6922 - Precision: 0.5641 - Recall: 0.2187\n",
      "Epoch 70/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5623 - accuracy: 0.7051 - Precision: 0.6066 - Recall: 0.2545\n",
      "Epoch 71/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5626 - accuracy: 0.7025 - Precision: 0.6258 - Recall: 0.2028\n",
      "Epoch 72/100\n",
      "1553/1553 [==============================] - 4s 2ms/sample - loss: 0.5592 - accuracy: 0.6838 - Precision: 0.5323 - Recall: 0.1968 1s - loss: 0.5721 - accuracy: 0.6893 - Precision: 0.\n",
      "Epoch 73/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5575 - accuracy: 0.7019 - Precision: 0.5962 - Recall: 0.2465\n",
      "Epoch 74/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5624 - accuracy: 0.6935 - Precision: 0.5628 - Recall: 0.2406\n",
      "Epoch 75/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5609 - accuracy: 0.6967 - Precision: 0.5755 - Recall: 0.2425\n",
      "Epoch 76/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5582 - accuracy: 0.7102 - Precision: 0.6167 - Recall: 0.2783\n",
      "Epoch 77/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5651 - accuracy: 0.7006 - Precision: 0.5896 - Recall: 0.2485\n",
      "Epoch 78/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5503 - accuracy: 0.6986 - Precision: 0.5799 - Recall: 0.2525\n",
      "Epoch 79/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5515 - accuracy: 0.7019 - Precision: 0.5952 - Recall: 0.2485\n",
      "Epoch 80/100\n",
      "1553/1553 [==============================] - 3s 2ms/sample - loss: 0.5673 - accuracy: 0.6896 - Precision: 0.5368 - Recall: 0.3042\n",
      "Epoch 81/100\n",
      "1553/1553 [==============================] - 3s 2ms/sample - loss: 0.5584 - accuracy: 0.6961 - Precision: 0.5756 - Recall: 0.2346\n",
      "Epoch 82/100\n",
      "1553/1553 [==============================] - 2s 2ms/sample - loss: 0.5650 - accuracy: 0.6806 - Precision: 0.5150 - Recall: 0.2386\n",
      "Epoch 83/100\n",
      "1553/1553 [==============================] - 2s 2ms/sample - loss: 0.5616 - accuracy: 0.6832 - Precision: 0.5274 - Recall: 0.2107\n",
      "Epoch 84/100\n",
      "1553/1553 [==============================] - 3s 2ms/sample - loss: 0.5625 - accuracy: 0.6948 - Precision: 0.5721 - Recall: 0.2286\n",
      "Epoch 85/100\n",
      "1553/1553 [==============================] - 3s 2ms/sample - loss: 0.5582 - accuracy: 0.7019 - Precision: 0.6020 - Recall: 0.2346\n",
      "Epoch 86/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5624 - accuracy: 0.6890 - Precision: 0.5505 - Recall: 0.2167\n",
      "Epoch 87/100\n",
      "1553/1553 [==============================] - 3s 2ms/sample - loss: 0.5504 - accuracy: 0.6916 - Precision: 0.5492 - Recall: 0.2664\n",
      "Epoch 88/100\n",
      "1553/1553 [==============================] - 2s 2ms/sample - loss: 0.5612 - accuracy: 0.6929 - Precision: 0.5575 - Recall: 0.2505\n",
      "Epoch 89/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5603 - accuracy: 0.6929 - Precision: 0.5802 - Recall: 0.1869\n",
      "Epoch 90/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5581 - accuracy: 0.6903 - Precision: 0.5524 - Recall: 0.2306\n",
      "Epoch 91/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5697 - accuracy: 0.6922 - Precision: 0.5758 - Recall: 0.1889\n",
      "Epoch 92/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5583 - accuracy: 0.6980 - Precision: 0.5825 - Recall: 0.2386 1s - loss: 0.5563 - accuracy: 0.7067 - Precision\n",
      "Epoch 93/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5608 - accuracy: 0.6986 - Precision: 0.5967 - Recall: 0.2147\n",
      "Epoch 94/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5620 - accuracy: 0.6935 - Precision: 0.5746 - Recall: 0.2068\n",
      "Epoch 95/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5651 - accuracy: 0.6948 - Precision: 0.5819 - Recall: 0.2048\n",
      "Epoch 96/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5511 - accuracy: 0.7044 - Precision: 0.5880 - Recall: 0.2922\n",
      "Epoch 97/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5511 - accuracy: 0.7057 - Precision: 0.6173 - Recall: 0.2406\n",
      "Epoch 98/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5583 - accuracy: 0.6980 - Precision: 0.5833 - Recall: 0.2366\n",
      "Epoch 99/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5523 - accuracy: 0.6980 - Precision: 0.5850 - Recall: 0.2326\n",
      "Epoch 100/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5651 - accuracy: 0.6974 - Precision: 0.5829 - Recall: 0.2306\n",
      "accuracy: 68.21%\n",
      "Precision: 55.56%\n",
      "Recall: 8.93%\n",
      "Fitting model...\n",
      "Train on 1553 samples\n",
      "Epoch 1/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5622 - accuracy: 0.6929 - Precision: 0.5691 - Recall: 0.2127\n",
      "Epoch 2/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5560 - accuracy: 0.6954 - Precision: 0.5789 - Recall: 0.2187\n",
      "Epoch 3/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5662 - accuracy: 0.6851 - Precision: 0.5372 - Recall: 0.2008\n",
      "Epoch 4/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5575 - accuracy: 0.7057 - Precision: 0.6278 - Recall: 0.2247\n",
      "Epoch 5/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5606 - accuracy: 0.6941 - Precision: 0.5560 - Recall: 0.2763\n",
      "Epoch 6/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5589 - accuracy: 0.6877 - Precision: 0.5349 - Recall: 0.2744\n",
      "Epoch 7/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5661 - accuracy: 0.6974 - Precision: 0.5760 - Recall: 0.2485\n",
      "Epoch 8/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5613 - accuracy: 0.7102 - Precision: 0.6188 - Recall: 0.2744\n",
      "Epoch 9/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5556 - accuracy: 0.6929 - Precision: 0.5560 - Recall: 0.2565\n",
      "Epoch 10/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5568 - accuracy: 0.6986 - Precision: 0.5837 - Recall: 0.2425\n",
      "Epoch 11/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5584 - accuracy: 0.7025 - Precision: 0.5865 - Recall: 0.2763\n",
      "Epoch 12/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5608 - accuracy: 0.6967 - Precision: 0.5784 - Recall: 0.2346\n",
      "Epoch 13/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5597 - accuracy: 0.6909 - Precision: 0.5466 - Recall: 0.2684\n",
      "Epoch 14/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5542 - accuracy: 0.6986 - Precision: 0.5806 - Recall: 0.2505\n",
      "Epoch 15/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5573 - accuracy: 0.7064 - Precision: 0.6169 - Recall: 0.2465 2s - loss: 0.5494 - accuracy: 0.7333 - Precisi\n",
      "Epoch 16/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5528 - accuracy: 0.7064 - Precision: 0.6073 - Recall: 0.2644\n",
      "Epoch 17/100\n",
      "1553/1553 [==============================] - 3s 2ms/sample - loss: 0.5585 - accuracy: 0.6961 - Precision: 0.5683 - Recall: 0.2565\n",
      "Epoch 18/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5469 - accuracy: 0.6961 - Precision: 0.5613 - Recall: 0.2823\n",
      "Epoch 19/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5540 - accuracy: 0.6929 - Precision: 0.5496 - Recall: 0.2863\n",
      "Epoch 20/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5557 - accuracy: 0.7064 - Precision: 0.5894 - Recall: 0.3082\n",
      "Epoch 21/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5502 - accuracy: 0.6999 - Precision: 0.5781 - Recall: 0.2724\n",
      "Epoch 22/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5510 - accuracy: 0.7064 - Precision: 0.6000 - Recall: 0.2803\n",
      "Epoch 23/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5482 - accuracy: 0.7057 - Precision: 0.5943 - Recall: 0.2883\n",
      "Epoch 24/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5544 - accuracy: 0.6961 - Precision: 0.5677 - Recall: 0.2584\n",
      "Epoch 25/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5511 - accuracy: 0.7038 - Precision: 0.5939 - Recall: 0.2704\n",
      "Epoch 26/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5481 - accuracy: 0.7057 - Precision: 0.6018 - Recall: 0.2704\n",
      "Epoch 27/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5543 - accuracy: 0.7019 - Precision: 0.5826 - Recall: 0.2803\n",
      "Epoch 28/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5579 - accuracy: 0.6929 - Precision: 0.5551 - Recall: 0.2604\n",
      "Epoch 29/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5485 - accuracy: 0.7019 - Precision: 0.5885 - Recall: 0.2644\n",
      "Epoch 30/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5552 - accuracy: 0.6896 - Precision: 0.5443 - Recall: 0.2565\n",
      "Epoch 31/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5530 - accuracy: 0.7006 - Precision: 0.5819 - Recall: 0.2684\n",
      "Epoch 32/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5625 - accuracy: 0.6980 - Precision: 0.5794 - Recall: 0.2465\n",
      "Epoch 33/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5500 - accuracy: 0.7077 - Precision: 0.5992 - Recall: 0.2942\n",
      "Epoch 34/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5478 - accuracy: 0.7019 - Precision: 0.5735 - Recall: 0.3101\n",
      "Epoch 35/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5519 - accuracy: 0.7032 - Precision: 0.5814 - Recall: 0.2982\n",
      "Epoch 36/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5470 - accuracy: 0.7057 - Precision: 0.5935 - Recall: 0.2903\n",
      "Epoch 37/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5467 - accuracy: 0.7038 - Precision: 0.5982 - Recall: 0.2604\n",
      "Epoch 38/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5582 - accuracy: 0.6883 - Precision: 0.5385 - Recall: 0.2644\n",
      "Epoch 39/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5515 - accuracy: 0.7044 - Precision: 0.5973 - Recall: 0.2684\n",
      "Epoch 40/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5551 - accuracy: 0.6986 - Precision: 0.5764 - Recall: 0.2624\n",
      "Epoch 41/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5514 - accuracy: 0.7077 - Precision: 0.5961 - Recall: 0.3022\n",
      "Epoch 42/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5620 - accuracy: 0.6967 - Precision: 0.5602 - Recall: 0.2962\n",
      "Epoch 43/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5437 - accuracy: 0.7044 - Precision: 0.5902 - Recall: 0.2863 1s - loss: 0.5258 - accuracy: 0.7267 - Precision\n",
      "Epoch 44/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5526 - accuracy: 0.7096 - Precision: 0.6102 - Recall: 0.2863\n",
      "Epoch 45/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5537 - accuracy: 0.6986 - Precision: 0.5703 - Recall: 0.2823\n",
      "Epoch 46/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5586 - accuracy: 0.7044 - Precision: 0.6019 - Recall: 0.2584\n",
      "Epoch 47/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5605 - accuracy: 0.6903 - Precision: 0.5420 - Recall: 0.2823\n",
      "Epoch 48/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5507 - accuracy: 0.7019 - Precision: 0.5885 - Recall: 0.2644\n",
      "Epoch 49/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5492 - accuracy: 0.7019 - Precision: 0.5885 - Recall: 0.2644\n",
      "Epoch 50/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5495 - accuracy: 0.7064 - Precision: 0.6083 - Recall: 0.2624\n",
      "Epoch 51/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5483 - accuracy: 0.7064 - Precision: 0.5959 - Recall: 0.2903\n",
      "Epoch 52/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5542 - accuracy: 0.7012 - Precision: 0.5783 - Recall: 0.2863\n",
      "Epoch 53/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5475 - accuracy: 0.7135 - Precision: 0.6198 - Recall: 0.2982\n",
      "Epoch 54/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5526 - accuracy: 0.6941 - Precision: 0.5496 - Recall: 0.3082\n",
      "Epoch 55/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5449 - accuracy: 0.7070 - Precision: 0.6043 - Recall: 0.2763\n",
      "Epoch 56/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5397 - accuracy: 0.7090 - Precision: 0.5970 - Recall: 0.3121\n",
      "Epoch 57/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5509 - accuracy: 0.7057 - Precision: 0.5927 - Recall: 0.2922\n",
      "Epoch 58/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5446 - accuracy: 0.7160 - Precision: 0.6220 - Recall: 0.3141\n",
      "Epoch 59/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5383 - accuracy: 0.7135 - Precision: 0.6133 - Recall: 0.3121\n",
      "Epoch 60/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5488 - accuracy: 0.7102 - Precision: 0.6109 - Recall: 0.2903\n",
      "Epoch 61/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5485 - accuracy: 0.7025 - Precision: 0.5928 - Recall: 0.2604\n",
      "Epoch 62/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5424 - accuracy: 0.7090 - Precision: 0.6000 - Recall: 0.3042\n",
      "Epoch 63/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5553 - accuracy: 0.6974 - Precision: 0.5673 - Recall: 0.2763\n",
      "Epoch 64/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5540 - accuracy: 0.6941 - Precision: 0.5511 - Recall: 0.3002\n",
      "Epoch 65/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5447 - accuracy: 0.7128 - Precision: 0.6014 - Recall: 0.3360\n",
      "Epoch 66/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5408 - accuracy: 0.7147 - Precision: 0.6327 - Recall: 0.2843\n",
      "Epoch 67/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5454 - accuracy: 0.7025 - Precision: 0.5810 - Recall: 0.2922\n",
      "Epoch 68/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5508 - accuracy: 0.7083 - Precision: 0.5893 - Recall: 0.3280\n",
      "Epoch 69/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5451 - accuracy: 0.7006 - Precision: 0.5709 - Recall: 0.3042\n",
      "Epoch 70/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5464 - accuracy: 0.7096 - Precision: 0.6111 - Recall: 0.2843\n",
      "Epoch 71/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5431 - accuracy: 0.7044 - Precision: 0.5853 - Recall: 0.3002\n",
      "Epoch 72/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5512 - accuracy: 0.7160 - Precision: 0.6220 - Recall: 0.3141\n",
      "Epoch 73/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5308 - accuracy: 0.7147 - Precision: 0.6261 - Recall: 0.2962\n",
      "Epoch 74/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5461 - accuracy: 0.6993 - Precision: 0.5703 - Recall: 0.2903\n",
      "Epoch 75/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5458 - accuracy: 0.6922 - Precision: 0.5536 - Recall: 0.2565\n",
      "Epoch 76/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5449 - accuracy: 0.6999 - Precision: 0.5768 - Recall: 0.2763\n",
      "Epoch 77/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5476 - accuracy: 0.6986 - Precision: 0.5726 - Recall: 0.2744\n",
      "Epoch 78/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5479 - accuracy: 0.7025 - Precision: 0.5810 - Recall: 0.2922\n",
      "Epoch 79/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5480 - accuracy: 0.6993 - Precision: 0.5750 - Recall: 0.2744\n",
      "Epoch 80/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5465 - accuracy: 0.6974 - Precision: 0.5579 - Recall: 0.3161\n",
      "Epoch 81/100\n",
      "1553/1553 [==============================] - 2s 2ms/sample - loss: 0.5468 - accuracy: 0.7051 - Precision: 0.5904 - Recall: 0.2922\n",
      "Epoch 82/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5365 - accuracy: 0.7128 - Precision: 0.6075 - Recall: 0.3201\n",
      "Epoch 83/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5384 - accuracy: 0.7012 - Precision: 0.5747 - Recall: 0.2982\n",
      "Epoch 84/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5400 - accuracy: 0.7128 - Precision: 0.6036 - Recall: 0.3300\n",
      "Epoch 85/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5448 - accuracy: 0.7135 - Precision: 0.6272 - Recall: 0.2843\n",
      "Epoch 86/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5440 - accuracy: 0.6961 - Precision: 0.5522 - Recall: 0.3260\n",
      "Epoch 87/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5492 - accuracy: 0.7038 - Precision: 0.5931 - Recall: 0.2724\n",
      "Epoch 88/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5394 - accuracy: 0.7090 - Precision: 0.5985 - Recall: 0.3082\n",
      "Epoch 89/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5419 - accuracy: 0.7096 - Precision: 0.5963 - Recall: 0.3201\n",
      "Epoch 90/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5423 - accuracy: 0.7096 - Precision: 0.5922 - Recall: 0.3320\n",
      "Epoch 91/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5370 - accuracy: 0.7070 - Precision: 0.5992 - Recall: 0.2883\n",
      "Epoch 92/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5382 - accuracy: 0.7167 - Precision: 0.6207 - Recall: 0.3221\n",
      "Epoch 93/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5482 - accuracy: 0.7070 - Precision: 0.5930 - Recall: 0.3042\n",
      "Epoch 94/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5389 - accuracy: 0.7019 - Precision: 0.5855 - Recall: 0.2724\n",
      "Epoch 95/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5380 - accuracy: 0.7077 - Precision: 0.5968 - Recall: 0.3002\n",
      "Epoch 96/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5487 - accuracy: 0.7057 - Precision: 0.5966 - Recall: 0.2823\n",
      "Epoch 97/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5422 - accuracy: 0.7038 - Precision: 0.5863 - Recall: 0.2903\n",
      "Epoch 98/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5364 - accuracy: 0.7231 - Precision: 0.6272 - Recall: 0.3579\n",
      "Epoch 99/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5369 - accuracy: 0.7090 - Precision: 0.6067 - Recall: 0.2883\n",
      "Epoch 100/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5371 - accuracy: 0.7122 - Precision: 0.6148 - Recall: 0.2982\n",
      "accuracy: 64.16%\n",
      "Precision: 43.18%\n",
      "Recall: 33.93%\n",
      "Fitting model...\n",
      "Train on 1553 samples\n",
      "Epoch 1/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5579 - accuracy: 0.6883 - Precision: 0.5408 - Recall: 0.2505\n",
      "Epoch 2/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5521 - accuracy: 0.7006 - Precision: 0.5779 - Recall: 0.2803\n",
      "Epoch 3/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5469 - accuracy: 0.6967 - Precision: 0.5510 - Recall: 0.3439\n",
      "Epoch 4/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5583 - accuracy: 0.6948 - Precision: 0.5523 - Recall: 0.3042\n",
      "Epoch 5/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5347 - accuracy: 0.7090 - Precision: 0.6016 - Recall: 0.3002\n",
      "Epoch 6/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5468 - accuracy: 0.7141 - Precision: 0.6007 - Recall: 0.3499\n",
      "Epoch 7/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5505 - accuracy: 0.6909 - Precision: 0.5466 - Recall: 0.2684\n",
      "Epoch 8/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5463 - accuracy: 0.6980 - Precision: 0.5620 - Recall: 0.3062\n",
      "Epoch 9/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5525 - accuracy: 0.6961 - Precision: 0.5618 - Recall: 0.2803\n",
      "Epoch 10/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5456 - accuracy: 0.7064 - Precision: 0.5741 - Recall: 0.3618\n",
      "Epoch 11/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5426 - accuracy: 0.7051 - Precision: 0.5889 - Recall: 0.2962\n",
      "Epoch 12/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5453 - accuracy: 0.6974 - Precision: 0.5579 - Recall: 0.3161\n",
      "Epoch 13/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5515 - accuracy: 0.6909 - Precision: 0.5455 - Recall: 0.2744\n",
      "Epoch 14/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5451 - accuracy: 0.7044 - Precision: 0.5902 - Recall: 0.2863\n",
      "Epoch 15/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5457 - accuracy: 0.6974 - Precision: 0.5690 - Recall: 0.2704\n",
      "Epoch 16/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5486 - accuracy: 0.6935 - Precision: 0.5538 - Recall: 0.2763\n",
      "Epoch 17/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5402 - accuracy: 0.7070 - Precision: 0.5938 - Recall: 0.3022\n",
      "Epoch 18/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5438 - accuracy: 0.7012 - Precision: 0.5809 - Recall: 0.2783\n",
      "Epoch 19/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5488 - accuracy: 0.6838 - Precision: 0.5246 - Recall: 0.2545\n",
      "Epoch 20/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5566 - accuracy: 0.6948 - Precision: 0.5668 - Recall: 0.2445\n",
      "Epoch 21/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5503 - accuracy: 0.7032 - Precision: 0.5745 - Recall: 0.3221\n",
      "Epoch 22/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5372 - accuracy: 0.7025 - Precision: 0.5655 - Recall: 0.3519\n",
      "Epoch 23/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5533 - accuracy: 0.6916 - Precision: 0.5508 - Recall: 0.2584\n",
      "Epoch 24/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5398 - accuracy: 0.7128 - Precision: 0.5993 - Recall: 0.3419\n",
      "Epoch 25/100\n",
      "1553/1553 [==============================] - 2s 2ms/sample - loss: 0.5465 - accuracy: 0.6986 - Precision: 0.5606 - Recall: 0.3221\n",
      "Epoch 26/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5321 - accuracy: 0.7032 - Precision: 0.5808 - Recall: 0.3002\n",
      "Epoch 27/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5363 - accuracy: 0.7083 - Precision: 0.5947 - Recall: 0.3121\n",
      "Epoch 28/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5381 - accuracy: 0.7019 - Precision: 0.5704 - Recall: 0.3221\n",
      "Epoch 29/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5402 - accuracy: 0.7173 - Precision: 0.6119 - Recall: 0.3479\n",
      "Epoch 30/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5389 - accuracy: 0.6986 - Precision: 0.5692 - Recall: 0.2863\n",
      "Epoch 31/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5412 - accuracy: 0.6993 - Precision: 0.5682 - Recall: 0.2982\n",
      "Epoch 32/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5344 - accuracy: 0.7032 - Precision: 0.5913 - Recall: 0.2704\n",
      "Epoch 33/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5410 - accuracy: 0.7102 - Precision: 0.5810 - Recall: 0.3777\n",
      "Epoch 34/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5444 - accuracy: 0.6948 - Precision: 0.5543 - Recall: 0.2942\n",
      "Epoch 35/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5424 - accuracy: 0.7012 - Precision: 0.5694 - Recall: 0.3181\n",
      "Epoch 36/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5379 - accuracy: 0.7141 - Precision: 0.5980 - Recall: 0.3579\n",
      "Epoch 37/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5393 - accuracy: 0.7096 - Precision: 0.5949 - Recall: 0.3241\n",
      "Epoch 38/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5379 - accuracy: 0.7122 - Precision: 0.5979 - Recall: 0.3400\n",
      "Epoch 39/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5487 - accuracy: 0.7057 - Precision: 0.5891 - Recall: 0.3022\n",
      "Epoch 40/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5401 - accuracy: 0.6903 - Precision: 0.5437 - Recall: 0.2724\n",
      "Epoch 41/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5407 - accuracy: 0.7122 - Precision: 0.5886 - Recall: 0.3698\n",
      "Epoch 42/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5452 - accuracy: 0.7044 - Precision: 0.5859 - Recall: 0.2982\n",
      "Epoch 43/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5351 - accuracy: 0.7012 - Precision: 0.5684 - Recall: 0.3221\n",
      "Epoch 44/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5322 - accuracy: 0.7160 - Precision: 0.6084 - Recall: 0.3459\n",
      "Epoch 45/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5319 - accuracy: 0.7135 - Precision: 0.6124 - Recall: 0.3141\n",
      "Epoch 46/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5432 - accuracy: 0.6903 - Precision: 0.5369 - Recall: 0.3181\n",
      "Epoch 47/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5369 - accuracy: 0.7019 - Precision: 0.5690 - Recall: 0.3280\n",
      "Epoch 48/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5355 - accuracy: 0.6845 - Precision: 0.5213 - Recall: 0.3161\n",
      "Epoch 49/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5328 - accuracy: 0.7044 - Precision: 0.5809 - Recall: 0.3141\n",
      "Epoch 50/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5437 - accuracy: 0.7006 - Precision: 0.5594 - Recall: 0.3559\n",
      "Epoch 51/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5408 - accuracy: 0.7115 - Precision: 0.5979 - Recall: 0.3340\n",
      "Epoch 52/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5276 - accuracy: 0.7154 - Precision: 0.5884 - Recall: 0.4036\n",
      "Epoch 53/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5403 - accuracy: 0.7038 - Precision: 0.5885 - Recall: 0.2843\n",
      "Epoch 54/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5307 - accuracy: 0.7019 - Precision: 0.5694 - Recall: 0.3260\n",
      "Epoch 55/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5327 - accuracy: 0.7057 - Precision: 0.5865 - Recall: 0.3101\n",
      "Epoch 56/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5418 - accuracy: 0.7038 - Precision: 0.5870 - Recall: 0.2883\n",
      "Epoch 57/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5330 - accuracy: 0.6961 - Precision: 0.5512 - Recall: 0.3320\n",
      "Epoch 58/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5408 - accuracy: 0.7038 - Precision: 0.5765 - Recall: 0.3221\n",
      "Epoch 59/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5425 - accuracy: 0.6980 - Precision: 0.5518 - Recall: 0.3598 0s - loss: 0.5272 - accuracy: 0.6953 - Precision: 0.5587 - Recall: 0\n",
      "Epoch 60/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5378 - accuracy: 0.7025 - Precision: 0.5817 - Recall: 0.2903\n",
      "Epoch 61/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5335 - accuracy: 0.6993 - Precision: 0.5692 - Recall: 0.2942\n",
      "Epoch 62/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5315 - accuracy: 0.7173 - Precision: 0.6032 - Recall: 0.3718\n",
      "Epoch 63/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5362 - accuracy: 0.7044 - Precision: 0.5692 - Recall: 0.3598\n",
      "Epoch 64/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5380 - accuracy: 0.6929 - Precision: 0.5474 - Recall: 0.2982\n",
      "Epoch 65/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5217 - accuracy: 0.7147 - Precision: 0.6007 - Recall: 0.3559\n",
      "Epoch 66/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5403 - accuracy: 0.7064 - Precision: 0.5874 - Recall: 0.3141\n",
      "Epoch 67/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5255 - accuracy: 0.7186 - Precision: 0.6289 - Recall: 0.3201\n",
      "Epoch 68/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5188 - accuracy: 0.6980 - Precision: 0.5616 - Recall: 0.3082\n",
      "Epoch 69/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5375 - accuracy: 0.7025 - Precision: 0.5627 - Recall: 0.3658\n",
      "Epoch 70/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5367 - accuracy: 0.7115 - Precision: 0.5926 - Recall: 0.3499\n",
      "Epoch 71/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5333 - accuracy: 0.6999 - Precision: 0.5731 - Recall: 0.2883\n",
      "Epoch 72/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5337 - accuracy: 0.7012 - Precision: 0.5604 - Recall: 0.3598\n",
      "Epoch 73/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5269 - accuracy: 0.7186 - Precision: 0.6107 - Recall: 0.3618\n",
      "Epoch 74/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5255 - accuracy: 0.7064 - Precision: 0.5701 - Recall: 0.3797\n",
      "Epoch 75/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5321 - accuracy: 0.7122 - Precision: 0.6077 - Recall: 0.3141\n",
      "Epoch 76/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5330 - accuracy: 0.7141 - Precision: 0.6097 - Recall: 0.3260\n",
      "Epoch 77/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5314 - accuracy: 0.7057 - Precision: 0.5858 - Recall: 0.3121\n",
      "Epoch 78/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5386 - accuracy: 0.7019 - Precision: 0.5637 - Recall: 0.3519\n",
      "Epoch 79/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5269 - accuracy: 0.7154 - Precision: 0.6013 - Recall: 0.3598\n",
      "Epoch 80/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5256 - accuracy: 0.7147 - Precision: 0.6020 - Recall: 0.3519\n",
      "Epoch 81/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5262 - accuracy: 0.7051 - Precision: 0.5801 - Recall: 0.3241\n",
      "Epoch 82/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5296 - accuracy: 0.7147 - Precision: 0.6007 - Recall: 0.3559\n",
      "Epoch 83/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5240 - accuracy: 0.7205 - Precision: 0.6139 - Recall: 0.3698\n",
      "Epoch 84/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5290 - accuracy: 0.7064 - Precision: 0.5765 - Recall: 0.3519\n",
      "Epoch 85/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5294 - accuracy: 0.7051 - Precision: 0.5789 - Recall: 0.3280\n",
      "Epoch 86/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5254 - accuracy: 0.7193 - Precision: 0.6084 - Recall: 0.3738\n",
      "Epoch 87/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5183 - accuracy: 0.7077 - Precision: 0.5745 - Recall: 0.3757\n",
      "Epoch 88/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5306 - accuracy: 0.7160 - Precision: 0.6115 - Recall: 0.3380\n",
      "Epoch 89/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5195 - accuracy: 0.7135 - Precision: 0.5853 - Recall: 0.3956\n",
      "Epoch 90/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5331 - accuracy: 0.7115 - Precision: 0.5836 - Recall: 0.3817\n",
      "Epoch 91/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5218 - accuracy: 0.7135 - Precision: 0.5954 - Recall: 0.3598\n",
      "Epoch 92/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5311 - accuracy: 0.6980 - Precision: 0.5545 - Recall: 0.3439\n",
      "Epoch 93/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5295 - accuracy: 0.7032 - Precision: 0.5640 - Recall: 0.3678\n",
      "Epoch 94/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5224 - accuracy: 0.7154 - Precision: 0.6000 - Recall: 0.3638\n",
      "Epoch 95/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5275 - accuracy: 0.7154 - Precision: 0.5956 - Recall: 0.3777\n",
      "Epoch 96/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5310 - accuracy: 0.6999 - Precision: 0.5649 - Recall: 0.3201\n",
      "Epoch 97/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5221 - accuracy: 0.7135 - Precision: 0.6066 - Recall: 0.3280\n",
      "Epoch 98/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5153 - accuracy: 0.7064 - Precision: 0.5761 - Recall: 0.3539\n",
      "Epoch 99/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5257 - accuracy: 0.7238 - Precision: 0.6233 - Recall: 0.3718\n",
      "Epoch 100/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5265 - accuracy: 0.7141 - Precision: 0.5993 - Recall: 0.3539\n",
      "accuracy: 70.52%\n",
      "Precision: 53.85%\n",
      "Recall: 62.50%\n",
      "Fitting model...\n",
      "Train on 1553 samples\n",
      "Epoch 1/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5387 - accuracy: 0.7038 - Precision: 0.5683 - Recall: 0.3559\n",
      "Epoch 2/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5440 - accuracy: 0.7012 - Precision: 0.5796 - Recall: 0.2823\n",
      "Epoch 3/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5281 - accuracy: 0.7109 - Precision: 0.5957 - Recall: 0.3340\n",
      "Epoch 4/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5415 - accuracy: 0.7070 - Precision: 0.5896 - Recall: 0.3141\n",
      "Epoch 5/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5281 - accuracy: 0.7051 - Precision: 0.5789 - Recall: 0.3280\n",
      "Epoch 6/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5482 - accuracy: 0.7102 - Precision: 0.5943 - Recall: 0.3320\n",
      "Epoch 7/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5280 - accuracy: 0.7115 - Precision: 0.5965 - Recall: 0.3380\n",
      "Epoch 8/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5326 - accuracy: 0.7186 - Precision: 0.6170 - Recall: 0.3459\n",
      "Epoch 9/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5318 - accuracy: 0.6909 - Precision: 0.5406 - Recall: 0.3042\n",
      "Epoch 10/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5327 - accuracy: 0.7122 - Precision: 0.5927 - Recall: 0.3559\n",
      "Epoch 11/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5307 - accuracy: 0.7122 - Precision: 0.5979 - Recall: 0.3400\n",
      "Epoch 12/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5401 - accuracy: 0.7044 - Precision: 0.5733 - Recall: 0.3419\n",
      "Epoch 13/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5309 - accuracy: 0.7057 - Precision: 0.5906 - Recall: 0.2982\n",
      "Epoch 14/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5249 - accuracy: 0.7135 - Precision: 0.6295 - Recall: 0.2803\n",
      "Epoch 15/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5241 - accuracy: 0.7186 - Precision: 0.6107 - Recall: 0.3618\n",
      "Epoch 16/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5353 - accuracy: 0.7096 - Precision: 0.5970 - Recall: 0.3181\n",
      "Epoch 17/100\n",
      "1553/1553 [==============================] - 2s 2ms/sample - loss: 0.5346 - accuracy: 0.7147 - Precision: 0.6128 - Recall: 0.3241\n",
      "Epoch 18/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5196 - accuracy: 0.7244 - Precision: 0.6230 - Recall: 0.3777\n",
      "Epoch 19/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5198 - accuracy: 0.7115 - Precision: 0.5868 - Recall: 0.3698\n",
      "Epoch 20/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5214 - accuracy: 0.7167 - Precision: 0.6129 - Recall: 0.3400\n",
      "Epoch 21/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5293 - accuracy: 0.7038 - Precision: 0.5691 - Recall: 0.3519\n",
      "Epoch 22/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5229 - accuracy: 0.7115 - Precision: 0.5908 - Recall: 0.3559\n",
      "Epoch 23/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5320 - accuracy: 0.7154 - Precision: 0.5933 - Recall: 0.3857\n",
      "Epoch 24/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5306 - accuracy: 0.7006 - Precision: 0.5605 - Recall: 0.3499\n",
      "Epoch 25/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5148 - accuracy: 0.7231 - Precision: 0.6197 - Recall: 0.3757\n",
      "Epoch 26/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5185 - accuracy: 0.7141 - Precision: 0.5908 - Recall: 0.3817\n",
      "Epoch 27/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5275 - accuracy: 0.7064 - Precision: 0.5728 - Recall: 0.3678\n",
      "Epoch 28/100\n",
      "1553/1553 [==============================] - 3s 2ms/sample - loss: 0.5156 - accuracy: 0.7180 - Precision: 0.6140 - Recall: 0.3479\n",
      "Epoch 29/100\n",
      "1553/1553 [==============================] - 2s 2ms/sample - loss: 0.5328 - accuracy: 0.7205 - Precision: 0.6042 - Recall: 0.3976\n",
      "Epoch 30/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5149 - accuracy: 0.7154 - Precision: 0.6117 - Recall: 0.3320\n",
      "Epoch 31/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5192 - accuracy: 0.7193 - Precision: 0.6037 - Recall: 0.3877\n",
      "Epoch 32/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5198 - accuracy: 0.7283 - Precision: 0.6311 - Recall: 0.3877\n",
      "Epoch 33/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5371 - accuracy: 0.7090 - Precision: 0.5934 - Recall: 0.3221\n",
      "Epoch 34/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5188 - accuracy: 0.7012 - Precision: 0.5652 - Recall: 0.3360\n",
      "Epoch 35/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5251 - accuracy: 0.7225 - Precision: 0.6161 - Recall: 0.3797\n",
      "Epoch 36/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5132 - accuracy: 0.7173 - Precision: 0.6039 - Recall: 0.3698\n",
      "Epoch 37/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5223 - accuracy: 0.7006 - Precision: 0.5569 - Recall: 0.3698\n",
      "Epoch 38/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5257 - accuracy: 0.7193 - Precision: 0.6077 - Recall: 0.3757\n",
      "Epoch 39/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5282 - accuracy: 0.7128 - Precision: 0.5856 - Recall: 0.3877\n",
      "Epoch 40/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5159 - accuracy: 0.7231 - Precision: 0.6166 - Recall: 0.3837\n",
      "Epoch 41/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5210 - accuracy: 0.7270 - Precision: 0.6348 - Recall: 0.3698\n",
      "Epoch 42/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5279 - accuracy: 0.7296 - Precision: 0.6301 - Recall: 0.3996 1s - loss: 0.5238 - accuracy: 0.7294 - Precision: 0.63\n",
      "Epoch 43/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5072 - accuracy: 0.7296 - Precision: 0.6254 - Recall: 0.4115\n",
      "Epoch 44/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5252 - accuracy: 0.7032 - Precision: 0.5714 - Recall: 0.3340\n",
      "Epoch 45/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5224 - accuracy: 0.7270 - Precision: 0.6287 - Recall: 0.3837\n",
      "Epoch 46/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5143 - accuracy: 0.7122 - Precision: 0.5921 - Recall: 0.3579\n",
      "Epoch 47/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5094 - accuracy: 0.7173 - Precision: 0.6053 - Recall: 0.3658\n",
      "Epoch 48/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5216 - accuracy: 0.7096 - Precision: 0.5793 - Recall: 0.3777\n",
      "Epoch 49/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5163 - accuracy: 0.7180 - Precision: 0.6117 - Recall: 0.3539\n",
      "Epoch 50/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5199 - accuracy: 0.7122 - Precision: 0.5909 - Recall: 0.3618\n",
      "Epoch 51/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5198 - accuracy: 0.7231 - Precision: 0.6137 - Recall: 0.3917\n",
      "Epoch 52/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5166 - accuracy: 0.7173 - Precision: 0.6053 - Recall: 0.3658\n",
      "Epoch 53/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5226 - accuracy: 0.7212 - Precision: 0.6306 - Recall: 0.3360 1s - loss: 0.5032 - accuracy: 0.7444 - Precision: 0.7174 \n",
      "Epoch 54/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5208 - accuracy: 0.7154 - Precision: 0.5900 - Recall: 0.3976\n",
      "Epoch 55/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5129 - accuracy: 0.7296 - Precision: 0.6379 - Recall: 0.3817\n",
      "Epoch 56/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5162 - accuracy: 0.7263 - Precision: 0.6318 - Recall: 0.3718\n",
      "Epoch 57/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5140 - accuracy: 0.7147 - Precision: 0.6000 - Recall: 0.3579\n",
      "Epoch 58/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5306 - accuracy: 0.7109 - Precision: 0.5860 - Recall: 0.3658\n",
      "Epoch 59/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5067 - accuracy: 0.7250 - Precision: 0.6348 - Recall: 0.3559\n",
      "Epoch 60/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5127 - accuracy: 0.7218 - Precision: 0.6066 - Recall: 0.4016 1s - loss: 0.5227 - accuracy: 0.7333 - Precision: 0.6466\n",
      "Epoch 61/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5093 - accuracy: 0.7302 - Precision: 0.6448 - Recall: 0.3718\n",
      "Epoch 62/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5151 - accuracy: 0.7090 - Precision: 0.5718 - Recall: 0.4036\n",
      "Epoch 63/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5181 - accuracy: 0.7064 - Precision: 0.5836 - Recall: 0.3260\n",
      "Epoch 64/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5051 - accuracy: 0.7296 - Precision: 0.6343 - Recall: 0.3897\n",
      "Epoch 65/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5082 - accuracy: 0.7238 - Precision: 0.6259 - Recall: 0.3658\n",
      "Epoch 66/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5082 - accuracy: 0.7212 - Precision: 0.6167 - Recall: 0.3678\n",
      "Epoch 67/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5198 - accuracy: 0.7141 - Precision: 0.5955 - Recall: 0.3658\n",
      "Epoch 68/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5046 - accuracy: 0.7173 - Precision: 0.6159 - Recall: 0.3380\n",
      "Epoch 69/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5122 - accuracy: 0.7296 - Precision: 0.6269 - Recall: 0.4076\n",
      "Epoch 70/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5050 - accuracy: 0.7160 - Precision: 0.5981 - Recall: 0.3757\n",
      "Epoch 71/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5099 - accuracy: 0.7199 - Precision: 0.6076 - Recall: 0.3817\n",
      "Epoch 72/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5051 - accuracy: 0.7244 - Precision: 0.6113 - Recall: 0.4095\n",
      "Epoch 73/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5089 - accuracy: 0.7315 - Precision: 0.6369 - Recall: 0.3976\n",
      "Epoch 74/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5046 - accuracy: 0.7289 - Precision: 0.6235 - Recall: 0.4115\n",
      "Epoch 75/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5072 - accuracy: 0.7270 - Precision: 0.6376 - Recall: 0.3638\n",
      "Epoch 76/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5114 - accuracy: 0.7263 - Precision: 0.6242 - Recall: 0.3897\n",
      "Epoch 77/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5071 - accuracy: 0.7218 - Precision: 0.6282 - Recall: 0.3459\n",
      "Epoch 78/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5035 - accuracy: 0.7173 - Precision: 0.6119 - Recall: 0.3479\n",
      "Epoch 79/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5138 - accuracy: 0.7193 - Precision: 0.6106 - Recall: 0.3678\n",
      "Epoch 80/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5192 - accuracy: 0.7141 - Precision: 0.5865 - Recall: 0.3976\n",
      "Epoch 81/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5090 - accuracy: 0.7193 - Precision: 0.5982 - Recall: 0.4056\n",
      "Epoch 82/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5062 - accuracy: 0.7321 - Precision: 0.6485 - Recall: 0.3777\n",
      "Epoch 83/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.4977 - accuracy: 0.7270 - Precision: 0.6287 - Recall: 0.3837\n",
      "Epoch 84/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5213 - accuracy: 0.7051 - Precision: 0.5728 - Recall: 0.3519\n",
      "Epoch 85/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5091 - accuracy: 0.7289 - Precision: 0.6165 - Recall: 0.4314\n",
      "Epoch 86/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5104 - accuracy: 0.7193 - Precision: 0.6091 - Recall: 0.3718\n",
      "Epoch 87/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5096 - accuracy: 0.7115 - Precision: 0.5986 - Recall: 0.3320\n",
      "Epoch 88/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5053 - accuracy: 0.7373 - Precision: 0.6489 - Recall: 0.4115\n",
      "Epoch 89/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5107 - accuracy: 0.7334 - Precision: 0.6404 - Recall: 0.4036\n",
      "Epoch 90/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5066 - accuracy: 0.7186 - Precision: 0.6058 - Recall: 0.3757\n",
      "Epoch 91/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5049 - accuracy: 0.7186 - Precision: 0.5922 - Recall: 0.4215\n",
      "Epoch 92/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5022 - accuracy: 0.7225 - Precision: 0.6161 - Recall: 0.3797\n",
      "Epoch 93/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5056 - accuracy: 0.7167 - Precision: 0.5952 - Recall: 0.3917\n",
      "Epoch 94/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5042 - accuracy: 0.7199 - Precision: 0.6043 - Recall: 0.3917\n",
      "Epoch 95/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5039 - accuracy: 0.7270 - Precision: 0.6053 - Recall: 0.4513\n",
      "Epoch 96/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5047 - accuracy: 0.7231 - Precision: 0.6151 - Recall: 0.3877\n",
      "Epoch 97/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5038 - accuracy: 0.7263 - Precision: 0.6336 - Recall: 0.3678\n",
      "Epoch 98/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5105 - accuracy: 0.7160 - Precision: 0.5906 - Recall: 0.4016\n",
      "Epoch 99/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.5075 - accuracy: 0.7199 - Precision: 0.6126 - Recall: 0.3678\n",
      "Epoch 100/100\n",
      "1553/1553 [==============================] - 2s 1ms/sample - loss: 0.4972 - accuracy: 0.7444 - Precision: 0.6677 - Recall: 0.4195\n",
      "accuracy: 69.36%\n",
      "Precision: 54.05%\n",
      "Recall: 35.71%\n",
      "Fitting model...\n",
      "Train on 1554 samples\n",
      "Epoch 1/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.5095 - accuracy: 0.7194 - Precision: 0.6159 - Recall: 0.3539\n",
      "Epoch 2/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.5175 - accuracy: 0.7124 - Precision: 0.5800 - Recall: 0.4036\n",
      "Epoch 3/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.5119 - accuracy: 0.7207 - Precision: 0.6237 - Recall: 0.3459\n",
      "Epoch 4/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.5081 - accuracy: 0.7278 - Precision: 0.6493 - Recall: 0.3459\n",
      "Epoch 5/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.5158 - accuracy: 0.7111 - Precision: 0.5951 - Recall: 0.3360\n",
      "Epoch 6/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.5091 - accuracy: 0.7130 - Precision: 0.5947 - Recall: 0.3559\n",
      "Epoch 7/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.5205 - accuracy: 0.7136 - Precision: 0.6028 - Recall: 0.3380\n",
      "Epoch 8/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.5268 - accuracy: 0.7079 - Precision: 0.6283 - Recall: 0.2386\n",
      "Epoch 9/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.5021 - accuracy: 0.7175 - Precision: 0.6185 - Recall: 0.3320\n",
      "Epoch 10/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.5071 - accuracy: 0.7207 - Precision: 0.6432 - Recall: 0.3082\n",
      "Epoch 11/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.5091 - accuracy: 0.7214 - Precision: 0.6471 - Recall: 0.3062\n",
      "Epoch 12/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.5111 - accuracy: 0.7156 - Precision: 0.6196 - Recall: 0.3141\n",
      "Epoch 13/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.5194 - accuracy: 0.7130 - Precision: 0.6000 - Recall: 0.3400\n",
      "Epoch 14/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.5125 - accuracy: 0.7181 - Precision: 0.6208 - Recall: 0.3320\n",
      "Epoch 15/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.5074 - accuracy: 0.7252 - Precision: 0.6357 - Recall: 0.3539\n",
      "Epoch 16/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.5082 - accuracy: 0.7220 - Precision: 0.6310 - Recall: 0.3400\n",
      "Epoch 17/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.5080 - accuracy: 0.7227 - Precision: 0.6176 - Recall: 0.3757\n",
      "Epoch 18/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.5049 - accuracy: 0.7233 - Precision: 0.6246 - Recall: 0.3638\n",
      "Epoch 19/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.5039 - accuracy: 0.7297 - Precision: 0.6334 - Recall: 0.3917\n",
      "Epoch 20/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4981 - accuracy: 0.7317 - Precision: 0.6236 - Recall: 0.4314\n",
      "Epoch 21/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.5061 - accuracy: 0.7252 - Precision: 0.6293 - Recall: 0.3678\n",
      "Epoch 22/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4894 - accuracy: 0.7252 - Precision: 0.6275 - Recall: 0.3718\n",
      "Epoch 23/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.5040 - accuracy: 0.7181 - Precision: 0.6052 - Recall: 0.3718\n",
      "Epoch 24/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.5019 - accuracy: 0.7220 - Precision: 0.6149 - Recall: 0.3777\n",
      "Epoch 25/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.5128 - accuracy: 0.7143 - Precision: 0.6113 - Recall: 0.3221\n",
      "Epoch 26/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4931 - accuracy: 0.7265 - Precision: 0.6266 - Recall: 0.3837\n",
      "Epoch 27/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.5037 - accuracy: 0.7272 - Precision: 0.6145 - Recall: 0.4215\n",
      "Epoch 28/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4932 - accuracy: 0.7362 - Precision: 0.6388 - Recall: 0.4254\n",
      "Epoch 29/100\n",
      "1554/1554 [==============================] - 2s 2ms/sample - loss: 0.5108 - accuracy: 0.7233 - Precision: 0.6290 - Recall: 0.3539\n",
      "Epoch 30/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.5104 - accuracy: 0.7194 - Precision: 0.6037 - Recall: 0.3877\n",
      "Epoch 31/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.5042 - accuracy: 0.7239 - Precision: 0.6568 - Recall: 0.3082\n",
      "Epoch 32/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.5041 - accuracy: 0.7304 - Precision: 0.6479 - Recall: 0.3658\n",
      "Epoch 33/100\n",
      "1554/1554 [==============================] - 2s 2ms/sample - loss: 0.5141 - accuracy: 0.7143 - Precision: 0.5865 - Recall: 0.3976\n",
      "Epoch 34/100\n",
      "1554/1554 [==============================] - 2s 2ms/sample - loss: 0.5010 - accuracy: 0.7336 - Precision: 0.6469 - Recall: 0.3897\n",
      "Epoch 35/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4931 - accuracy: 0.7207 - Precision: 0.6102 - Recall: 0.3797\n",
      "Epoch 36/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4976 - accuracy: 0.7362 - Precision: 0.6545 - Recall: 0.3917\n",
      "Epoch 37/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.5066 - accuracy: 0.7291 - Precision: 0.6340 - Recall: 0.3857\n",
      "Epoch 38/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4899 - accuracy: 0.7284 - Precision: 0.6392 - Recall: 0.3698\n",
      "Epoch 39/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4899 - accuracy: 0.7342 - Precision: 0.6364 - Recall: 0.4175\n",
      "Epoch 40/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.5011 - accuracy: 0.7297 - Precision: 0.6246 - Recall: 0.4135\n",
      "Epoch 41/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.5154 - accuracy: 0.7111 - Precision: 0.6038 - Recall: 0.3121\n",
      "Epoch 42/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.5043 - accuracy: 0.7175 - Precision: 0.6203 - Recall: 0.3280\n",
      "Epoch 43/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.5018 - accuracy: 0.7188 - Precision: 0.6146 - Recall: 0.3519\n",
      "Epoch 44/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.5018 - accuracy: 0.7278 - Precision: 0.6307 - Recall: 0.3837\n",
      "Epoch 45/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4976 - accuracy: 0.7194 - Precision: 0.6044 - Recall: 0.3857\n",
      "Epoch 46/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.5100 - accuracy: 0.7201 - Precision: 0.6156 - Recall: 0.3598\n",
      "Epoch 47/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4929 - accuracy: 0.7317 - Precision: 0.6369 - Recall: 0.3976\n",
      "Epoch 48/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4992 - accuracy: 0.7239 - Precision: 0.6186 - Recall: 0.3837\n",
      "Epoch 49/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.5045 - accuracy: 0.7239 - Precision: 0.6217 - Recall: 0.3757\n",
      "Epoch 50/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.5028 - accuracy: 0.7156 - Precision: 0.5950 - Recall: 0.3797\n",
      "Epoch 51/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.5084 - accuracy: 0.7194 - Precision: 0.6070 - Recall: 0.3777\n",
      "Epoch 52/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4849 - accuracy: 0.7117 - Precision: 0.5873 - Recall: 0.3678\n",
      "Epoch 53/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4882 - accuracy: 0.7362 - Precision: 0.6587 - Recall: 0.3837\n",
      "Epoch 54/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.5028 - accuracy: 0.7413 - Precision: 0.6613 - Recall: 0.4115\n",
      "Epoch 55/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.5058 - accuracy: 0.7207 - Precision: 0.6146 - Recall: 0.3678\n",
      "Epoch 56/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4958 - accuracy: 0.7265 - Precision: 0.6309 - Recall: 0.3738\n",
      "Epoch 57/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.5062 - accuracy: 0.7079 - Precision: 0.5809 - Recall: 0.3499\n",
      "Epoch 58/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.5060 - accuracy: 0.7246 - Precision: 0.6206 - Recall: 0.3837\n",
      "Epoch 59/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4957 - accuracy: 0.7175 - Precision: 0.5920 - Recall: 0.4095\n",
      "Epoch 60/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4968 - accuracy: 0.7304 - Precision: 0.6364 - Recall: 0.3897\n",
      "Epoch 61/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4902 - accuracy: 0.7272 - Precision: 0.6278 - Recall: 0.3857\n",
      "Epoch 62/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4953 - accuracy: 0.7329 - Precision: 0.6528 - Recall: 0.3738\n",
      "Epoch 63/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4972 - accuracy: 0.7252 - Precision: 0.6138 - Recall: 0.4076\n",
      "Epoch 64/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.5024 - accuracy: 0.7265 - Precision: 0.6275 - Recall: 0.3817\n",
      "Epoch 65/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4988 - accuracy: 0.7175 - Precision: 0.6103 - Recall: 0.3519\n",
      "Epoch 66/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4977 - accuracy: 0.7259 - Precision: 0.6453 - Recall: 0.3400\n",
      "Epoch 67/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4854 - accuracy: 0.7259 - Precision: 0.6192 - Recall: 0.3976\n",
      "Epoch 68/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.5016 - accuracy: 0.7201 - Precision: 0.6206 - Recall: 0.3479\n",
      "Epoch 69/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4856 - accuracy: 0.7207 - Precision: 0.6088 - Recall: 0.3837\n",
      "Epoch 70/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4937 - accuracy: 0.7233 - Precision: 0.6090 - Recall: 0.4056\n",
      "Epoch 71/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4965 - accuracy: 0.7297 - Precision: 0.6352 - Recall: 0.3877\n",
      "Epoch 72/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.5093 - accuracy: 0.7201 - Precision: 0.5929 - Recall: 0.4314\n",
      "Epoch 73/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.5030 - accuracy: 0.7349 - Precision: 0.6619 - Recall: 0.3698\n",
      "Epoch 74/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4933 - accuracy: 0.7362 - Precision: 0.6729 - Recall: 0.3598\n",
      "Epoch 75/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4753 - accuracy: 0.7329 - Precision: 0.6447 - Recall: 0.3897\n",
      "Epoch 76/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4923 - accuracy: 0.7297 - Precision: 0.6388 - Recall: 0.3797\n",
      "Epoch 77/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4933 - accuracy: 0.7265 - Precision: 0.6318 - Recall: 0.3718\n",
      "Epoch 78/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.5016 - accuracy: 0.7220 - Precision: 0.6035 - Recall: 0.4115\n",
      "Epoch 79/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4882 - accuracy: 0.7284 - Precision: 0.6373 - Recall: 0.3738\n",
      "Epoch 80/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4927 - accuracy: 0.7207 - Precision: 0.6124 - Recall: 0.3738\n",
      "Epoch 81/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4937 - accuracy: 0.7368 - Precision: 0.6599 - Recall: 0.3857\n",
      "Epoch 82/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4994 - accuracy: 0.7169 - Precision: 0.6013 - Recall: 0.3718\n",
      "Epoch 83/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4882 - accuracy: 0.7310 - Precision: 0.6269 - Recall: 0.4175\n",
      "Epoch 84/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4998 - accuracy: 0.7297 - Precision: 0.6397 - Recall: 0.3777\n",
      "Epoch 85/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4885 - accuracy: 0.7420 - Precision: 0.6747 - Recall: 0.3917\n",
      "Epoch 86/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4898 - accuracy: 0.7432 - Precision: 0.6722 - Recall: 0.4036 0s - loss: 0.4910 - accuracy: 0.7447 - Precision: 0.6592 - Recall: 0.3\n",
      "Epoch 87/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4917 - accuracy: 0.7297 - Precision: 0.6239 - Recall: 0.4155\n",
      "Epoch 88/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.5050 - accuracy: 0.7342 - Precision: 0.6389 - Recall: 0.4115\n",
      "Epoch 89/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4878 - accuracy: 0.7201 - Precision: 0.6069 - Recall: 0.3837\n",
      "Epoch 90/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.5048 - accuracy: 0.7162 - Precision: 0.6020 - Recall: 0.3638\n",
      "Epoch 91/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4885 - accuracy: 0.7252 - Precision: 0.6242 - Recall: 0.3797\n",
      "Epoch 92/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4951 - accuracy: 0.7284 - Precision: 0.6262 - Recall: 0.3996\n",
      "Epoch 93/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4918 - accuracy: 0.7329 - Precision: 0.6486 - Recall: 0.3817\n",
      "Epoch 94/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4966 - accuracy: 0.7291 - Precision: 0.6306 - Recall: 0.3936\n",
      "Epoch 95/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4963 - accuracy: 0.7278 - Precision: 0.6250 - Recall: 0.3976\n",
      "Epoch 96/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4894 - accuracy: 0.7349 - Precision: 0.6383 - Recall: 0.4175 0s - loss: 0.4840 - accuracy: 0.7427 - Precision: 0.6722 - Recall:\n",
      "Epoch 97/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4842 - accuracy: 0.7278 - Precision: 0.6205 - Recall: 0.4095\n",
      "Epoch 98/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4986 - accuracy: 0.7368 - Precision: 0.6655 - Recall: 0.3757\n",
      "Epoch 99/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4994 - accuracy: 0.7169 - Precision: 0.6000 - Recall: 0.3757\n",
      "Epoch 100/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4966 - accuracy: 0.7188 - Precision: 0.5971 - Recall: 0.4036\n",
      "accuracy: 77.91%\n",
      "Precision: 73.68%\n",
      "Recall: 50.00%\n",
      "Fitting model...\n",
      "Train on 1554 samples\n",
      "Epoch 1/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4918 - accuracy: 0.7304 - Precision: 0.6148 - Recall: 0.4473\n",
      "Epoch 2/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4959 - accuracy: 0.7214 - Precision: 0.5994 - Recall: 0.4195\n",
      "Epoch 3/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4837 - accuracy: 0.7471 - Precision: 0.6618 - Recall: 0.4473\n",
      "Epoch 4/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4880 - accuracy: 0.7381 - Precision: 0.6194 - Recall: 0.4950\n",
      "Epoch 5/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4804 - accuracy: 0.7323 - Precision: 0.6225 - Recall: 0.4394\n",
      "Epoch 6/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4918 - accuracy: 0.7265 - Precision: 0.5947 - Recall: 0.4871\n",
      "Epoch 7/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4964 - accuracy: 0.7233 - Precision: 0.6109 - Recall: 0.3996\n",
      "Epoch 8/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4906 - accuracy: 0.7265 - Precision: 0.6242 - Recall: 0.3897\n",
      "Epoch 9/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4919 - accuracy: 0.7291 - Precision: 0.6114 - Recall: 0.4473\n",
      "Epoch 10/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4865 - accuracy: 0.7420 - Precision: 0.6527 - Recall: 0.4334\n",
      "Epoch 11/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4882 - accuracy: 0.7329 - Precision: 0.6294 - Recall: 0.4254\n",
      "Epoch 12/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4886 - accuracy: 0.7284 - Precision: 0.6209 - Recall: 0.4135\n",
      "Epoch 13/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4776 - accuracy: 0.7310 - Precision: 0.6190 - Recall: 0.4394\n",
      "Epoch 14/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4886 - accuracy: 0.7323 - Precision: 0.6160 - Recall: 0.4592\n",
      "Epoch 15/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.5070 - accuracy: 0.7169 - Precision: 0.5887 - Recall: 0.4155\n",
      "Epoch 16/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4920 - accuracy: 0.7227 - Precision: 0.5874 - Recall: 0.4811\n",
      "Epoch 17/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4977 - accuracy: 0.7323 - Precision: 0.6390 - Recall: 0.3976\n",
      "Epoch 18/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4942 - accuracy: 0.7278 - Precision: 0.6163 - Recall: 0.4215\n",
      "Epoch 19/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4849 - accuracy: 0.7323 - Precision: 0.6330 - Recall: 0.4115\n",
      "Epoch 20/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4978 - accuracy: 0.7265 - Precision: 0.6134 - Recall: 0.4195\n",
      "Epoch 21/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4838 - accuracy: 0.7420 - Precision: 0.6518 - Recall: 0.4354\n",
      "Epoch 22/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4825 - accuracy: 0.7323 - Precision: 0.6225 - Recall: 0.4394\n",
      "Epoch 23/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4846 - accuracy: 0.7297 - Precision: 0.6143 - Recall: 0.4433\n",
      "Epoch 24/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4805 - accuracy: 0.7490 - Precision: 0.6601 - Recall: 0.4632\n",
      "Epoch 25/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4923 - accuracy: 0.7181 - Precision: 0.5858 - Recall: 0.4414\n",
      "Epoch 26/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4757 - accuracy: 0.7458 - Precision: 0.6371 - Recall: 0.4990\n",
      "Epoch 27/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4935 - accuracy: 0.7497 - Precision: 0.6792 - Recall: 0.4294\n",
      "Epoch 28/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4885 - accuracy: 0.7246 - Precision: 0.6045 - Recall: 0.4314\n",
      "Epoch 29/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4852 - accuracy: 0.7297 - Precision: 0.6210 - Recall: 0.4235\n",
      "Epoch 30/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4913 - accuracy: 0.7362 - Precision: 0.6247 - Recall: 0.4632\n",
      "Epoch 31/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4716 - accuracy: 0.7439 - Precision: 0.6471 - Recall: 0.4592\n",
      "Epoch 32/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4918 - accuracy: 0.7329 - Precision: 0.6122 - Recall: 0.4771\n",
      "Epoch 33/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4866 - accuracy: 0.7432 - Precision: 0.6615 - Recall: 0.4235\n",
      "Epoch 34/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4850 - accuracy: 0.7477 - Precision: 0.6504 - Recall: 0.4771\n",
      "Epoch 35/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4897 - accuracy: 0.7355 - Precision: 0.6337 - Recall: 0.4334\n",
      "Epoch 36/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4766 - accuracy: 0.7278 - Precision: 0.6093 - Recall: 0.4433 0s - loss: 0.4711 - accuracy: 0.7293 - Precision: 0.6138 - Recall: 0.44\n",
      "Epoch 37/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4744 - accuracy: 0.7407 - Precision: 0.6506 - Recall: 0.4294\n",
      "Epoch 38/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4866 - accuracy: 0.7375 - Precision: 0.6260 - Recall: 0.4692\n",
      "Epoch 39/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4721 - accuracy: 0.7458 - Precision: 0.6598 - Recall: 0.4433\n",
      "Epoch 40/100\n",
      "1554/1554 [==============================] - 2s 2ms/sample - loss: 0.4915 - accuracy: 0.7375 - Precision: 0.6196 - Recall: 0.4891\n",
      "Epoch 41/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4847 - accuracy: 0.7465 - Precision: 0.6346 - Recall: 0.5109\n",
      "Epoch 42/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4722 - accuracy: 0.7439 - Precision: 0.6462 - Recall: 0.4612\n",
      "Epoch 43/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4801 - accuracy: 0.7477 - Precision: 0.6609 - Recall: 0.4533\n",
      "Epoch 44/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4853 - accuracy: 0.7497 - Precision: 0.6575 - Recall: 0.4732\n",
      "Epoch 45/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4782 - accuracy: 0.7432 - Precision: 0.6405 - Recall: 0.4712\n",
      "Epoch 46/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4649 - accuracy: 0.7587 - Precision: 0.6624 - Recall: 0.5189\n",
      "Epoch 47/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4821 - accuracy: 0.7394 - Precision: 0.6354 - Recall: 0.4573\n",
      "Epoch 48/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4815 - accuracy: 0.7317 - Precision: 0.6059 - Recall: 0.4891\n",
      "Epoch 49/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4760 - accuracy: 0.7394 - Precision: 0.6296 - Recall: 0.4732\n",
      "Epoch 50/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4774 - accuracy: 0.7561 - Precision: 0.6722 - Recall: 0.4811\n",
      "Epoch 51/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4780 - accuracy: 0.7452 - Precision: 0.6361 - Recall: 0.4970\n",
      "Epoch 52/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4774 - accuracy: 0.7375 - Precision: 0.6489 - Recall: 0.4115\n",
      "Epoch 53/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4720 - accuracy: 0.7375 - Precision: 0.6280 - Recall: 0.4632\n",
      "Epoch 54/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4660 - accuracy: 0.7529 - Precision: 0.6587 - Recall: 0.4911\n",
      "Epoch 55/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4872 - accuracy: 0.7329 - Precision: 0.6152 - Recall: 0.4672\n",
      "Epoch 56/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4751 - accuracy: 0.7368 - Precision: 0.6416 - Recall: 0.4235\n",
      "Epoch 57/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4760 - accuracy: 0.7439 - Precision: 0.6296 - Recall: 0.5070\n",
      "Epoch 58/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4895 - accuracy: 0.7291 - Precision: 0.6171 - Recall: 0.4294\n",
      "Epoch 59/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4730 - accuracy: 0.7355 - Precision: 0.6292 - Recall: 0.4453\n",
      "Epoch 60/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4812 - accuracy: 0.7413 - Precision: 0.6399 - Recall: 0.4592\n",
      "Epoch 61/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4853 - accuracy: 0.7310 - Precision: 0.6171 - Recall: 0.4453\n",
      "Epoch 62/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4737 - accuracy: 0.7413 - Precision: 0.6354 - Recall: 0.4712\n",
      "Epoch 63/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4673 - accuracy: 0.7542 - Precision: 0.6613 - Recall: 0.4930\n",
      "Epoch 64/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4802 - accuracy: 0.7342 - Precision: 0.6271 - Recall: 0.4414\n",
      "Epoch 65/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4815 - accuracy: 0.7516 - Precision: 0.6716 - Recall: 0.4553\n",
      "Epoch 66/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4748 - accuracy: 0.7497 - Precision: 0.6462 - Recall: 0.5010\n",
      "Epoch 67/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4818 - accuracy: 0.7362 - Precision: 0.6260 - Recall: 0.4592\n",
      "Epoch 68/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4782 - accuracy: 0.7355 - Precision: 0.6307 - Recall: 0.4414\n",
      "Epoch 69/100\n",
      "1554/1554 [==============================] - 2s 2ms/sample - loss: 0.4704 - accuracy: 0.7387 - Precision: 0.6351 - Recall: 0.4533\n",
      "Epoch 70/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4865 - accuracy: 0.7510 - Precision: 0.6480 - Recall: 0.5050\n",
      "Epoch 71/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4749 - accuracy: 0.7625 - Precision: 0.6709 - Recall: 0.5229\n",
      "Epoch 72/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4979 - accuracy: 0.7317 - Precision: 0.6168 - Recall: 0.4513\n",
      "Epoch 73/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4839 - accuracy: 0.7349 - Precision: 0.6164 - Recall: 0.4791\n",
      "Epoch 74/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4705 - accuracy: 0.7329 - Precision: 0.6183 - Recall: 0.4573\n",
      "Epoch 75/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4739 - accuracy: 0.7336 - Precision: 0.6290 - Recall: 0.4314\n",
      "Epoch 76/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4699 - accuracy: 0.7407 - Precision: 0.6289 - Recall: 0.4851\n",
      "Epoch 77/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4740 - accuracy: 0.7381 - Precision: 0.6341 - Recall: 0.4513\n",
      "Epoch 78/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4770 - accuracy: 0.7381 - Precision: 0.6270 - Recall: 0.4712\n",
      "Epoch 79/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4772 - accuracy: 0.7355 - Precision: 0.6257 - Recall: 0.4553\n",
      "Epoch 80/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4867 - accuracy: 0.7381 - Precision: 0.6379 - Recall: 0.4414\n",
      "Epoch 81/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4793 - accuracy: 0.7484 - Precision: 0.6474 - Recall: 0.4891\n",
      "Epoch 82/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4596 - accuracy: 0.7477 - Precision: 0.6599 - Recall: 0.4553\n",
      "Epoch 83/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4790 - accuracy: 0.7465 - Precision: 0.6453 - Recall: 0.4811\n",
      "Epoch 84/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4704 - accuracy: 0.7426 - Precision: 0.6419 - Recall: 0.4632\n",
      "Epoch 85/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4751 - accuracy: 0.7555 - Precision: 0.6573 - Recall: 0.5109\n",
      "Epoch 86/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4692 - accuracy: 0.7426 - Precision: 0.6484 - Recall: 0.4473\n",
      "Epoch 87/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4727 - accuracy: 0.7375 - Precision: 0.6221 - Recall: 0.4811\n",
      "Epoch 88/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4724 - accuracy: 0.7310 - Precision: 0.6081 - Recall: 0.4751\n",
      "Epoch 89/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4752 - accuracy: 0.7458 - Precision: 0.6414 - Recall: 0.4871\n",
      "Epoch 90/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4845 - accuracy: 0.7452 - Precision: 0.6458 - Recall: 0.4712\n",
      "Epoch 91/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4682 - accuracy: 0.7471 - Precision: 0.6447 - Recall: 0.4871\n",
      "Epoch 92/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4717 - accuracy: 0.7407 - Precision: 0.6289 - Recall: 0.4851\n",
      "Epoch 93/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4749 - accuracy: 0.7445 - Precision: 0.6472 - Recall: 0.4632\n",
      "Epoch 94/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4720 - accuracy: 0.7407 - Precision: 0.6330 - Recall: 0.4732\n",
      "Epoch 95/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4764 - accuracy: 0.7426 - Precision: 0.6359 - Recall: 0.4791\n",
      "Epoch 96/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4823 - accuracy: 0.7484 - Precision: 0.6489 - Recall: 0.4851\n",
      "Epoch 97/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4655 - accuracy: 0.7484 - Precision: 0.6458 - Recall: 0.4930\n",
      "Epoch 98/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4790 - accuracy: 0.7375 - Precision: 0.6361 - Recall: 0.4414 3s - loss: 0.4631 - accuracy: 0.7000 - Precisio\n",
      "Epoch 99/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4715 - accuracy: 0.7400 - Precision: 0.6349 - Recall: 0.4632\n",
      "Epoch 100/100\n",
      "1554/1554 [==============================] - 2s 1ms/sample - loss: 0.4609 - accuracy: 0.7497 - Precision: 0.6549 - Recall: 0.4791\n",
      "accuracy: 67.44%\n",
      "Precision: 50.00%\n",
      "Recall: 46.43%\n",
      "Fitting model...\n",
      "Train on 1555 samples\n",
      "Epoch 1/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4948 - accuracy: 0.7145 - Precision: 0.5785 - Recall: 0.4385\n",
      "Epoch 2/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4791 - accuracy: 0.7389 - Precision: 0.6310 - Recall: 0.4683\n",
      "Epoch 3/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4836 - accuracy: 0.7370 - Precision: 0.6323 - Recall: 0.4504\n",
      "Epoch 4/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4892 - accuracy: 0.7453 - Precision: 0.6406 - Recall: 0.4881\n",
      "Epoch 5/100\n",
      "1555/1555 [==============================] - 4s 2ms/sample - loss: 0.4712 - accuracy: 0.7338 - Precision: 0.6190 - Recall: 0.4643\n",
      "Epoch 6/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4741 - accuracy: 0.7370 - Precision: 0.6353 - Recall: 0.4425\n",
      "Epoch 7/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4747 - accuracy: 0.7492 - Precision: 0.6447 - Recall: 0.5040\n",
      "Epoch 8/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4759 - accuracy: 0.7453 - Precision: 0.6452 - Recall: 0.4762\n",
      "Epoch 9/100\n",
      "1555/1555 [==============================] - 2s 2ms/sample - loss: 0.4653 - accuracy: 0.7389 - Precision: 0.6276 - Recall: 0.4782\n",
      "Epoch 10/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4651 - accuracy: 0.7498 - Precision: 0.6686 - Recall: 0.4524\n",
      "Epoch 11/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4733 - accuracy: 0.7344 - Precision: 0.6304 - Recall: 0.4365\n",
      "Epoch 12/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4802 - accuracy: 0.7447 - Precision: 0.6368 - Recall: 0.4940\n",
      "Epoch 13/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4769 - accuracy: 0.7460 - Precision: 0.6598 - Recall: 0.4464\n",
      "Epoch 14/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4788 - accuracy: 0.7408 - Precision: 0.6319 - Recall: 0.4802\n",
      "Epoch 15/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4676 - accuracy: 0.7338 - Precision: 0.6203 - Recall: 0.4603\n",
      "Epoch 16/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4735 - accuracy: 0.7280 - Precision: 0.6052 - Recall: 0.4623\n",
      "Epoch 17/100\n",
      "1555/1555 [==============================] - ETA: 0s - loss: 0.4784 - accuracy: 0.7367 - Precision: 0.6197 - Recall: 0.458 - 2s 1ms/sample - loss: 0.4779 - accuracy: 0.7350 - Precision: 0.6250 - Recall: 0.4563\n",
      "Epoch 18/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4714 - accuracy: 0.7460 - Precision: 0.6394 - Recall: 0.4960\n",
      "Epoch 19/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4632 - accuracy: 0.7447 - Precision: 0.6334 - Recall: 0.5040\n",
      "Epoch 20/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4660 - accuracy: 0.7428 - Precision: 0.6461 - Recall: 0.4563\n",
      "Epoch 21/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4611 - accuracy: 0.7537 - Precision: 0.6667 - Recall: 0.4802\n",
      "Epoch 22/100\n",
      "1555/1555 [==============================] - 3s 2ms/sample - loss: 0.4756 - accuracy: 0.7299 - Precision: 0.6071 - Recall: 0.4722\n",
      "Epoch 23/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4804 - accuracy: 0.7389 - Precision: 0.6189 - Recall: 0.5060\n",
      "Epoch 24/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4769 - accuracy: 0.7344 - Precision: 0.6123 - Recall: 0.4921\n",
      "Epoch 25/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4634 - accuracy: 0.7421 - Precision: 0.6459 - Recall: 0.4524\n",
      "Epoch 26/100\n",
      "1555/1555 [==============================] - 2s 2ms/sample - loss: 0.4706 - accuracy: 0.7505 - Precision: 0.6676 - Recall: 0.4583\n",
      "Epoch 27/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4766 - accuracy: 0.7486 - Precision: 0.6499 - Recall: 0.4861\n",
      "Epoch 28/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4647 - accuracy: 0.7421 - Precision: 0.6510 - Recall: 0.4405\n",
      "Epoch 29/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4642 - accuracy: 0.7492 - Precision: 0.6566 - Recall: 0.4742\n",
      "Epoch 30/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4603 - accuracy: 0.7415 - Precision: 0.6335 - Recall: 0.4802\n",
      "Epoch 31/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4793 - accuracy: 0.7331 - Precision: 0.6132 - Recall: 0.4782\n",
      "Epoch 32/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4738 - accuracy: 0.7492 - Precision: 0.6484 - Recall: 0.4940\n",
      "Epoch 33/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4682 - accuracy: 0.7518 - Precision: 0.6528 - Recall: 0.5000\n",
      "Epoch 34/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4724 - accuracy: 0.7402 - Precision: 0.6330 - Recall: 0.4722\n",
      "Epoch 35/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4702 - accuracy: 0.7453 - Precision: 0.6337 - Recall: 0.5079\n",
      "Epoch 36/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4639 - accuracy: 0.7453 - Precision: 0.6452 - Recall: 0.4762\n",
      "Epoch 37/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4599 - accuracy: 0.7479 - Precision: 0.6481 - Recall: 0.4861\n",
      "Epoch 38/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4653 - accuracy: 0.7550 - Precision: 0.6534 - Recall: 0.5198\n",
      "Epoch 39/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4648 - accuracy: 0.7447 - Precision: 0.6466 - Recall: 0.4683\n",
      "Epoch 40/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4734 - accuracy: 0.7434 - Precision: 0.6265 - Recall: 0.5159\n",
      "Epoch 41/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4662 - accuracy: 0.7434 - Precision: 0.6303 - Recall: 0.5040 1s - loss: 0.4669 - accuracy: 0.7667 - Precision:\n",
      "Epoch 42/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4711 - accuracy: 0.7312 - Precision: 0.6156 - Recall: 0.4544\n",
      "Epoch 43/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4646 - accuracy: 0.7441 - Precision: 0.6456 - Recall: 0.4663 2s - loss: 0.4235 - accuracy: 0.7222 - Precisio\n",
      "Epoch 44/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4588 - accuracy: 0.7498 - Precision: 0.6366 - Recall: 0.5317\n",
      "Epoch 45/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4648 - accuracy: 0.7441 - Precision: 0.6489 - Recall: 0.4583\n",
      "Epoch 46/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4656 - accuracy: 0.7531 - Precision: 0.6587 - Recall: 0.4940\n",
      "Epoch 47/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4767 - accuracy: 0.7511 - Precision: 0.6519 - Recall: 0.4980\n",
      "Epoch 48/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4600 - accuracy: 0.7524 - Precision: 0.6448 - Recall: 0.5258\n",
      "Epoch 49/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4684 - accuracy: 0.7460 - Precision: 0.6408 - Recall: 0.4921\n",
      "Epoch 50/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4496 - accuracy: 0.7524 - Precision: 0.6537 - Recall: 0.5020\n",
      "Epoch 51/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4647 - accuracy: 0.7395 - Precision: 0.6286 - Recall: 0.4802\n",
      "Epoch 52/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4519 - accuracy: 0.7556 - Precision: 0.6632 - Recall: 0.5000\n",
      "Epoch 53/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4648 - accuracy: 0.7576 - Precision: 0.6516 - Recall: 0.5417\n",
      "Epoch 54/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4594 - accuracy: 0.7466 - Precision: 0.6389 - Recall: 0.5020\n",
      "Epoch 55/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4659 - accuracy: 0.7434 - Precision: 0.6378 - Recall: 0.4821\n",
      "Epoch 56/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4775 - accuracy: 0.7299 - Precision: 0.5977 - Recall: 0.5099\n",
      "Epoch 57/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4649 - accuracy: 0.7460 - Precision: 0.6485 - Recall: 0.4722\n",
      "Epoch 58/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4637 - accuracy: 0.7543 - Precision: 0.6556 - Recall: 0.5099\n",
      "Epoch 59/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4620 - accuracy: 0.7395 - Precision: 0.6222 - Recall: 0.5000\n",
      "Epoch 60/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4436 - accuracy: 0.7672 - Precision: 0.6830 - Recall: 0.5258\n",
      "Epoch 61/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4816 - accuracy: 0.7338 - Precision: 0.6077 - Recall: 0.5040\n",
      "Epoch 62/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4644 - accuracy: 0.7627 - Precision: 0.6650 - Recall: 0.5397\n",
      "Epoch 63/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4504 - accuracy: 0.7659 - Precision: 0.6832 - Recall: 0.5179\n",
      "Epoch 64/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4561 - accuracy: 0.7505 - Precision: 0.6503 - Recall: 0.4980\n",
      "Epoch 65/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4537 - accuracy: 0.7518 - Precision: 0.6569 - Recall: 0.4901 1s - loss: 0.4263 - accuracy: 0.7597 - Precision: 0.6742 -\n",
      "Epoch 66/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4532 - accuracy: 0.7498 - Precision: 0.6463 - Recall: 0.5040\n",
      "Epoch 67/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4662 - accuracy: 0.7363 - Precision: 0.6284 - Recall: 0.4563\n",
      "Epoch 68/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4518 - accuracy: 0.7486 - Precision: 0.6548 - Recall: 0.4742\n",
      "Epoch 69/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4576 - accuracy: 0.7569 - Precision: 0.6615 - Recall: 0.5119\n",
      "Epoch 70/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4578 - accuracy: 0.7518 - Precision: 0.6545 - Recall: 0.4960\n",
      "Epoch 71/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4651 - accuracy: 0.7453 - Precision: 0.6399 - Recall: 0.4901\n",
      "Epoch 72/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4676 - accuracy: 0.7550 - Precision: 0.6649 - Recall: 0.4921\n",
      "Epoch 73/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4531 - accuracy: 0.7614 - Precision: 0.6727 - Recall: 0.5139\n",
      "Epoch 74/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4612 - accuracy: 0.7537 - Precision: 0.6458 - Recall: 0.5317\n",
      "Epoch 75/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4634 - accuracy: 0.7408 - Precision: 0.6211 - Recall: 0.5139\n",
      "Epoch 76/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4534 - accuracy: 0.7505 - Precision: 0.6551 - Recall: 0.4861\n",
      "Epoch 77/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4647 - accuracy: 0.7434 - Precision: 0.6271 - Recall: 0.5139\n",
      "Epoch 78/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4512 - accuracy: 0.7505 - Precision: 0.6576 - Recall: 0.4802\n",
      "Epoch 79/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4602 - accuracy: 0.7357 - Precision: 0.6214 - Recall: 0.4722\n",
      "Epoch 80/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4605 - accuracy: 0.7563 - Precision: 0.6771 - Recall: 0.4742\n",
      "Epoch 81/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4509 - accuracy: 0.7666 - Precision: 0.6732 - Recall: 0.5437\n",
      "Epoch 82/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4480 - accuracy: 0.7531 - Precision: 0.6579 - Recall: 0.4960\n",
      "Epoch 83/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4508 - accuracy: 0.7563 - Precision: 0.6574 - Recall: 0.5179\n",
      "Epoch 84/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4513 - accuracy: 0.7531 - Precision: 0.6515 - Recall: 0.5119\n",
      "Epoch 85/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4619 - accuracy: 0.7595 - Precision: 0.6633 - Recall: 0.5238\n",
      "Epoch 86/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4551 - accuracy: 0.7511 - Precision: 0.6481 - Recall: 0.5079\n",
      "Epoch 87/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4587 - accuracy: 0.7556 - Precision: 0.6476 - Recall: 0.5397\n",
      "Epoch 88/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4617 - accuracy: 0.7498 - Precision: 0.6629 - Recall: 0.4643\n",
      "Epoch 89/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4653 - accuracy: 0.7543 - Precision: 0.6667 - Recall: 0.4841\n",
      "Epoch 90/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4633 - accuracy: 0.7556 - Precision: 0.6442 - Recall: 0.5496\n",
      "Epoch 91/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4597 - accuracy: 0.7460 - Precision: 0.6394 - Recall: 0.4960\n",
      "Epoch 92/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4589 - accuracy: 0.7543 - Precision: 0.6614 - Recall: 0.4960\n",
      "Epoch 93/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4506 - accuracy: 0.7576 - Precision: 0.6608 - Recall: 0.5179 0s - loss: 0.4261 - accuracy: 0.7763 - Precision: 0.6772 - Recall:\n",
      "Epoch 94/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4658 - accuracy: 0.7473 - Precision: 0.6398 - Recall: 0.5040\n",
      "Epoch 95/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4521 - accuracy: 0.7563 - Precision: 0.6685 - Recall: 0.4921\n",
      "Epoch 96/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4630 - accuracy: 0.7434 - Precision: 0.6378 - Recall: 0.4821\n",
      "Epoch 97/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4673 - accuracy: 0.7421 - Precision: 0.6200 - Recall: 0.5278\n",
      "Epoch 98/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4645 - accuracy: 0.7421 - Precision: 0.6459 - Recall: 0.4524\n",
      "Epoch 99/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4729 - accuracy: 0.7428 - Precision: 0.6262 - Recall: 0.5119 0s - loss: 0.4579 - accuracy: 0.7364 - Precision: 0.6222 - Rec\n",
      "Epoch 100/100\n",
      "1555/1555 [==============================] - 2s 1ms/sample - loss: 0.4453 - accuracy: 0.7498 - Precision: 0.6427 - Recall: 0.5139\n",
      "accuracy: 71.35%\n",
      "Precision: 57.89%\n",
      "Recall: 40.00%\n",
      "accuracy:  69.07% (+/- 3.53%)\n",
      "precision:  48.92% (+/- 18.09%)\n",
      "recall:  29.54% (+/- 20.26%)\n"
     ]
    }
   ],
   "source": [
    "#manual 10-fold \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "X=data0.loc[:,\"PSA360\":\"PSA0\"].values\n",
    "X=X.astype(np.float32)\n",
    "y=data0.event.values\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "'''\n",
    "from imblearn.over_sampling import SMOTE\n",
    "os = SMOTE(random_state=0)\n",
    "\n",
    "os_data_X,os_data_y=os.fit_sample(X_train, y_train)\n",
    "    '''\n",
    "'''\n",
    "from sklearn.preprocessing import StandardScaler  # doctest: +SKIP\n",
    "scaler = StandardScaler()  # doctest: +SKIP\n",
    "scaler.fit(X_train)  # doctest: +SKIP\n",
    "X_train = scaler.transform(X_train)  # doctest: +SKIP\n",
    "X_test = scaler.transform(X_test)  # doctest: +SKIP'''\n",
    "\n",
    "X_train_t = X_train.reshape(X_train.shape[0], 5, 1) \n",
    "X_test_t = X_test.reshape(X_test.shape[0], 5, 1)\n",
    "\n",
    "X_t=X.reshape(X.shape[0], 5, 1) \n",
    "\n",
    "from tensorflow.keras.layers import LSTM \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense \n",
    "import tensorflow.keras.backend as K \n",
    "from tensorflow.keras.callbacks import EarlyStopping \n",
    "\n",
    "'''K.clear_session() \n",
    "model = Sequential() # Sequeatial Model \n",
    "model.add(LSTM(20, input_shape=(9, 1),activation='sigmoid')) # (timestep, feature) \n",
    "model.add(Dense(1,activation='sigmoid')) # output = 1 \n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['acc']) \n",
    "model.summary()\n",
    "'''\n",
    "\n",
    "# define 10-fold cross validation test harness\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=np.random.seed(35))\n",
    "acc = []\n",
    "prec=[]\n",
    "rec=[]\n",
    "\n",
    "# create model\n",
    "def create_model():\n",
    "    print ('Creating model...')\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50,input_shape=(5,1)))\n",
    "    model.add(LSTM(256, activation='sigmoid', return_sequences=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(256, activation='sigmoid'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    print ('Compiling...')\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy','Precision','Recall'])\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "for train, test in kfold.split(X_t, y):\n",
    "    # Fit the model\n",
    "    print ('Fitting model...')\n",
    "    model.fit(X_t[train], y[train], epochs=100, batch_size=30, verbose=1)\n",
    "    # evaluate the model\n",
    "    scores = model.evaluate(X_t[test], y[test], verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[2], scores[2]*100))\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[3], scores[3]*100))\n",
    "    acc.append(scores[1] * 100)\n",
    "    prec.append(scores[2] * 100)\n",
    "    rec.append(scores[3]*100)\n",
    "print('accuracy: ',\"%.2f%% (+/- %.2f%%)\" % (np.mean(acc), np.std(acc)))\n",
    "print('precision: ',\"%.2f%% (+/- %.2f%%)\" % (np.mean(prec), np.std(prec)))\n",
    "print('recall: ',\"%.2f%% (+/- %.2f%%)\" % (np.mean(rec), np.std(rec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2. Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0]\n",
      "0.630057803468208\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign       0.68      0.83      0.75       115\n",
      "         PCa       0.41      0.24      0.30        58\n",
      "\n",
      "    accuracy                           0.63       173\n",
      "   macro avg       0.55      0.53      0.53       173\n",
      "weighted avg       0.59      0.63      0.60       173\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "for i in data0.index:\n",
    "    #data0.loc[i,\"PSA_mean\"]=(data0.loc[i,\"PSA360\"]+data0.loc[i,\"PSA270\"]+data0.loc[i,\"PSA180\"]+data0.loc[i,\"PSA90\"]+data0.loc[i,\"PSA0\"])/5\n",
    "    data0.loc[i,\"PSA_delta1\"]=data0.loc[i,\"PSA270\"]-data0.loc[i,\"PSA360\"]\n",
    "    data0.loc[i,\"PSA_delta2\"]=data0.loc[i,\"PSA180\"]-data0.loc[i,\"PSA270\"]\n",
    "    data0.loc[i,\"PSA_delta3\"]=data0.loc[i,\"PSA90\"]-data0.loc[i,\"PSA180\"]\n",
    "    data0.loc[i,\"PSA_delta4\"]=data0.loc[i,\"PSA0\"]-data0.loc[i,\"PSA90\"]\n",
    "\n",
    "y=data0.event.values\n",
    "data0__=data0.drop(columns=\"event\")\n",
    "\n",
    "X=data0__.loc[:,\"index\":\"PSA0\"].values\n",
    "\n",
    "'''from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)'''\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=64)\n",
    "\n",
    "X_train2=X_train[:,1:]\n",
    "X_test2=X_test[:,1:]\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Initialize our classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Train our classifier\n",
    "model = gnb.fit(X_train2, y_train)\n",
    "\n",
    "y_pred = gnb.predict(X_test2)\n",
    "print(y_pred)\n",
    "y_pred=y_pred.reshape(173,1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred, target_names=['benign', 'PCa']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.03292751, 0.00897646, 0.0917542 , 0.03789854, 0.00498676,\n",
       "        0.00698137, 0.00897479, 0.00498629, 0.01296473, 0.01196814]),\n",
       " 'score_time': array([0.00598407, 0.00498605, 0.00398922, 0.00498581, 0.00299239,\n",
       "        0.00399017, 0.00498724, 0.004987  , 0.00598454, 0.00698137]),\n",
       " 'test_accuracy': array([0.67630058, 0.67630058, 0.67630058, 0.67630058, 0.67630058,\n",
       "        0.67630058, 0.67630058, 0.6744186 , 0.6744186 , 0.67836257]),\n",
       " 'test_precision': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'test_recall': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'test_f1': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#참고: https://scikit-learn.org/stable/supervised_learning.html\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "bagging = BaggingClassifier(KNeighborsClassifier(),max_samples=0.5, max_features=0.5)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=10)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "clf = AdaBoostClassifier(n_estimators=100)\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "X=data0__.loc[:,\"Age\":\"PSA0\"].values\n",
    "score=cross_validate(clf, X, y, cv=10,scoring=('accuracy', 'precision','recall','f1'), verbose=0)\n",
    "\n",
    "print(\"test accuracy:\",score[\"test_accuracy\"].mean())\n",
    "print(\"test_precision:\",score[\"test_precision\"].mean())\n",
    "print(\"test_recall:\",score[\"test_recall\"].mean())\n",
    "print(\"test_f1:\",score[\"test_f1\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X[test])\n",
    "y_pred=y_pred.reshape(171,1)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y[test], y_pred,labels=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[97, 19],\n",
       "       [44, 11]], dtype=int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y[test], y_pred,labels=[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 패턴 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PSA360</th>\n",
       "      <th>PSA270</th>\n",
       "      <th>PSA180</th>\n",
       "      <th>PSA90</th>\n",
       "      <th>PSA0</th>\n",
       "      <th>event=1</th>\n",
       "      <th>pred</th>\n",
       "      <th>TF</th>\n",
       "      <th>CM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.304531</td>\n",
       "      <td>3.987717</td>\n",
       "      <td>4.682282</td>\n",
       "      <td>5.376848</td>\n",
       "      <td>4.84</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>True</td>\n",
       "      <td>TP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.597519</td>\n",
       "      <td>2.482481</td>\n",
       "      <td>2.408242</td>\n",
       "      <td>4.129121</td>\n",
       "      <td>5.85</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>True</td>\n",
       "      <td>TP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>18.737209</td>\n",
       "      <td>18.318605</td>\n",
       "      <td>17.900000</td>\n",
       "      <td>17.521276</td>\n",
       "      <td>18.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>True</td>\n",
       "      <td>TP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.761539</td>\n",
       "      <td>6.700000</td>\n",
       "      <td>6.981818</td>\n",
       "      <td>7.80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>False</td>\n",
       "      <td>FN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7.533778</td>\n",
       "      <td>6.613778</td>\n",
       "      <td>5.693778</td>\n",
       "      <td>4.773778</td>\n",
       "      <td>4.37</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>False</td>\n",
       "      <td>FN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>2.750840</td>\n",
       "      <td>3.200840</td>\n",
       "      <td>3.667227</td>\n",
       "      <td>4.133614</td>\n",
       "      <td>4.60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>False</td>\n",
       "      <td>FP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>2.984436</td>\n",
       "      <td>3.340827</td>\n",
       "      <td>3.697218</td>\n",
       "      <td>4.053609</td>\n",
       "      <td>4.41</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>False</td>\n",
       "      <td>FP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>3.784286</td>\n",
       "      <td>3.855714</td>\n",
       "      <td>3.943810</td>\n",
       "      <td>4.181905</td>\n",
       "      <td>4.42</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>False</td>\n",
       "      <td>FP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>4.713589</td>\n",
       "      <td>4.635191</td>\n",
       "      <td>4.556795</td>\n",
       "      <td>4.478397</td>\n",
       "      <td>4.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>False</td>\n",
       "      <td>FP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>7.565056</td>\n",
       "      <td>7.919631</td>\n",
       "      <td>8.274207</td>\n",
       "      <td>8.628783</td>\n",
       "      <td>13.92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>False</td>\n",
       "      <td>FP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>171 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        PSA360     PSA270     PSA180      PSA90   PSA0  event=1      pred  \\\n",
       "0     4.304531   3.987717   4.682282   5.376848   4.84      1.0  Positive   \n",
       "1     2.597519   2.482481   2.408242   4.129121   5.85      1.0  Positive   \n",
       "2    18.737209  18.318605  17.900000  17.521276  18.00      1.0  Positive   \n",
       "3     0.000000   1.761539   6.700000   6.981818   7.80      1.0  Negative   \n",
       "4     7.533778   6.613778   5.693778   4.773778   4.37      1.0  Negative   \n",
       "..         ...        ...        ...        ...    ...      ...       ...   \n",
       "166   2.750840   3.200840   3.667227   4.133614   4.60      0.0  Positive   \n",
       "167   2.984436   3.340827   3.697218   4.053609   4.41      0.0  Positive   \n",
       "168   3.784286   3.855714   3.943810   4.181905   4.42      0.0  Positive   \n",
       "169   4.713589   4.635191   4.556795   4.478397   4.40      0.0  Positive   \n",
       "170   7.565056   7.919631   8.274207   8.628783  13.92      0.0  Positive   \n",
       "\n",
       "        TF  CM  \n",
       "0     True  TP  \n",
       "1     True  TP  \n",
       "2     True  TP  \n",
       "3    False  FN  \n",
       "4    False  FN  \n",
       "..     ...  ..  \n",
       "166  False  FP  \n",
       "167  False  FP  \n",
       "168  False  FP  \n",
       "169  False  FP  \n",
       "170  False  FP  \n",
       "\n",
       "[171 rows x 9 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TP, TN, FP, FN 분류해서 index 붙이기\n",
    "testset=np.concatenate((X[test][:,2:],y[test].reshape(171,1)),axis=1)\n",
    "testset=pd.DataFrame(testset,columns=[\"PSA360\",\"PSA270\",\"PSA180\",\"PSA90\",\"PSA0\",\"event=1\"])\n",
    "\n",
    "testset.loc[y_pred[:,-1]==1,\"pred\"]=\"Positive\"\n",
    "testset.loc[y_pred[:,-1]==0,\"pred\"]=\"Negative\"\n",
    "testset.loc[testset.loc[:,\"event=1\"].values==y_pred[:,-1],\"TF\"]=\"True\"\n",
    "testset.loc[testset.loc[:,\"event=1\"].values!=y_pred[:,-1],\"TF\"]=\"False\"\n",
    "\n",
    "for n in testset.index:\n",
    "    if testset.loc[n,\"pred\"]==\"Positive\" and testset.loc[n,\"TF\"]==\"True\":\n",
    "        testset.loc[n,\"CM\"]=\"TP\"\n",
    "    elif testset.loc[n,\"pred\"]==\"Negative\" and testset.loc[n,\"TF\"]==\"True\":\n",
    "        testset.loc[n,\"CM\"]=\"TN\"\n",
    "    elif testset.loc[n,\"pred\"]==\"Positive\" and testset.loc[n,\"TF\"]==\"False\":\n",
    "        testset.loc[n,\"CM\"]=\"FP\"\n",
    "    elif testset.loc[n,\"pred\"]==\"Negative\" and testset.loc[n,\"TF\"]==\"False\":\n",
    "        testset.loc[n,\"CM\"]=\"FN\"\n",
    "\n",
    "testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PSA0m</th>\n",
       "      <th>PSA1m</th>\n",
       "      <th>PSA2m</th>\n",
       "      <th>PSA3m</th>\n",
       "      <th>PSA4m</th>\n",
       "      <th>PSA5m</th>\n",
       "      <th>PSA6m</th>\n",
       "      <th>PSA7m</th>\n",
       "      <th>PSA8m</th>\n",
       "      <th>PSA9m</th>\n",
       "      <th>PSA10m</th>\n",
       "      <th>PSA11m</th>\n",
       "      <th>PSA12m</th>\n",
       "      <th>event=1</th>\n",
       "      <th>pred</th>\n",
       "      <th>TF</th>\n",
       "      <th>CM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6.320</td>\n",
       "      <td>5.501951</td>\n",
       "      <td>5.604390</td>\n",
       "      <td>5.700664</td>\n",
       "      <td>5.710622</td>\n",
       "      <td>5.720581</td>\n",
       "      <td>5.730539</td>\n",
       "      <td>5.740498</td>\n",
       "      <td>5.750456</td>\n",
       "      <td>5.760415</td>\n",
       "      <td>5.770373</td>\n",
       "      <td>5.792286</td>\n",
       "      <td>6.160857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>True</td>\n",
       "      <td>TN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.080</td>\n",
       "      <td>3.773333</td>\n",
       "      <td>3.466667</td>\n",
       "      <td>3.160000</td>\n",
       "      <td>3.315738</td>\n",
       "      <td>3.471475</td>\n",
       "      <td>3.627213</td>\n",
       "      <td>3.782951</td>\n",
       "      <td>3.938689</td>\n",
       "      <td>4.094426</td>\n",
       "      <td>3.920881</td>\n",
       "      <td>3.710749</td>\n",
       "      <td>3.500617</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>False</td>\n",
       "      <td>FN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>13.100</td>\n",
       "      <td>12.521428</td>\n",
       "      <td>11.942857</td>\n",
       "      <td>11.364285</td>\n",
       "      <td>10.785714</td>\n",
       "      <td>10.374707</td>\n",
       "      <td>10.298829</td>\n",
       "      <td>10.222951</td>\n",
       "      <td>10.147073</td>\n",
       "      <td>10.071195</td>\n",
       "      <td>9.995317</td>\n",
       "      <td>9.919438</td>\n",
       "      <td>9.843559</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>True</td>\n",
       "      <td>TN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.842</td>\n",
       "      <td>0.899231</td>\n",
       "      <td>0.956462</td>\n",
       "      <td>1.013692</td>\n",
       "      <td>1.070923</td>\n",
       "      <td>1.098101</td>\n",
       "      <td>1.110253</td>\n",
       "      <td>1.122405</td>\n",
       "      <td>1.134557</td>\n",
       "      <td>1.146709</td>\n",
       "      <td>1.158861</td>\n",
       "      <td>1.171013</td>\n",
       "      <td>1.183165</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>True</td>\n",
       "      <td>TN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.860</td>\n",
       "      <td>4.118462</td>\n",
       "      <td>4.376923</td>\n",
       "      <td>4.141939</td>\n",
       "      <td>3.808265</td>\n",
       "      <td>3.474592</td>\n",
       "      <td>3.525402</td>\n",
       "      <td>3.870230</td>\n",
       "      <td>4.215057</td>\n",
       "      <td>3.973636</td>\n",
       "      <td>3.439091</td>\n",
       "      <td>3.264596</td>\n",
       "      <td>3.162112</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>True</td>\n",
       "      <td>TN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>9.620</td>\n",
       "      <td>16.553404</td>\n",
       "      <td>14.766171</td>\n",
       "      <td>12.978936</td>\n",
       "      <td>11.191702</td>\n",
       "      <td>9.404469</td>\n",
       "      <td>7.617234</td>\n",
       "      <td>5.830000</td>\n",
       "      <td>4.042766</td>\n",
       "      <td>2.255532</td>\n",
       "      <td>0.468298</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>True</td>\n",
       "      <td>TN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>16.900</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>True</td>\n",
       "      <td>TN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>4.730</td>\n",
       "      <td>4.628913</td>\n",
       "      <td>4.527826</td>\n",
       "      <td>4.426739</td>\n",
       "      <td>4.293067</td>\n",
       "      <td>4.157067</td>\n",
       "      <td>4.557934</td>\n",
       "      <td>4.337273</td>\n",
       "      <td>4.116611</td>\n",
       "      <td>3.895950</td>\n",
       "      <td>3.685256</td>\n",
       "      <td>3.614093</td>\n",
       "      <td>3.542930</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>False</td>\n",
       "      <td>FN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>15.060</td>\n",
       "      <td>14.540358</td>\n",
       "      <td>14.020715</td>\n",
       "      <td>13.501071</td>\n",
       "      <td>12.981428</td>\n",
       "      <td>12.461785</td>\n",
       "      <td>11.942142</td>\n",
       "      <td>11.422500</td>\n",
       "      <td>10.902857</td>\n",
       "      <td>10.383214</td>\n",
       "      <td>9.863571</td>\n",
       "      <td>9.343928</td>\n",
       "      <td>9.248571</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>False</td>\n",
       "      <td>FN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>11.800</td>\n",
       "      <td>11.597904</td>\n",
       "      <td>11.395808</td>\n",
       "      <td>11.193712</td>\n",
       "      <td>10.991617</td>\n",
       "      <td>10.789521</td>\n",
       "      <td>10.587425</td>\n",
       "      <td>10.385329</td>\n",
       "      <td>10.183233</td>\n",
       "      <td>9.981137</td>\n",
       "      <td>9.779042</td>\n",
       "      <td>9.576946</td>\n",
       "      <td>9.374850</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>False</td>\n",
       "      <td>FN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>173 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      PSA0m      PSA1m      PSA2m      PSA3m      PSA4m      PSA5m      PSA6m  \\\n",
       "0     6.320   5.501951   5.604390   5.700664   5.710622   5.720581   5.730539   \n",
       "1     4.080   3.773333   3.466667   3.160000   3.315738   3.471475   3.627213   \n",
       "2    13.100  12.521428  11.942857  11.364285  10.785714  10.374707  10.298829   \n",
       "3     0.842   0.899231   0.956462   1.013692   1.070923   1.098101   1.110253   \n",
       "4     3.860   4.118462   4.376923   4.141939   3.808265   3.474592   3.525402   \n",
       "..      ...        ...        ...        ...        ...        ...        ...   \n",
       "168   9.620  16.553404  14.766171  12.978936  11.191702   9.404469   7.617234   \n",
       "169  16.900  20.000000  20.000000  20.000000  20.000000  20.000000  20.000000   \n",
       "170   4.730   4.628913   4.527826   4.426739   4.293067   4.157067   4.557934   \n",
       "171  15.060  14.540358  14.020715  13.501071  12.981428  12.461785  11.942142   \n",
       "172  11.800  11.597904  11.395808  11.193712  10.991617  10.789521  10.587425   \n",
       "\n",
       "         PSA7m      PSA8m      PSA9m     PSA10m     PSA11m     PSA12m  \\\n",
       "0     5.740498   5.750456   5.760415   5.770373   5.792286   6.160857   \n",
       "1     3.782951   3.938689   4.094426   3.920881   3.710749   3.500617   \n",
       "2    10.222951  10.147073  10.071195   9.995317   9.919438   9.843559   \n",
       "3     1.122405   1.134557   1.146709   1.158861   1.171013   1.183165   \n",
       "4     3.870230   4.215057   3.973636   3.439091   3.264596   3.162112   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "168   5.830000   4.042766   2.255532   0.468298   0.000000   0.000000   \n",
       "169  20.000000  20.000000  20.000000  20.000000  20.000000  20.000000   \n",
       "170   4.337273   4.116611   3.895950   3.685256   3.614093   3.542930   \n",
       "171  11.422500  10.902857  10.383214   9.863571   9.343928   9.248571   \n",
       "172  10.385329  10.183233   9.981137   9.779042   9.576946   9.374850   \n",
       "\n",
       "     event=1      pred     TF  CM  \n",
       "0        0.0  Negative   True  TN  \n",
       "1        1.0  Negative  False  FN  \n",
       "2        0.0  Negative   True  TN  \n",
       "3        0.0  Negative   True  TN  \n",
       "4        0.0  Negative   True  TN  \n",
       "..       ...       ...    ...  ..  \n",
       "168      0.0  Negative   True  TN  \n",
       "169      0.0  Negative   True  TN  \n",
       "170      1.0  Negative  False  FN  \n",
       "171      1.0  Negative  False  FN  \n",
       "172      1.0  Negative  False  FN  \n",
       "\n",
       "[173 rows x 17 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TP, TN, FP, FN 분류해서 index 붙이기 (1개월 단위)\n",
    "testset=np.concatenate((X_test,y_test.reshape(173,1)),axis=1)\n",
    "testset=pd.DataFrame(testset,columns=[\"PSA0m\",\"PSA1m\",\"PSA2m\", \"PSA3m\",\"PSA4m\",\"PSA5m\",\"PSA6m\",\"PSA7m\",\"PSA8m\",\"PSA9m\",\"PSA10m\",\"PSA11m\",\"PSA12m\",\"event=1\"])\n",
    "\n",
    "testset.loc[y_pred[:,-1]==1,\"pred\"]=\"Positive\"\n",
    "testset.loc[y_pred[:,-1]==0,\"pred\"]=\"Negative\"\n",
    "testset.loc[testset.loc[:,\"event=1\"].values==y_pred[:,-1],\"TF\"]=\"True\"\n",
    "testset.loc[testset.loc[:,\"event=1\"].values!=y_pred[:,-1],\"TF\"]=\"False\"\n",
    "\n",
    "for n in testset.index:\n",
    "    if testset.loc[n,\"pred\"]==\"Positive\" and testset.loc[n,\"TF\"]==\"True\":\n",
    "        testset.loc[n,\"CM\"]=\"TP\"\n",
    "    elif testset.loc[n,\"pred\"]==\"Negative\" and testset.loc[n,\"TF\"]==\"True\":\n",
    "        testset.loc[n,\"CM\"]=\"TN\"\n",
    "    elif testset.loc[n,\"pred\"]==\"Positive\" and testset.loc[n,\"TF\"]==\"False\":\n",
    "        testset.loc[n,\"CM\"]=\"FP\"\n",
    "    elif testset.loc[n,\"pred\"]==\"Negative\" and testset.loc[n,\"TF\"]==\"False\":\n",
    "        testset.loc[n,\"CM\"]=\"FN\"\n",
    "\n",
    "testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PSA360     float64\n",
       "PSA270     float64\n",
       "PSA180     float64\n",
       "PSA90      float64\n",
       "PSA0       float64\n",
       "event=1    float64\n",
       "pred        object\n",
       "TF          object\n",
       "CM          object\n",
       "dtype: object"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset2=testset\n",
    "testset2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PSA1</th>\n",
       "      <th>PSA2</th>\n",
       "      <th>PSA3</th>\n",
       "      <th>PSA4</th>\n",
       "      <th>PSA5</th>\n",
       "      <th>PSA6</th>\n",
       "      <th>PSA7</th>\n",
       "      <th>PSA8</th>\n",
       "      <th>PSA9</th>\n",
       "      <th>PSA10</th>\n",
       "      <th>...</th>\n",
       "      <th>interval22</th>\n",
       "      <th>PSA360</th>\n",
       "      <th>PSA270</th>\n",
       "      <th>PSA180</th>\n",
       "      <th>PSA90</th>\n",
       "      <th>PSA0</th>\n",
       "      <th>event=1</th>\n",
       "      <th>pred</th>\n",
       "      <th>TF</th>\n",
       "      <th>CM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.30</td>\n",
       "      <td>4.10</td>\n",
       "      <td>4.60</td>\n",
       "      <td>5.86</td>\n",
       "      <td>9.20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>4.87881</td>\n",
       "      <td>5.36136</td>\n",
       "      <td>5.84391</td>\n",
       "      <td>7.50169</td>\n",
       "      <td>9.2</td>\n",
       "      <td>1</td>\n",
       "      <td>Negative</td>\n",
       "      <td>False</td>\n",
       "      <td>FN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.83</td>\n",
       "      <td>4.80</td>\n",
       "      <td>4.90</td>\n",
       "      <td>3.60</td>\n",
       "      <td>4.50</td>\n",
       "      <td>4.80</td>\n",
       "      <td>7.40</td>\n",
       "      <td>6.990</td>\n",
       "      <td>7.93</td>\n",
       "      <td>6.53</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>3.03288</td>\n",
       "      <td>2.93857</td>\n",
       "      <td>3.79571</td>\n",
       "      <td>4.65286</td>\n",
       "      <td>5.51</td>\n",
       "      <td>1</td>\n",
       "      <td>Negative</td>\n",
       "      <td>False</td>\n",
       "      <td>FN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>42.70</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.725</td>\n",
       "      <td>1.03</td>\n",
       "      <td>1.18</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>8.80786</td>\n",
       "      <td>9.73614</td>\n",
       "      <td>10.6644</td>\n",
       "      <td>11.5927</td>\n",
       "      <td>10.79</td>\n",
       "      <td>1</td>\n",
       "      <td>Positive</td>\n",
       "      <td>True</td>\n",
       "      <td>TP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.81</td>\n",
       "      <td>3.53</td>\n",
       "      <td>3.05</td>\n",
       "      <td>12.81</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>3.48392</td>\n",
       "      <td>3.13832</td>\n",
       "      <td>5.69745</td>\n",
       "      <td>9.25372</td>\n",
       "      <td>12.81</td>\n",
       "      <td>1</td>\n",
       "      <td>Positive</td>\n",
       "      <td>True</td>\n",
       "      <td>TP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>9.24</td>\n",
       "      <td>9.90</td>\n",
       "      <td>9.67</td>\n",
       "      <td>9.60</td>\n",
       "      <td>12.85</td>\n",
       "      <td>16.60</td>\n",
       "      <td>12.16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>9.7189</td>\n",
       "      <td>10.9079</td>\n",
       "      <td>12.097</td>\n",
       "      <td>14.1951</td>\n",
       "      <td>12.16</td>\n",
       "      <td>1</td>\n",
       "      <td>Positive</td>\n",
       "      <td>True</td>\n",
       "      <td>TP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>3.27</td>\n",
       "      <td>3.92</td>\n",
       "      <td>4.42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.78429</td>\n",
       "      <td>3.85571</td>\n",
       "      <td>3.94381</td>\n",
       "      <td>4.1819</td>\n",
       "      <td>4.42</td>\n",
       "      <td>0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>True</td>\n",
       "      <td>TN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>3.58</td>\n",
       "      <td>3.92</td>\n",
       "      <td>6.16</td>\n",
       "      <td>5.24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.91984</td>\n",
       "      <td>4.49618</td>\n",
       "      <td>5.07884</td>\n",
       "      <td>5.6615</td>\n",
       "      <td>5.24</td>\n",
       "      <td>0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>True</td>\n",
       "      <td>TN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>4.82</td>\n",
       "      <td>5.93</td>\n",
       "      <td>6.91</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.61353</td>\n",
       "      <td>6.68765</td>\n",
       "      <td>6.76176</td>\n",
       "      <td>6.83588</td>\n",
       "      <td>6.91</td>\n",
       "      <td>0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>True</td>\n",
       "      <td>TN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>7.87</td>\n",
       "      <td>9.87</td>\n",
       "      <td>7.48</td>\n",
       "      <td>5.71</td>\n",
       "      <td>6.60</td>\n",
       "      <td>8.47</td>\n",
       "      <td>10.06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.01627</td>\n",
       "      <td>7.59264</td>\n",
       "      <td>8.16901</td>\n",
       "      <td>8.98406</td>\n",
       "      <td>10.06</td>\n",
       "      <td>0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>False</td>\n",
       "      <td>FP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>4.79</td>\n",
       "      <td>7.79</td>\n",
       "      <td>8.61</td>\n",
       "      <td>7.65</td>\n",
       "      <td>9.53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.48755</td>\n",
       "      <td>8.04673</td>\n",
       "      <td>7.73952</td>\n",
       "      <td>8.63476</td>\n",
       "      <td>9.53</td>\n",
       "      <td>0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>False</td>\n",
       "      <td>FP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>173 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      PSA1  PSA2  PSA3   PSA4   PSA5   PSA6   PSA7   PSA8  PSA9  PSA10  ...  \\\n",
       "0     4.30  4.10  4.60   5.86   9.20    NaN    NaN    NaN   NaN    NaN  ...   \n",
       "1     3.83  4.80  4.90   3.60   4.50   4.80   7.40  6.990  7.93   6.53  ...   \n",
       "2    42.70  0.80  0.40   1.29   0.43   0.66   0.63  0.725  1.03   1.18  ...   \n",
       "3     2.81  3.53  3.05  12.81    NaN    NaN    NaN    NaN   NaN    NaN  ...   \n",
       "4     9.24  9.90  9.67   9.60  12.85  16.60  12.16    NaN   NaN    NaN  ...   \n",
       "..     ...   ...   ...    ...    ...    ...    ...    ...   ...    ...  ...   \n",
       "168   3.27  3.92  4.42    NaN    NaN    NaN    NaN    NaN   NaN    NaN  ...   \n",
       "169   3.58  3.92  6.16   5.24    NaN    NaN    NaN    NaN   NaN    NaN  ...   \n",
       "170   4.82  5.93  6.91    NaN    NaN    NaN    NaN    NaN   NaN    NaN  ...   \n",
       "171   7.87  9.87  7.48   5.71   6.60   8.47  10.06    NaN   NaN    NaN  ...   \n",
       "172   4.79  7.79  8.61   7.65   9.53    NaN    NaN    NaN   NaN    NaN  ...   \n",
       "\n",
       "     interval22   PSA360   PSA270   PSA180    PSA90   PSA0  event=1      pred  \\\n",
       "0           NaT  4.87881  5.36136  5.84391  7.50169    9.2        1  Negative   \n",
       "1           NaT  3.03288  2.93857  3.79571  4.65286   5.51        1  Negative   \n",
       "2           NaT  8.80786  9.73614  10.6644  11.5927  10.79        1  Positive   \n",
       "3           NaT  3.48392  3.13832  5.69745  9.25372  12.81        1  Positive   \n",
       "4           NaT   9.7189  10.9079   12.097  14.1951  12.16        1  Positive   \n",
       "..          ...      ...      ...      ...      ...    ...      ...       ...   \n",
       "168         NaN  3.78429  3.85571  3.94381   4.1819   4.42        0  Negative   \n",
       "169         NaN  3.91984  4.49618  5.07884   5.6615   5.24        0  Negative   \n",
       "170         NaN  6.61353  6.68765  6.76176  6.83588   6.91        0  Negative   \n",
       "171         NaN  7.01627  7.59264  8.16901  8.98406  10.06        0  Positive   \n",
       "172         NaN  8.48755  8.04673  7.73952  8.63476   9.53        0  Positive   \n",
       "\n",
       "        TF  CM  \n",
       "0    False  FN  \n",
       "1    False  FN  \n",
       "2     True  TP  \n",
       "3     True  TP  \n",
       "4     True  TP  \n",
       "..     ...  ..  \n",
       "168   True  TN  \n",
       "169   True  TN  \n",
       "170   True  TN  \n",
       "171  False  FP  \n",
       "172  False  FP  \n",
       "\n",
       "[173 rows x 54 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#index 사용 시 merge\n",
    "data_=pd.merge(data_drop.loc[:,\"PSA1\":\"index\"],data_drop.loc[:,\"interval1\":\"interval22\"],left_index=True,right_index=True)\n",
    "testset2=pd.merge(data_,testset,on=\"index\",how=\"right\")\n",
    "\n",
    "testset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in testset2.columns[:-3]:\n",
    "    testset2.loc[:,col]=pd.to_numeric(testset2.loc[:,col],errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testset_PCa=testset2[testset2[\"event=1\"]==1].groupby(testset2[\"CM\"]).mean()\n",
    "testset_benign=testset2[testset2[\"event=1\"]==0].groupby(testset2[\"CM\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'module://ipykernel.pylab.backend_inline'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('module://ipykernel.pylab.backend_inline')\n",
    "matplotlib.get_backend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:55: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x=np.array([360,270,180,90,0])\n",
    "\n",
    "fig=plt.figure(figsize=(10,7))\n",
    "\n",
    "ax1=fig.add_subplot(2,2,1)\n",
    "ax1.plot(x,testset_benign.loc[\"FP\",\"PSA360\":\"PSA0\"],'o-r',label=\"False Positive\")\n",
    "ax1.invert_xaxis()\n",
    "ax1.set_xticks([360,270,180,90,0])\n",
    "ax1.set_xlabel('Days before DOD')\n",
    "ax1.set_ylabel('PSA level')\n",
    "ax1.axvline(x=270,color='k',linewidth=1)\n",
    "ax1.axvline(x=180,color='k',linewidth=1)\n",
    "ax1.axvline(x=90,color='k',linewidth=1)\n",
    "ax1.set_title('1) False Positive case',fontsize=15)\n",
    "\n",
    "ax2=fig.add_subplot(2,2,2)\n",
    "ax2.plot(x,testset_benign.loc[\"TN\",\"PSA360\":\"PSA0\"],'o-r',label=\"True Negative\")\n",
    "ax2.invert_xaxis()\n",
    "ax2.set_xticks([360,270,180,90,0])\n",
    "ax2.set_xlabel('Days before DOD')\n",
    "ax2.set_ylabel('PSA level')\n",
    "ax2.axvline(x=270,color='k',linewidth=1)\n",
    "ax2.axvline(x=180,color='k',linewidth=1)\n",
    "ax2.axvline(x=90,color='k',linewidth=1)\n",
    "ax2.set_title('2) True Negative case',fontsize=15)\n",
    "\n",
    "ax3=fig.add_subplot(2,2,3)\n",
    "ax3.plot(x,testset_PCa.loc[\"FN\",\"PSA360\":\"PSA0\"],'o-r',label=\"False Negative\")\n",
    "ax3.invert_xaxis()\n",
    "ax3.set_xticks([360,270,180,90,0])\n",
    "ax3.set_xlabel('Days before DOD')\n",
    "ax3.set_ylabel('PSA level')\n",
    "ax3.axvline(x=270,color='k',linewidth=1)\n",
    "ax3.axvline(x=180,color='k',linewidth=1)\n",
    "ax3.axvline(x=90,color='k',linewidth=1)\n",
    "ax3.set_title('3) False Negative case',fontsize=15)\n",
    "\n",
    "\n",
    "ax4=fig.add_subplot(2,2,4)\n",
    "ax4.plot(x,testset_PCa.loc[\"TP\",\"PSA360\":\"PSA0\"],'o-r',label=\"True Positive\")\n",
    "ax4.invert_xaxis()\n",
    "ax4.set_xticks([360,270,180,90,0])\n",
    "ax4.set_xlabel('Days before DOD')\n",
    "ax4.set_ylabel('PSA level')\n",
    "ax4.axvline(x=270,color='k',linewidth=1)\n",
    "ax4.axvline(x=180,color='k',linewidth=1)\n",
    "ax4.axvline(x=90,color='k',linewidth=1)\n",
    "ax4.set_title('4) True Positive case',fontsize=15)\n",
    "\n",
    "fig.savefig(\"./output.png\",dpi=80)\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'FP'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2896\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2897\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2898\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'FP'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-87-dade42c2c726>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0max1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0max1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtestset_benign\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"FP\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"PSA0m\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"PSA12m\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'o-r'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"False Positive\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0max1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minvert_xaxis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0max1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_xticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1416\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m                     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1419\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m             \u001b[1;31m# we by definition only have the 0th axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m    803\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    804\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 805\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    806\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_lowerdim\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m    927\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_label_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 929\u001b[1;33m                 \u001b[0msection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m                 \u001b[1;31m# we have yielded a scalar ?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1848\u001b[0m         \u001b[1;31m# fall thru to straight lookup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1849\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1850\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1851\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_label\u001b[1;34m(self, label, axis)\u001b[0m\n\u001b[0;32m    158\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"no slices here, handle elsewhere\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_xs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mxs\u001b[1;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[0;32m   3735\u001b[0m             \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc_level\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop_level\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop_level\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3736\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3737\u001b[1;33m             \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3738\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3739\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2897\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2898\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2899\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2901\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'FP'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#1개월 단위\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x=np.array(range(0,13))\n",
    "\n",
    "fig=plt.figure(figsize=(12,9))\n",
    "\n",
    "ax1=fig.add_subplot(2,2,1)\n",
    "ax1.plot(x,testset_benign.loc[\"FP\",\"PSA0m\":\"PSA12m\"],'o-r',label=\"False Positive\")\n",
    "ax1.invert_xaxis()\n",
    "ax1.set_xticks(np.array(range(12,-1,-1)))\n",
    "ax1.legend()\n",
    "ax1.set_xlabel('Days before DOD')\n",
    "ax1.set_ylabel('PSA level')\n",
    "\n",
    "ax2=fig.add_subplot(2,2,2)\n",
    "ax2.plot(x,testset_benign.loc[\"TN\",\"PSA0m\":\"PSA12m\"],'o-r',label=\"True Negative\")\n",
    "ax2.invert_xaxis()\n",
    "ax2.set_xticks(np.array(range(12,-1,-1)))\n",
    "ax2.legend()\n",
    "ax2.set_xlabel('Days before DOD')\n",
    "ax2.set_ylabel('PSA level')\n",
    "\n",
    "ax3=fig.add_subplot(2,2,3)\n",
    "ax3.plot(x,testset_PCa.loc[\"FN\",\"PSA0m\":\"PSA12m\"],'o-r',label=\"False Negative\")\n",
    "ax3.invert_xaxis()\n",
    "ax3.set_xticks(np.array(range(12,-1,-1)))\n",
    "ax3.legend()\n",
    "ax3.set_xlabel('Days before DOD')\n",
    "ax3.set_ylabel('PSA level')\n",
    "\n",
    "ax4=fig.add_subplot(2,2,4)\n",
    "ax4.plot(x,testset_PCa.loc[\"TP\",\"PSA0m\":\"PSA12m\"],'o-r',label=\"True Positive\")\n",
    "ax4.invert_xaxis()\n",
    "ax4.set_xticks(np.array(range(12,-1,-1)))\n",
    "ax4.legend()\n",
    "ax4.set_xlabel('Days before DOD')\n",
    "ax4.set_ylabel('PSA level')\n",
    "\n",
    "fig.savefig(\"./output.png\",dpi=150)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 540x468 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Figure 1\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x=np.array([360,270,180,90,0])\n",
    "\n",
    "fig=plt.figure(figsize=(7.5,6.5))\n",
    "\n",
    "ax1=fig.add_subplot(2,2,2)\n",
    "ax1.plot(testset2.loc[68,\"interval1\":\"interval5\"],testset2.loc[68,\"PSA1\":\"PSA5\"],'o-b') #FP\n",
    "ax1.plot(x,testset2.loc[68,\"PSA360\":\"PSA0\"],'sr')\n",
    "ax1.set_xlim([0,360])\n",
    "ax1.invert_xaxis()\n",
    "ax1.set_xticks([360,270,180,90,0])\n",
    "ax1.axvline(x=270,color='k',linewidth=1)\n",
    "ax1.axvline(x=180,color='k',linewidth=1)\n",
    "ax1.axvline(x=90,color='k',linewidth=1)\n",
    "ax1.set_xlabel('Days before DOD')\n",
    "ax1.set_ylabel('PSA level')\n",
    "ax1.set_title('2) False Positive case',fontsize=15)\n",
    "\n",
    "ax2=fig.add_subplot(2,2,3)\n",
    "ax2.plot(testset2.loc[65,\"interval1\":\"interval9\"],testset2.loc[65,\"PSA1\":\"PSA9\"],'o-b') #TN\n",
    "#ax2.plot([testset2.loc[106,\"interval4\"],0],[testset2.loc[106,\"PSA4\"],testset2.loc[106,\"PSA0\"]],'o-b')\n",
    "ax2.plot([360,testset2.loc[65,\"interval1\"]],[testset2.loc[65,\"PSA360\"],testset2.loc[65,\"PSA9\"]],'o-b')\n",
    "ax2.plot(x,testset2.loc[65,\"PSA360\":\"PSA0\"],'sr')\n",
    "ax2.set_xlim([0,360])\n",
    "ax2.invert_xaxis()\n",
    "ax2.set_xticks([360,270,180,90,0])\n",
    "ax2.axvline(x=270,color='k',linewidth=1)\n",
    "ax2.axvline(x=180,color='k',linewidth=1)\n",
    "ax2.axvline(x=90,color='k',linewidth=1)\n",
    "ax2.set_xlabel('Days before DOD')\n",
    "ax2.set_ylabel('PSA level')\n",
    "ax2.set_title('3) True Negative case',fontsize=15)\n",
    "\n",
    "ax3=fig.add_subplot(2,2,1)\n",
    "ax3.plot(testset2.loc[91,\"interval1\":\"interval5\"],testset2.loc[91,\"PSA1\":\"PSA5\"],'o-b') #TN #91 점차 증가 (5) #91 조금 증가()\n",
    "ax3.plot(x,testset2.loc[91,\"PSA360\":\"PSA0\"],'sr')\n",
    "ax3.set_xlim([0,360])\n",
    "ax3.invert_xaxis()\n",
    "ax3.set_xticks([360,270,180,90,0])\n",
    "ax3.axvline(x=270,color='k',linewidth=1)\n",
    "ax3.axvline(x=180,color='k',linewidth=1)\n",
    "ax3.axvline(x=90,color='k',linewidth=1)\n",
    "ax3.set_xlabel('Days before DOD')\n",
    "ax3.set_ylabel('PSA level')\n",
    "ax3.set_title('1) True Negative case',fontsize=15)\n",
    "\n",
    "ax4=fig.add_subplot(2,2,4)\n",
    "ax4.plot(testset2.loc[89,\"interval1\":\"interval10\"],testset2.loc[89,\"PSA1\":\"PSA10\"],'o-b') \n",
    "ax4.plot(x,testset2.loc[89,\"PSA360\":\"PSA0\"],'sr')\n",
    "ax4.set_xlim([0,360])\n",
    "ax4.set_ylim([2,10])\n",
    "ax4.invert_xaxis()\n",
    "ax4.set_xticks([360,270,180,90,0])\n",
    "ax4.axvline(x=270,color='k',linewidth=1)\n",
    "ax4.axvline(x=180,color='k',linewidth=1)\n",
    "ax4.axvline(x=90,color='k',linewidth=1)\n",
    "ax4.set_xlabel('Days before DOD')\n",
    "ax4.set_ylabel('PSA level')\n",
    "ax4.set_title('4) True Negative case',fontsize=15)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"./output.png\",dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 540x468 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Figure 2\n",
    "\n",
    "#https://kongdols-room.tistory.com/86 참고\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x=np.array([360,270,180,90,0])\n",
    "\n",
    "fig=plt.figure(figsize=(7.5,6.5))\n",
    "\n",
    "ax1=fig.add_subplot(2,2,1)\n",
    "ax1.plot(testset2.loc[3,\"interval1\":\"interval4\"],testset2.loc[3,\"PSA1\":\"PSA4\"],'o-b') #TP\n",
    "ax1.plot(x,testset2.loc[3,\"PSA360\":\"PSA0\"],'or')\n",
    "ax1.set_xlim([0,360])\n",
    "ax1.invert_xaxis()\n",
    "ax1.set_xticks([360,270,180,90,0])\n",
    "ax1.axvline(x=270,color='k',linewidth=1)\n",
    "ax1.axvline(x=180,color='k',linewidth=1)\n",
    "ax1.axvline(x=90,color='k',linewidth=1)\n",
    "ax1.set_xlabel('Days before DOD')\n",
    "ax1.set_ylabel('PSA level')\n",
    "ax1.set_title('1) True Positive case',fontsize=15)\n",
    "\n",
    "#26 감소했다 급증\n",
    "ax2=fig.add_subplot(2,2,2)\n",
    "ax2.plot(testset2.loc[155,\"interval1\":\"interval9\"],testset2.loc[155,\"PSA1\":\"PSA9\"],'o-b')\n",
    "ax2.plot(x,testset2.loc[155,\"PSA360\":\"PSA0\"],'sr')\n",
    "ax2.set_xlim([0,360])\n",
    "ax2.set_ylim([3,4.5])\n",
    "ax2.invert_xaxis()\n",
    "ax2.set_xticks([360,270,180,90,0])\n",
    "ax2.axvline(x=270,color='k',linewidth=1)\n",
    "ax2.axvline(x=180,color='k',linewidth=1)\n",
    "ax2.axvline(x=90,color='k',linewidth=1)\n",
    "ax2.set_xlabel('Days before DOD')\n",
    "ax2.set_ylabel('PSA level')\n",
    "ax2.set_title('2) False Negative case',fontsize=15)\n",
    "\n",
    "ax3=fig.add_subplot(2,2,3)\n",
    "ax3.plot(testset2.loc[8,\"interval1\":\"interval12\"],testset2.loc[8,\"PSA1\":\"PSA12\"],'o-b') #TP\n",
    "ax3.plot(x,testset2.loc[8,\"PSA360\":\"PSA0\"],'or')\n",
    "ax3.set_xlim([0,360])\n",
    "ax3.invert_xaxis()\n",
    "ax3.set_xticks([360,270,180,90,0])\n",
    "ax3.axvline(x=270,color='k',linewidth=1)\n",
    "ax3.axvline(x=180,color='k',linewidth=1)\n",
    "ax3.axvline(x=90,color='k',linewidth=1)\n",
    "ax3.set_xlabel('Days before DOD')\n",
    "ax3.set_ylabel('PSA level')\n",
    "ax3.set_title('3) True Positive case',fontsize=15)\n",
    "\n",
    "ax4=fig.add_subplot(2,2,4)\n",
    "ax4.plot(testset2.loc[27,\"interval1\":\"interval4\"],testset2.loc[27,\"PSA1\":\"PSA4\"],'o-b') #TP\n",
    "ax4.plot(x,testset2.loc[27,\"PSA360\":\"PSA0\"],'or')\n",
    "ax4.set_xlim([0,360])\n",
    "ax4.invert_xaxis()\n",
    "ax4.set_xticks([360,270,180,90,0])\n",
    "ax4.axvline(x=270,color='k',linewidth=1)\n",
    "ax4.axvline(x=180,color='k',linewidth=1)\n",
    "ax4.axvline(x=90,color='k',linewidth=1)\n",
    "ax4.set_xlabel('Days before DOD')\n",
    "ax4.set_ylabel('PSA level')\n",
    "ax4.set_title('4) True Positive case',fontsize=15)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"./output.png\",dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 1~3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1726.000000\n",
       "mean        5.771113\n",
       "std         4.075204\n",
       "min         0.230000\n",
       "25%         3.255714\n",
       "50%         4.724167\n",
       "75%         7.207083\n",
       "max        42.557500\n",
       "Name: PSA_average, dtype: float64"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_drop.groupby('event')[\"final_num\"].describe()\n",
    "data_drop[\"PSA_average\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=0.9959404074097448, pvalue=0.3194882589514328)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "stats.ttest_ind(data_drop[data_drop['event']==1][\"PSA_average\"].values,data_drop[data_drop['event']==0][\"PSA_average\"].values,equal_var = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
